{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions ##\n",
    "971 yeast\n",
    "\n",
    "linear regression, snp weight vector, 选top 10%， 20%， 30%。。\n",
    "\n",
    "分别用这些snp来再跑linear regression的model( no gene)\n",
    "\n",
    "再把这些snp放到gene上，再跑一次linear regression model\n",
    "\n",
    " \n",
    "\n",
    "神经网络input snp\n",
    "\n",
    "最多两千个feature，用linear regression来筛选\n",
    "\n",
    " \n",
    "\n",
    "还要考虑promoter， enhancer（先不搞）\n",
    "\n",
    " \n",
    "\n",
    "1. 先选snp （Science paper：看snp preproccesing的procedure）<5 删掉\n",
    "\n",
    "2. fit linear regression model\n",
    "\n",
    "3. 选大约2000个\n",
    "\n",
    "4. 再用神经网络 （pytorch）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import bisect as bs\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import scipy.stats as stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18921</th>\n",
       "      <th>18922</th>\n",
       "      <th>18923</th>\n",
       "      <th>18924</th>\n",
       "      <th>18925</th>\n",
       "      <th>18926</th>\n",
       "      <th>18927</th>\n",
       "      <th>18928</th>\n",
       "      <th>18929</th>\n",
       "      <th>18930</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAA_AAA</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAB_AAB</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAC_AAC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAD_AAD</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAE_AAE</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 18931 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4      5      6      7      8      9      \\\n",
       "AAA_AAA      0      0      0      1      0      0      0      0      0      0   \n",
       "AAB_AAB      0      0      0      0      1      0      0      0      0      0   \n",
       "AAC_AAC      0      0      0      1      0      0      0      0      0      0   \n",
       "AAD_AAD      0      0      0      0      1      1      1      0      0      0   \n",
       "AAE_AAE      0      0      0      1      0      0      0      0      0      0   \n",
       "\n",
       "         ...  18921  18922  18923  18924  18925  18926  18927  18928  18929  \\\n",
       "AAA_AAA  ...      0      0      0      0      0      0      1      0      0   \n",
       "AAB_AAB  ...      0      0      0      0      0      0      1      0      0   \n",
       "AAC_AAC  ...      0      0      0      0      0      0      0      0      0   \n",
       "AAD_AAD  ...      0      0      1      1      1      1      0      0      1   \n",
       "AAE_AAE  ...      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "         18930  \n",
       "AAA_AAA      0  \n",
       "AAB_AAB      0  \n",
       "AAC_AAC      0  \n",
       "AAD_AAD      1  \n",
       "AAE_AAE      0  \n",
       "\n",
       "[5 rows x 18931 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNP_url = \"./1011geneSNP.csv\"\n",
    "Gene_url = \"./GCF_000146045.2_R64_genomic.gff\"\n",
    "\n",
    "snp_yeast_matrix = pd.read_csv(SNP_url, sep = ',' , error_bad_lines=False)\n",
    "del snp_yeast_matrix['#CHROM']\n",
    "del snp_yeast_matrix['POS']\n",
    "del snp_yeast_matrix['REF']\n",
    "del snp_yeast_matrix['ALT']\n",
    "del snp_yeast_matrix['ANN[*].GENE']\n",
    "del snp_yeast_matrix['ANN[*].GENEID']\n",
    "index_list = snp_yeast_matrix[\"ID\"].tolist()\n",
    "del snp_yeast_matrix['ID']\n",
    "snp_yeast_matrix = snp_yeast_matrix.transpose()\n",
    "column_names = list(snp_yeast_matrix.index)\n",
    "to_delete = [\"ABC\", \"ABF\", \"ASP\", \"BGS\",\"BGF\", \"BHL\",\"BHQ\",\"BID\",\"BIR\",\"BKG\",\"BKN\",\"CFH\",\"SACE_YAL\",\"SACE_YBA\",\n",
    "             \"SACE_YBB\",\"SACE_YBM\",\"SACE_YBN\",\"SACE_YBO\",\"SACE_YBP\",\"SACE_YCJ\",\"SACE_YCS\",\"SACE_YCT\",\"SACE_YCU\",\"SACE_YCV\",\"SACE_YCW\",\n",
    "             \"SACE_YCX\",\"SACE_YCY\",\"SACE_YCZ\",\"SACE_YDA\",\"SACE_YDB\",\"SACE_YDC\",\"SACE_YDD\", \"SACE_YDE\",\"SACE_YDF\",\"SACE_YDG\",\"SACE_YDH\",\"SACE_YDI\",\n",
    "             \"SACE_YDJ\", \"SACE_YDK\", \"SACE_YDL\"]\n",
    "print(len(to_delete))\n",
    "index = 0\n",
    "for index in range(len(to_delete)):\n",
    "    curr = to_delete[index]\n",
    "    curr = curr+\"_\"+curr\n",
    "    to_delete[index] = curr\n",
    "    index+=1\n",
    "to_delete_index = []\n",
    "for item in to_delete:\n",
    "    curr_index = column_names.index(item)\n",
    "    to_delete_index.append(curr_index)\n",
    "snp_yeast_matrix = snp_yeast_matrix.drop(to_delete)\n",
    "snp_yeast_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_url = \"./phenoMatrix_35ConditionsNormalizedByYPD.csv\"\n",
    "pheno_yeast_matrix = pd.read_csv(pheno_url, delimiter='\\t', error_bad_lines=False)\n",
    "YPD6AU = pheno_yeast_matrix[\"YPD6AU\"]\n",
    "YPDBENOMYL200 = pheno_yeast_matrix[\"YPDBENOMYL200\"]\n",
    "YPDNACL1M = pheno_yeast_matrix[\"YPDNACL1M\"]\n",
    "YPDSDS = pheno_yeast_matrix[\"YPDSDS\"]\n",
    "YPGALACTOSE = pheno_yeast_matrix[\"YPGALACTOSE\"]\n",
    "YPGLYCEROL = pheno_yeast_matrix[\"YPGLYCEROL\"]\n",
    "YPRIBOSE = pheno_yeast_matrix[\"YPRIBOSE\"]\n",
    "YPSORBITOL = pheno_yeast_matrix[\"YPSORBITOL\"]\n",
    "total_run = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017625507644391997\n",
      "[-2.40193329e+08 -9.51329668e+07 -6.91509560e+07 ... -2.07819641e+05\n",
      "  1.72123812e+06 -3.65489209e+06]\n",
      "[   22    40   177 ... 12580 13439  1064]\n",
      "[35909712.86755651 30969496.06911329 30172606.27411069 ...\n",
      "  2057249.79204882  2056066.05203741  2055314.69394521]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_1 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPD6AU, test_size=0.2)\n",
    "    lr1 = linear_model.LinearRegression()\n",
    "    lr1.fit(x_train, y_train)\n",
    "    temp = np.array(lr1.coef_)\n",
    "    weight_vector_1 = [x + y for x, y in zip(weight_vector_1, temp)]\n",
    "    predictions_1 = lr1.predict(x_test)\n",
    "    pcc_1 = stat.pearsonr(predictions_1, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_1[0]\n",
    "\n",
    "avg_assoc_1 = sum/total_run\n",
    "avg_weight_vector_1 = np.divide(weight_vector_1, total_run)\n",
    "print(avg_assoc_1)\n",
    "print(avg_weight_vector_1)\n",
    "top2000_index = (-avg_weight_vector_1).argsort()[:2000]\n",
    "print (top2000_index)\n",
    "print (avg_weight_vector_1[top2000_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.037436911180733945\n",
      "[-3.37849673e+05  6.46576234e+07 -1.76284387e+07 ...  5.74838207e+04\n",
      "  6.64331609e+05 -1.09117706e+06]\n",
      "[    1     4     7 ... 17459  8807   579]\n",
      "[64657623.42498443 43433330.5155755  14566282.72546469 ...\n",
      "   724000.37730427   723879.2780653    723841.16101986]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_3 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDNACL1M, test_size=0.2)\n",
    "    lr3 = linear_model.LinearRegression()\n",
    "    lr3.fit(x_train, y_train)\n",
    "    temp = np.array(lr3.coef_)\n",
    "    weight_vector_3 = [x + y for x, y in zip(weight_vector_3, temp)]\n",
    "    predictions_3 = lr3.predict(x_test)\n",
    "    pcc_3 = stat.pearsonr(predictions_3, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_3[0]\n",
    "\n",
    "avg_assoc_3 = sum/total_run\n",
    "avg_weight_vector_3 = np.divide(weight_vector_3, total_run)\n",
    "print(avg_assoc_3)\n",
    "print(avg_weight_vector_3)\n",
    "top2000_index_3 = (-avg_weight_vector_3).argsort()[:2000]\n",
    "print (top2000_index_3)\n",
    "print (avg_weight_vector_3[top2000_index_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.050776698519380235\n",
      "[ 1.80293179e+08  3.00320564e+07  1.00394208e+07 ... -3.23214271e+04\n",
      " -1.37441458e+06 -3.48155840e+05]\n",
      "[   0    9    1 ... 8013 2623 8329]\n",
      "[1.80293179e+08 3.77394678e+07 3.00320564e+07 ... 7.83034738e+05\n",
      " 7.82003570e+05 7.81750319e+05]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_4 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDSDS, test_size=0.2)\n",
    "    lr4 = linear_model.LinearRegression()\n",
    "    lr4.fit(x_train, y_train)\n",
    "    temp = np.array(lr4.coef_)\n",
    "    weight_vector_4 = [x + y for x, y in zip(weight_vector_4, temp)]\n",
    "    predictions_4 = lr4.predict(x_test)\n",
    "    pcc_4 = stat.pearsonr(predictions_4, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_4[0]\n",
    "\n",
    "avg_assoc_4 = sum/total_run\n",
    "avg_weight_vector_4 = np.divide(weight_vector_4, total_run)\n",
    "print(avg_assoc_4)\n",
    "print(avg_weight_vector_4)\n",
    "top2000_index_4 = (-avg_weight_vector_4).argsort()[:2000]\n",
    "print (top2000_index_4)\n",
    "print (avg_weight_vector_4[top2000_index_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01487416324101254\n",
      "[ -7086718.755928    19818053.69945217 -10674090.39858287 ...\n",
      "    368086.6244495    -112899.05171859   -325806.63563745]\n",
      "[    4     6     1 ... 15876 13231  7014]\n",
      "[28971723.24723437 20022340.91732469 19818053.69945217 ...\n",
      "   698925.65741911   698919.24679987   698725.96386096]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_2 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDBENOMYL200, test_size=0.2)\n",
    "    lr2 = linear_model.LinearRegression()\n",
    "    lr2.fit(x_train, y_train)\n",
    "    temp = np.array(lr2.coef_)\n",
    "    weight_vector_2 = [x + y for x, y in zip(weight_vector_2, temp)]\n",
    "    predictions_2 = lr2.predict(x_test)\n",
    "    pcc_2 = stat.pearsonr(predictions_2, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_2[0]\n",
    "\n",
    "avg_assoc_2 = sum/total_run\n",
    "avg_weight_vector_2 = np.divide(weight_vector_2, total_run)\n",
    "print(avg_assoc_2)\n",
    "print(avg_weight_vector_2)\n",
    "top2000_index_2 = (-avg_weight_vector_2).argsort()[:2000]\n",
    "print (top2000_index_2)\n",
    "print (avg_weight_vector_2[top2000_index_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027988724790353886\n",
      "[-3.55220592e+08  1.09278129e+08 -2.61123680e+08 ... -4.55756319e+05\n",
      "  2.54604957e+06 -5.72904128e+04]\n",
      "[    1    11    10 ...  3537 12134 11862]\n",
      "[1.09278129e+08 7.41107208e+07 6.94411198e+07 ... 1.69495390e+06\n",
      " 1.69492387e+06 1.69119076e+06]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_5 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPGALACTOSE, test_size=0.2)\n",
    "    lr5 = linear_model.LinearRegression()\n",
    "    lr5.fit(x_train, y_train)\n",
    "    temp = np.array(lr5.coef_)\n",
    "    weight_vector_5 = [x + y for x, y in zip(weight_vector_5, temp)]\n",
    "    predictions_5 = lr5.predict(x_test)\n",
    "    pcc_5 = stat.pearsonr(predictions_5, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_5[0]\n",
    "\n",
    "avg_assoc_5 = sum/total_run\n",
    "avg_weight_vector_5 = np.divide(weight_vector_5, total_run)\n",
    "print(avg_assoc_5)\n",
    "print(avg_weight_vector_5)\n",
    "top2000_index_5 = (-avg_weight_vector_5).argsort()[:2000]\n",
    "print (top2000_index_5)\n",
    "print (avg_weight_vector_5[top2000_index_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01669042277109073\n",
      "[ 9.57655442e+07  8.30312598e+07 -5.45238087e+07 ... -6.39761225e+03\n",
      " -4.70708416e+06  2.03677672e+06]\n",
      "[    0     1     4 ... 13165 16181  7643]\n",
      "[95765544.23048124 83031259.82149577 63159836.09703083 ...\n",
      "  4184377.91238532  4183117.47498691  4180022.78792203]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_6 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPGLYCEROL, test_size=0.2)\n",
    "    lr6 = linear_model.LinearRegression()\n",
    "    lr6.fit(x_train, y_train)\n",
    "    temp = np.array(lr6.coef_)\n",
    "    weight_vector_6 = [x + y for x, y in zip(weight_vector_6, temp)]\n",
    "    predictions_6 = lr6.predict(x_test)\n",
    "    pcc_6 = stat.pearsonr(predictions_6, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_6[0]\n",
    "\n",
    "avg_assoc_6 = sum/total_run\n",
    "avg_weight_vector_6 = np.divide(weight_vector_6, total_run)\n",
    "print(avg_assoc_6)\n",
    "print(avg_weight_vector_6)\n",
    "top2000_index_6 = (-avg_weight_vector_6).argsort()[:2000]\n",
    "print (top2000_index_6)\n",
    "print (avg_weight_vector_6[top2000_index_6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012708775898420057\n",
      "[ 1.37363610e+08  2.55347170e+07 -2.27010896e+07 ... -5.95999093e+05\n",
      " -1.03600312e+04  1.72148497e+06]\n",
      "[    0     3    22 ...  9194  6502 15553]\n",
      "[1.37363610e+08 3.90485854e+07 3.55737953e+07 ... 1.04004830e+06\n",
      " 1.03954627e+06 1.03873070e+06]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_7 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPRIBOSE, test_size=0.2)\n",
    "    lr7 = linear_model.LinearRegression()\n",
    "    lr7.fit(x_train, y_train)\n",
    "    temp = np.array(lr7.coef_)\n",
    "    weight_vector_7 = [x + y for x, y in zip(weight_vector_7, temp)]\n",
    "    predictions_7 = lr7.predict(x_test)\n",
    "    pcc_7 = stat.pearsonr(predictions_7, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_7[0]\n",
    "\n",
    "avg_assoc_7 = sum/total_run\n",
    "avg_weight_vector_7 = np.divide(weight_vector_7, total_run)\n",
    "print(avg_assoc_7)\n",
    "print(avg_weight_vector_7)\n",
    "top2000_index_7 = (-avg_weight_vector_7).argsort()[:2000]\n",
    "print (top2000_index_7)\n",
    "print (avg_weight_vector_7[top2000_index_7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00658958047780311\n",
      "[-1.98248928e+08  2.49733290e+07 -2.84093445e+07 ...  6.25713982e+05\n",
      "  3.72645493e+05 -1.22592640e+06]\n",
      "[  14   18   19 ... 3716 2050 3460]\n",
      "[56802836.23059125 42934110.51236925 37816279.76193699 ...\n",
      "  2528406.61915241  2527849.51669522  2527272.76368826]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_8 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPSORBITOL, test_size=0.2)\n",
    "    lr8 = linear_model.LinearRegression()\n",
    "    lr8.fit(x_train, y_train)\n",
    "    temp = np.array(lr8.coef_)\n",
    "    weight_vector_8 = [x + y for x, y in zip(weight_vector_8, temp)]\n",
    "    predictions_8 = lr8.predict(x_test)\n",
    "    pcc_8 = stat.pearsonr(predictions_8, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_8[0]\n",
    "\n",
    "avg_assoc_8 = sum/total_run\n",
    "avg_weight_vector_8 = np.divide(weight_vector_8, total_run)\n",
    "print(avg_assoc_8)\n",
    "print(avg_weight_vector_8)\n",
    "top2000_index_8 = (-avg_weight_vector_8).argsort()[:2000]\n",
    "print (top2000_index_8)\n",
    "print (avg_weight_vector_8[top2000_index_8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = [\"YPD6AU\", \"YPDBENOMYL200\", \"YPDNACL1M\", \"YPDSDS\", \"YPGALACTOSE\", \"YPGLYCEROL\", \"YPRIBOSE\", \"YPSORBITOL\"]\n",
    "index_df = pd.DataFrame(list(zip(top2000_index, top2000_index_2, top2000_index_3, top2000_index_4, top2000_index_5, top2000_index_6, top2000_index_7, top2000_index_8)), columns = df_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一层hiddeen nueron 数量为一\n",
    "\n",
    "2000 1000 500 100 1 \n",
    "\n",
    "adam optimize 要\n",
    "\n",
    "batch normalization \n",
    "\n",
    ".cuda 挪到gpu\n",
    "\n",
    "batch training \n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Class #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as dt\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, h1, h2, h3, output_size, lr):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, h1)\n",
    "        self.l2 = nn.Linear(h1, h2)\n",
    "        self.l3 = nn.Linear(h2, h3)\n",
    "        self.l4 = nn.Linear(h3, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l4(out)\n",
    "        return out\n",
    "    \n",
    "    #Not used. \n",
    "    def train_net(self, iterations, train_loader, test_loader):\n",
    "        for epoch in range (num_epochs):\n",
    "            total_loss = 0\n",
    "            counter = 0\n",
    "            for i, (data, target) in enumerate(train_loader):\n",
    "                self.zero_grad()\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = self.forward(data).to(device)\n",
    "                loss = self.loss_fn(output, target)\n",
    "                total_loss += loss\n",
    "                counter += 1\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            check_correlation(test_loader, self)    \n",
    "            epoch += 1\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                .format(epoch, num_epochs, total_loss/len(train_loader)))\n",
    "    \n",
    "    def check_correlation(test_loader, net):\n",
    "        counter = 0\n",
    "        with torch.no_grad():\n",
    "            total_err = 0.0\n",
    "            for (data, target) in test_loader:\n",
    "                data = data.to(device)\n",
    "                target = target.cpu().numpy()\n",
    "                output = torch.squeeze(net(data)).cpu().numpy()\n",
    "                result = stat.pearsonr(target, output)\n",
    "        print(\"Correlation: %f \\n P-Value: %f\" %(result[0], result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Config Section ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "input_size = 2000\n",
    "h1 = 1000\n",
    "h2 = 200\n",
    "h3 = 100\n",
    "output_size = 1\n",
    "lr = 0.0001\n",
    "num_epochs = 10\n",
    "batch_size = 200\n",
    "split = 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: -0.198183 \n",
      " P-Value: 0.005482\n",
      "Epoch [1/10], Loss: 0.1250\n",
      "Correlation: -0.192899 \n",
      " P-Value: 0.006897\n",
      "Epoch [2/10], Loss: 0.0555\n",
      "Correlation: -0.038615 \n",
      " P-Value: 0.591988\n",
      "Epoch [3/10], Loss: 0.0380\n",
      "Correlation: 0.266241 \n",
      " P-Value: 0.000169\n",
      "Epoch [4/10], Loss: 0.0258\n",
      "Correlation: 0.255406 \n",
      " P-Value: 0.000314\n",
      "Epoch [5/10], Loss: 0.0266\n",
      "Correlation: 0.084310 \n",
      " P-Value: 0.241261\n",
      "Epoch [6/10], Loss: 0.0241\n",
      "Correlation: 0.015851 \n",
      " P-Value: 0.825920\n",
      "Epoch [7/10], Loss: 0.0218\n",
      "Correlation: -0.151693 \n",
      " P-Value: 0.034266\n",
      "Epoch [8/10], Loss: 0.0211\n",
      "Correlation: -0.162698 \n",
      " P-Value: 0.023056\n",
      "Epoch [9/10], Loss: 0.0205\n",
      "Correlation: -0.057930 \n",
      " P-Value: 0.421151\n",
      "Epoch [10/10], Loss: 0.0200\n"
     ]
    }
   ],
   "source": [
    "traning_data = torch.tensor(snp_yeast_matrix[top2000_index].values, dtype = torch.float)\n",
    "answer = torch.tensor(YPD6AU, dtype = torch.float)\n",
    "train_set = dt.TensorDataset(traning_data, answer)\n",
    "train_size = int(split * len(train_set))\n",
    "test_size = len(train_set) - train_size \n",
    "\n",
    "train_dataset, test_dataset = dt.random_split(train_set, [train_size, test_size])\n",
    "train_loader = dt.DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
    "\n",
    "net = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "\n",
    "test_loader = dt.DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
    "\n",
    "net.train_net(num_epochs, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.105323 \n",
      " P-Value: 0.142817\n",
      "Epoch [1/10], Loss: 0.0352\n",
      "Correlation: 0.063321 \n",
      " P-Value: 0.379168\n",
      "Epoch [2/10], Loss: 0.0153\n",
      "Correlation: 0.011934 \n",
      " P-Value: 0.868487\n",
      "Epoch [3/10], Loss: 0.0125\n",
      "Correlation: 0.065269 \n",
      " P-Value: 0.364651\n",
      "Epoch [4/10], Loss: 0.0113\n",
      "Correlation: 0.084616 \n",
      " P-Value: 0.239553\n",
      "Epoch [5/10], Loss: 0.0107\n",
      "Correlation: 0.056291 \n",
      " P-Value: 0.434434\n",
      "Epoch [6/10], Loss: 0.0103\n",
      "Correlation: 0.015617 \n",
      " P-Value: 0.828452\n",
      "Epoch [7/10], Loss: 0.0100\n",
      "Correlation: 0.036896 \n",
      " P-Value: 0.608592\n",
      "Epoch [8/10], Loss: 0.0100\n",
      "Correlation: 0.016002 \n",
      " P-Value: 0.824288\n",
      "Epoch [9/10], Loss: 0.0099\n",
      "Correlation: -0.011238 \n",
      " P-Value: 0.876088\n",
      "Epoch [10/10], Loss: 0.0098\n"
     ]
    }
   ],
   "source": [
    "traning_data_2 = torch.tensor(snp_yeast_matrix[top2000_index_2].values, dtype = torch.float)\n",
    "answer_2 = torch.tensor(YPDBENOMYL200, dtype = torch.float)\n",
    "train_set_2 = dt.TensorDataset(traning_data_2, answer_2)\n",
    "\n",
    "train_dataset_2, test_dataset_2 = dt.random_split(train_set_2, [train_size, test_size])\n",
    "train_loader_2 = dt.DataLoader(dataset=train_dataset_2, batch_size=batch_size)\n",
    "test_loader_2 = dt.DataLoader(dataset=test_dataset_2, batch_size=batch_size)\n",
    "\n",
    "net_2 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_2.train_net(num_epochs, train_loader_2, test_loader_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: -0.078553 \n",
      " P-Value: 0.275021\n",
      "Epoch [1/10], Loss: 0.0630\n",
      "Correlation: -0.008401 \n",
      " P-Value: 0.907211\n",
      "Epoch [2/10], Loss: 0.0227\n",
      "Correlation: 0.192797 \n",
      " P-Value: 0.006927\n",
      "Epoch [3/10], Loss: 0.0160\n",
      "Correlation: 0.207223 \n",
      " P-Value: 0.003652\n",
      "Epoch [4/10], Loss: 0.0144\n",
      "Correlation: 0.051339 \n",
      " P-Value: 0.475988\n",
      "Epoch [5/10], Loss: 0.0125\n",
      "Correlation: 0.001463 \n",
      " P-Value: 0.983804\n",
      "Epoch [6/10], Loss: 0.0118\n",
      "Correlation: 0.110147 \n",
      " P-Value: 0.125303\n",
      "Epoch [7/10], Loss: 0.0110\n",
      "Correlation: 0.117783 \n",
      " P-Value: 0.101029\n",
      "Epoch [8/10], Loss: 0.0108\n",
      "Correlation: 0.094068 \n",
      " P-Value: 0.190858\n",
      "Epoch [9/10], Loss: 0.0108\n",
      "Correlation: 0.088130 \n",
      " P-Value: 0.220524\n",
      "Epoch [10/10], Loss: 0.0106\n"
     ]
    }
   ],
   "source": [
    "traning_data_3 = torch.tensor(snp_yeast_matrix[top2000_index_3].values, dtype = torch.float)\n",
    "answer_3 = torch.tensor(YPDNACL1M, dtype = torch.float)\n",
    "train_set_3 = dt.TensorDataset(traning_data_3, answer_3)\n",
    "\n",
    "train_dataset_3, test_dataset_3 = dt.random_split(train_set_3, [train_size, test_size])\n",
    "train_loader_3 = dt.DataLoader(dataset=train_dataset_3, batch_size=batch_size)\n",
    "test_loader_3 = dt.DataLoader(dataset=test_dataset_3, batch_size=batch_size)\n",
    "\n",
    "net_3 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_3.train_net(num_epochs, train_loader_3, test_loader_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.135619 \n",
      " P-Value: 0.058707\n",
      "Epoch [1/10], Loss: 0.0440\n",
      "Correlation: -0.106545 \n",
      " P-Value: 0.138214\n",
      "Epoch [2/10], Loss: 0.0292\n",
      "Correlation: -0.153386 \n",
      " P-Value: 0.032288\n",
      "Epoch [3/10], Loss: 0.0276\n",
      "Correlation: 0.049778 \n",
      " P-Value: 0.489523\n",
      "Epoch [4/10], Loss: 0.0269\n",
      "Correlation: -0.025895 \n",
      " P-Value: 0.719340\n",
      "Epoch [5/10], Loss: 0.0267\n",
      "Correlation: -0.183215 \n",
      " P-Value: 0.010355\n",
      "Epoch [6/10], Loss: 0.0264\n",
      "Correlation: -0.170133 \n",
      " P-Value: 0.017413\n",
      "Epoch [7/10], Loss: 0.0264\n",
      "Correlation: -0.146172 \n",
      " P-Value: 0.041448\n",
      "Epoch [8/10], Loss: 0.0263\n",
      "Correlation: -0.097923 \n",
      " P-Value: 0.173229\n",
      "Epoch [9/10], Loss: 0.0263\n",
      "Correlation: -0.091464 \n",
      " P-Value: 0.203487\n",
      "Epoch [10/10], Loss: 0.0263\n"
     ]
    }
   ],
   "source": [
    "traning_data_4 = torch.tensor(snp_yeast_matrix[top2000_index_4].values, dtype = torch.float)\n",
    "answer_4 = torch.tensor(YPDSDS, dtype = torch.float)\n",
    "train_set_4 = dt.TensorDataset(traning_data_4, answer_4)\n",
    "\n",
    "train_dataset_4, test_dataset_4 = dt.random_split(train_set_4, [train_size, test_size])\n",
    "train_loader_4 = dt.DataLoader(dataset=train_dataset_4, batch_size=batch_size)\n",
    "test_loader_4 = dt.DataLoader(dataset=test_dataset_4, batch_size=batch_size)\n",
    "\n",
    "net_4 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_4.train_net(num_epochs, train_loader_4, test_loader_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.149645 \n",
      " P-Value: 0.036797\n",
      "Epoch [1/10], Loss: 0.6477\n",
      "Correlation: 0.154437 \n",
      " P-Value: 0.031109\n",
      "Epoch [2/10], Loss: 0.3905\n",
      "Correlation: 0.143922 \n",
      " P-Value: 0.044718\n",
      "Epoch [3/10], Loss: 0.2681\n",
      "Correlation: 0.091020 \n",
      " P-Value: 0.205700\n",
      "Epoch [4/10], Loss: 0.2069\n",
      "Correlation: -0.056869 \n",
      " P-Value: 0.429721\n",
      "Epoch [5/10], Loss: 0.1401\n",
      "Correlation: -0.132319 \n",
      " P-Value: 0.065188\n",
      "Epoch [6/10], Loss: 0.1305\n",
      "Correlation: -0.125712 \n",
      " P-Value: 0.079923\n",
      "Epoch [7/10], Loss: 0.1317\n",
      "Correlation: -0.092125 \n",
      " P-Value: 0.200223\n",
      "Epoch [8/10], Loss: 0.1210\n",
      "Correlation: -0.077723 \n",
      " P-Value: 0.280144\n",
      "Epoch [9/10], Loss: 0.1113\n",
      "Correlation: -0.066405 \n",
      " P-Value: 0.356347\n",
      "Epoch [10/10], Loss: 0.1077\n"
     ]
    }
   ],
   "source": [
    "traning_data_5 = torch.tensor(snp_yeast_matrix[top2000_index_5].values, dtype = torch.float)\n",
    "answer_5 = torch.tensor(YPGALACTOSE, dtype = torch.float)\n",
    "train_set_5 = dt.TensorDataset(traning_data_5, answer_5)\n",
    "\n",
    "train_dataset_5, test_dataset_5 = dt.random_split(train_set_5, [train_size, test_size])\n",
    "train_loader_5 = dt.DataLoader(dataset=train_dataset_5, batch_size=batch_size)\n",
    "test_loader_5 = dt.DataLoader(dataset=test_dataset_5, batch_size=batch_size)\n",
    "\n",
    "net_5 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_5.train_net(num_epochs, train_loader_5, test_loader_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.065097 \n",
      " P-Value: 0.365921\n",
      "Epoch [1/10], Loss: 0.2307\n",
      "Correlation: 0.072467 \n",
      " P-Value: 0.314050\n",
      "Epoch [2/10], Loss: 0.0942\n",
      "Correlation: 0.112307 \n",
      " P-Value: 0.118019\n",
      "Epoch [3/10], Loss: 0.0715\n",
      "Correlation: 0.104831 \n",
      " P-Value: 0.144703\n",
      "Epoch [4/10], Loss: 0.0538\n",
      "Correlation: 0.082529 \n",
      " P-Value: 0.251379\n",
      "Epoch [5/10], Loss: 0.0525\n",
      "Correlation: 0.062075 \n",
      " P-Value: 0.388636\n",
      "Epoch [6/10], Loss: 0.0492\n",
      "Correlation: 0.023762 \n",
      " P-Value: 0.741606\n",
      "Epoch [7/10], Loss: 0.0441\n",
      "Correlation: -0.031889 \n",
      " P-Value: 0.658089\n",
      "Epoch [8/10], Loss: 0.0428\n",
      "Correlation: -0.099866 \n",
      " P-Value: 0.164814\n",
      "Epoch [9/10], Loss: 0.0402\n",
      "Correlation: -0.136849 \n",
      " P-Value: 0.056432\n",
      "Epoch [10/10], Loss: 0.0394\n"
     ]
    }
   ],
   "source": [
    "traning_data_6 = torch.tensor(snp_yeast_matrix[top2000_index_6].values, dtype = torch.float)\n",
    "answer_6 = torch.tensor(YPGLYCEROL, dtype = torch.float)\n",
    "train_set_6 = dt.TensorDataset(traning_data_6, answer_6)\n",
    "\n",
    "train_dataset_6, test_dataset_6 = dt.random_split(train_set_6, [train_size, test_size])\n",
    "train_loader_6 = dt.DataLoader(dataset=train_dataset_6, batch_size=batch_size)\n",
    "test_loader_6 = dt.DataLoader(dataset=test_dataset_6, batch_size=batch_size)\n",
    "\n",
    "net_6 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_6.train_net(num_epochs, train_loader_6, test_loader_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.199129 \n",
      " P-Value: 0.005258\n",
      "Epoch [1/10], Loss: 0.1729\n",
      "Correlation: 0.196270 \n",
      " P-Value: 0.005961\n",
      "Epoch [2/10], Loss: 0.0760\n",
      "Correlation: 0.168728 \n",
      " P-Value: 0.018376\n",
      "Epoch [3/10], Loss: 0.0505\n",
      "Correlation: 0.036329 \n",
      " P-Value: 0.614109\n",
      "Epoch [4/10], Loss: 0.0345\n",
      "Correlation: -0.001790 \n",
      " P-Value: 0.980183\n",
      "Epoch [5/10], Loss: 0.0334\n",
      "Correlation: 0.067814 \n",
      " P-Value: 0.346211\n",
      "Epoch [6/10], Loss: 0.0291\n",
      "Correlation: 0.104366 \n",
      " P-Value: 0.146503\n",
      "Epoch [7/10], Loss: 0.0259\n",
      "Correlation: 0.021755 \n",
      " P-Value: 0.762749\n",
      "Epoch [8/10], Loss: 0.0247\n",
      "Correlation: -0.051956 \n",
      " P-Value: 0.470699\n",
      "Epoch [9/10], Loss: 0.0233\n",
      "Correlation: -0.042936 \n",
      " P-Value: 0.551185\n",
      "Epoch [10/10], Loss: 0.0226\n"
     ]
    }
   ],
   "source": [
    "traning_data_7 = torch.tensor(snp_yeast_matrix[top2000_index_7].values, dtype = torch.float)\n",
    "answer_7 = torch.tensor(YPRIBOSE, dtype = torch.float)\n",
    "train_set_7 = dt.TensorDataset(traning_data_7, answer_7)\n",
    "\n",
    "train_dataset_7, test_dataset_7 = dt.random_split(train_set_7, [train_size, test_size])\n",
    "train_loader_7 = dt.DataLoader(dataset=train_dataset_7, batch_size=batch_size)\n",
    "test_loader_7 = dt.DataLoader(dataset=test_dataset_7, batch_size=batch_size)\n",
    "\n",
    "net_7 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_7.train_net(num_epochs, train_loader_7, test_loader_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.154566 \n",
      " P-Value: 0.030966\n",
      "Epoch [1/10], Loss: 0.0919\n",
      "Correlation: 0.052128 \n",
      " P-Value: 0.469224\n",
      "Epoch [2/10], Loss: 0.0418\n",
      "Correlation: -0.151743 \n",
      " P-Value: 0.034206\n",
      "Epoch [3/10], Loss: 0.0349\n",
      "Correlation: -0.197579 \n",
      " P-Value: 0.005629\n",
      "Epoch [4/10], Loss: 0.0308\n",
      "Correlation: -0.136939 \n",
      " P-Value: 0.056268\n",
      "Epoch [5/10], Loss: 0.0300\n",
      "Correlation: -0.004157 \n",
      " P-Value: 0.954003\n",
      "Epoch [6/10], Loss: 0.0280\n",
      "Correlation: -0.061247 \n",
      " P-Value: 0.395007\n",
      "Epoch [7/10], Loss: 0.0273\n",
      "Correlation: -0.234795 \n",
      " P-Value: 0.000953\n",
      "Epoch [8/10], Loss: 0.0262\n",
      "Correlation: -0.237103 \n",
      " P-Value: 0.000846\n",
      "Epoch [9/10], Loss: 0.0259\n",
      "Correlation: -0.138230 \n",
      " P-Value: 0.053963\n",
      "Epoch [10/10], Loss: 0.0257\n"
     ]
    }
   ],
   "source": [
    "traning_data_8 = torch.tensor(snp_yeast_matrix[top2000_index_8].values, dtype = torch.float)\n",
    "answer_8 = torch.tensor(YPSORBITOL, dtype = torch.float)\n",
    "train_set_8 = dt.TensorDataset(traning_data_8, answer_8)\n",
    "\n",
    "train_dataset_8, test_dataset_8 = dt.random_split(train_set_8, [train_size, test_size])\n",
    "train_loader_8 = dt.DataLoader(dataset=train_dataset_8, batch_size=batch_size)\n",
    "test_loader_8 = dt.DataLoader(dataset=test_dataset_8, batch_size=batch_size)\n",
    "\n",
    "net_8 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_8.train_net(num_epochs, train_loader_8, test_loader_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
