{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions ##\n",
    "971 yeast\n",
    "\n",
    "linear regression, snp weight vector, 选top 10%， 20%， 30%。。\n",
    "\n",
    "分别用这些snp来再跑linear regression的model( no gene)\n",
    "\n",
    "再把这些snp放到gene上，再跑一次linear regression model\n",
    "\n",
    " \n",
    "\n",
    "神经网络input snp\n",
    "\n",
    "最多两千个feature，用linear regression来筛选\n",
    "\n",
    " \n",
    "\n",
    "还要考虑promoter， enhancer（先不搞）\n",
    "\n",
    " \n",
    "\n",
    "1. 先选snp （Science paper：看snp preproccesing的procedure）<5 删掉\n",
    "\n",
    "2. fit linear regression model\n",
    "\n",
    "3. 选大约2000个\n",
    "\n",
    "4. 再用神经网络 （pytorch）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import bisect as bs\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import scipy.stats as stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18921</th>\n",
       "      <th>18922</th>\n",
       "      <th>18923</th>\n",
       "      <th>18924</th>\n",
       "      <th>18925</th>\n",
       "      <th>18926</th>\n",
       "      <th>18927</th>\n",
       "      <th>18928</th>\n",
       "      <th>18929</th>\n",
       "      <th>18930</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAA_AAA</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAB_AAB</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAC_AAC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAD_AAD</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAE_AAE</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 18931 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4      5      6      7      8      9      \\\n",
       "AAA_AAA      0      0      0      1      0      0      0      0      0      0   \n",
       "AAB_AAB      0      0      0      0      1      0      0      0      0      0   \n",
       "AAC_AAC      0      0      0      1      0      0      0      0      0      0   \n",
       "AAD_AAD      0      0      0      0      1      1      1      0      0      0   \n",
       "AAE_AAE      0      0      0      1      0      0      0      0      0      0   \n",
       "\n",
       "         ...  18921  18922  18923  18924  18925  18926  18927  18928  18929  \\\n",
       "AAA_AAA  ...      0      0      0      0      0      0      1      0      0   \n",
       "AAB_AAB  ...      0      0      0      0      0      0      1      0      0   \n",
       "AAC_AAC  ...      0      0      0      0      0      0      0      0      0   \n",
       "AAD_AAD  ...      0      0      1      1      1      1      0      0      1   \n",
       "AAE_AAE  ...      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "         18930  \n",
       "AAA_AAA      0  \n",
       "AAB_AAB      0  \n",
       "AAC_AAC      0  \n",
       "AAD_AAD      1  \n",
       "AAE_AAE      0  \n",
       "\n",
       "[5 rows x 18931 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNP_url = \"./1011geneSNP.csv\"\n",
    "Gene_url = \"./GCF_000146045.2_R64_genomic.gff\"\n",
    "\n",
    "snp_yeast_matrix = pd.read_csv(SNP_url, sep = ',' , error_bad_lines=False)\n",
    "del snp_yeast_matrix['#CHROM']\n",
    "del snp_yeast_matrix['POS']\n",
    "del snp_yeast_matrix['REF']\n",
    "del snp_yeast_matrix['ALT']\n",
    "del snp_yeast_matrix['ANN[*].GENE']\n",
    "del snp_yeast_matrix['ANN[*].GENEID']\n",
    "index_list = snp_yeast_matrix[\"ID\"].tolist()\n",
    "del snp_yeast_matrix['ID']\n",
    "snp_yeast_matrix = snp_yeast_matrix.transpose()\n",
    "column_names = list(snp_yeast_matrix.index)\n",
    "to_delete = [\"ABC\", \"ABF\", \"ASP\", \"BGS\",\"BGF\", \"BHL\",\"BHQ\",\"BID\",\"BIR\",\"BKG\",\"BKN\",\"CFH\",\"SACE_YAL\",\"SACE_YBA\",\n",
    "             \"SACE_YBB\",\"SACE_YBM\",\"SACE_YBN\",\"SACE_YBO\",\"SACE_YBP\",\"SACE_YCJ\",\"SACE_YCS\",\"SACE_YCT\",\"SACE_YCU\",\"SACE_YCV\",\"SACE_YCW\",\n",
    "             \"SACE_YCX\",\"SACE_YCY\",\"SACE_YCZ\",\"SACE_YDA\",\"SACE_YDB\",\"SACE_YDC\",\"SACE_YDD\", \"SACE_YDE\",\"SACE_YDF\",\"SACE_YDG\",\"SACE_YDH\",\"SACE_YDI\",\n",
    "             \"SACE_YDJ\", \"SACE_YDK\", \"SACE_YDL\"]\n",
    "print(len(to_delete))\n",
    "index = 0\n",
    "for index in range(len(to_delete)):\n",
    "    curr = to_delete[index]\n",
    "    curr = curr+\"_\"+curr\n",
    "    to_delete[index] = curr\n",
    "    index+=1\n",
    "to_delete_index = []\n",
    "for item in to_delete:\n",
    "    curr_index = column_names.index(item)\n",
    "    to_delete_index.append(curr_index)\n",
    "snp_yeast_matrix = snp_yeast_matrix.drop(to_delete)\n",
    "snp_yeast_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_url = \"./phenoMatrix_35ConditionsNormalizedByYPD.csv\"\n",
    "pheno_yeast_matrix = pd.read_csv(pheno_url, delimiter='\\t', error_bad_lines=False)\n",
    "YPD6AU = pheno_yeast_matrix[\"YPD6AU\"]\n",
    "YPDBENOMYL200 = pheno_yeast_matrix[\"YPDBENOMYL200\"]\n",
    "YPDNACL1M = pheno_yeast_matrix[\"YPDNACL1M\"]\n",
    "YPDSDS = pheno_yeast_matrix[\"YPDSDS\"]\n",
    "YPGALACTOSE = pheno_yeast_matrix[\"YPGALACTOSE\"]\n",
    "YPGLYCEROL = pheno_yeast_matrix[\"YPGLYCEROL\"]\n",
    "YPRIBOSE = pheno_yeast_matrix[\"YPRIBOSE\"]\n",
    "YPSORBITOL = pheno_yeast_matrix[\"YPSORBITOL\"]\n",
    "total_run = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017625507644391997\n",
      "[-2.40193329e+08 -9.51329668e+07 -6.91509560e+07 ... -2.07819641e+05\n",
      "  1.72123812e+06 -3.65489209e+06]\n",
      "[   22    40   177 ... 12580 13439  1064]\n",
      "[35909712.86755651 30969496.06911329 30172606.27411069 ...\n",
      "  2057249.79204882  2056066.05203741  2055314.69394521]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_1 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPD6AU, test_size=0.2)\n",
    "    lr1 = linear_model.LinearRegression()\n",
    "    lr1.fit(x_train, y_train)\n",
    "    temp = np.array(lr1.coef_)\n",
    "    weight_vector_1 = [x + y for x, y in zip(weight_vector_1, temp)]\n",
    "    predictions_1 = lr1.predict(x_test)\n",
    "    pcc_1 = stat.pearsonr(predictions_1, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_1[0]\n",
    "\n",
    "avg_assoc_1 = sum/total_run\n",
    "avg_weight_vector_1 = np.divide(weight_vector_1, total_run)\n",
    "print(avg_assoc_1)\n",
    "print(avg_weight_vector_1)\n",
    "top2000_index = (-avg_weight_vector_1).argsort()[:2000]\n",
    "print (top2000_index)\n",
    "print (avg_weight_vector_1[top2000_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.037436911180733945\n",
      "[-3.37849673e+05  6.46576234e+07 -1.76284387e+07 ...  5.74838207e+04\n",
      "  6.64331609e+05 -1.09117706e+06]\n",
      "[    1     4     7 ... 17459  8807   579]\n",
      "[64657623.42498443 43433330.5155755  14566282.72546469 ...\n",
      "   724000.37730427   723879.2780653    723841.16101986]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_3 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDNACL1M, test_size=0.2)\n",
    "    lr3 = linear_model.LinearRegression()\n",
    "    lr3.fit(x_train, y_train)\n",
    "    temp = np.array(lr3.coef_)\n",
    "    weight_vector_3 = [x + y for x, y in zip(weight_vector_3, temp)]\n",
    "    predictions_3 = lr3.predict(x_test)\n",
    "    pcc_3 = stat.pearsonr(predictions_3, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_3[0]\n",
    "\n",
    "avg_assoc_3 = sum/total_run\n",
    "avg_weight_vector_3 = np.divide(weight_vector_3, total_run)\n",
    "print(avg_assoc_3)\n",
    "print(avg_weight_vector_3)\n",
    "top2000_index_3 = (-avg_weight_vector_3).argsort()[:2000]\n",
    "print (top2000_index_3)\n",
    "print (avg_weight_vector_3[top2000_index_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.050776698519380235\n",
      "[ 1.80293179e+08  3.00320564e+07  1.00394208e+07 ... -3.23214271e+04\n",
      " -1.37441458e+06 -3.48155840e+05]\n",
      "[   0    9    1 ... 8013 2623 8329]\n",
      "[1.80293179e+08 3.77394678e+07 3.00320564e+07 ... 7.83034738e+05\n",
      " 7.82003570e+05 7.81750319e+05]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_4 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDSDS, test_size=0.2)\n",
    "    lr4 = linear_model.LinearRegression()\n",
    "    lr4.fit(x_train, y_train)\n",
    "    temp = np.array(lr4.coef_)\n",
    "    weight_vector_4 = [x + y for x, y in zip(weight_vector_4, temp)]\n",
    "    predictions_4 = lr4.predict(x_test)\n",
    "    pcc_4 = stat.pearsonr(predictions_4, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_4[0]\n",
    "\n",
    "avg_assoc_4 = sum/total_run\n",
    "avg_weight_vector_4 = np.divide(weight_vector_4, total_run)\n",
    "print(avg_assoc_4)\n",
    "print(avg_weight_vector_4)\n",
    "top2000_index_4 = (-avg_weight_vector_4).argsort()[:2000]\n",
    "print (top2000_index_4)\n",
    "print (avg_weight_vector_4[top2000_index_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01487416324101254\n",
      "[ -7086718.755928    19818053.69945217 -10674090.39858287 ...\n",
      "    368086.6244495    -112899.05171859   -325806.63563745]\n",
      "[    4     6     1 ... 15876 13231  7014]\n",
      "[28971723.24723437 20022340.91732469 19818053.69945217 ...\n",
      "   698925.65741911   698919.24679987   698725.96386096]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_2 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDBENOMYL200, test_size=0.2)\n",
    "    lr2 = linear_model.LinearRegression()\n",
    "    lr2.fit(x_train, y_train)\n",
    "    temp = np.array(lr2.coef_)\n",
    "    weight_vector_2 = [x + y for x, y in zip(weight_vector_2, temp)]\n",
    "    predictions_2 = lr2.predict(x_test)\n",
    "    pcc_2 = stat.pearsonr(predictions_2, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_2[0]\n",
    "\n",
    "avg_assoc_2 = sum/total_run\n",
    "avg_weight_vector_2 = np.divide(weight_vector_2, total_run)\n",
    "print(avg_assoc_2)\n",
    "print(avg_weight_vector_2)\n",
    "top2000_index_2 = (-avg_weight_vector_2).argsort()[:2000]\n",
    "print (top2000_index_2)\n",
    "print (avg_weight_vector_2[top2000_index_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027988724790353886\n",
      "[-3.55220592e+08  1.09278129e+08 -2.61123680e+08 ... -4.55756319e+05\n",
      "  2.54604957e+06 -5.72904128e+04]\n",
      "[    1    11    10 ...  3537 12134 11862]\n",
      "[1.09278129e+08 7.41107208e+07 6.94411198e+07 ... 1.69495390e+06\n",
      " 1.69492387e+06 1.69119076e+06]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_5 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPGALACTOSE, test_size=0.2)\n",
    "    lr5 = linear_model.LinearRegression()\n",
    "    lr5.fit(x_train, y_train)\n",
    "    temp = np.array(lr5.coef_)\n",
    "    weight_vector_5 = [x + y for x, y in zip(weight_vector_5, temp)]\n",
    "    predictions_5 = lr5.predict(x_test)\n",
    "    pcc_5 = stat.pearsonr(predictions_5, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_5[0]\n",
    "\n",
    "avg_assoc_5 = sum/total_run\n",
    "avg_weight_vector_5 = np.divide(weight_vector_5, total_run)\n",
    "print(avg_assoc_5)\n",
    "print(avg_weight_vector_5)\n",
    "top2000_index_5 = (-avg_weight_vector_5).argsort()[:2000]\n",
    "print (top2000_index_5)\n",
    "print (avg_weight_vector_5[top2000_index_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01669042277109073\n",
      "[ 9.57655442e+07  8.30312598e+07 -5.45238087e+07 ... -6.39761225e+03\n",
      " -4.70708416e+06  2.03677672e+06]\n",
      "[    0     1     4 ... 13165 16181  7643]\n",
      "[95765544.23048124 83031259.82149577 63159836.09703083 ...\n",
      "  4184377.91238532  4183117.47498691  4180022.78792203]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_6 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPGLYCEROL, test_size=0.2)\n",
    "    lr6 = linear_model.LinearRegression()\n",
    "    lr6.fit(x_train, y_train)\n",
    "    temp = np.array(lr6.coef_)\n",
    "    weight_vector_6 = [x + y for x, y in zip(weight_vector_6, temp)]\n",
    "    predictions_6 = lr6.predict(x_test)\n",
    "    pcc_6 = stat.pearsonr(predictions_6, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_6[0]\n",
    "\n",
    "avg_assoc_6 = sum/total_run\n",
    "avg_weight_vector_6 = np.divide(weight_vector_6, total_run)\n",
    "print(avg_assoc_6)\n",
    "print(avg_weight_vector_6)\n",
    "top2000_index_6 = (-avg_weight_vector_6).argsort()[:2000]\n",
    "print (top2000_index_6)\n",
    "print (avg_weight_vector_6[top2000_index_6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012708775898420057\n",
      "[ 1.37363610e+08  2.55347170e+07 -2.27010896e+07 ... -5.95999093e+05\n",
      " -1.03600312e+04  1.72148497e+06]\n",
      "[    0     3    22 ...  9194  6502 15553]\n",
      "[1.37363610e+08 3.90485854e+07 3.55737953e+07 ... 1.04004830e+06\n",
      " 1.03954627e+06 1.03873070e+06]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_7 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPRIBOSE, test_size=0.2)\n",
    "    lr7 = linear_model.LinearRegression()\n",
    "    lr7.fit(x_train, y_train)\n",
    "    temp = np.array(lr7.coef_)\n",
    "    weight_vector_7 = [x + y for x, y in zip(weight_vector_7, temp)]\n",
    "    predictions_7 = lr7.predict(x_test)\n",
    "    pcc_7 = stat.pearsonr(predictions_7, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_7[0]\n",
    "\n",
    "avg_assoc_7 = sum/total_run\n",
    "avg_weight_vector_7 = np.divide(weight_vector_7, total_run)\n",
    "print(avg_assoc_7)\n",
    "print(avg_weight_vector_7)\n",
    "top2000_index_7 = (-avg_weight_vector_7).argsort()[:2000]\n",
    "print (top2000_index_7)\n",
    "print (avg_weight_vector_7[top2000_index_7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00658958047780311\n",
      "[-1.98248928e+08  2.49733290e+07 -2.84093445e+07 ...  6.25713982e+05\n",
      "  3.72645493e+05 -1.22592640e+06]\n",
      "[  14   18   19 ... 3716 2050 3460]\n",
      "[56802836.23059125 42934110.51236925 37816279.76193699 ...\n",
      "  2528406.61915241  2527849.51669522  2527272.76368826]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_8 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPSORBITOL, test_size=0.2)\n",
    "    lr8 = linear_model.LinearRegression()\n",
    "    lr8.fit(x_train, y_train)\n",
    "    temp = np.array(lr8.coef_)\n",
    "    weight_vector_8 = [x + y for x, y in zip(weight_vector_8, temp)]\n",
    "    predictions_8 = lr8.predict(x_test)\n",
    "    pcc_8 = stat.pearsonr(predictions_8, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_8[0]\n",
    "\n",
    "avg_assoc_8 = sum/total_run\n",
    "avg_weight_vector_8 = np.divide(weight_vector_8, total_run)\n",
    "print(avg_assoc_8)\n",
    "print(avg_weight_vector_8)\n",
    "top2000_index_8 = (-avg_weight_vector_8).argsort()[:2000]\n",
    "print (top2000_index_8)\n",
    "print (avg_weight_vector_8[top2000_index_8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = [\"YPD6AU\", \"YPDBENOMYL200\", \"YPDNACL1M\", \"YPDSDS\", \"YPGALACTOSE\", \"YPGLYCEROL\", \"YPRIBOSE\", \"YPSORBITOL\"]\n",
    "index_df = pd.DataFrame(list(zip(top2000_index, top2000_index_2, top2000_index_3, top2000_index_4, top2000_index_5, top2000_index_6, top2000_index_7, top2000_index_8)), columns = df_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一层hiddeen nueron 数量为一\n",
    "\n",
    "2000 1000 500 100 1 \n",
    "\n",
    "adam optimize 要\n",
    "\n",
    "batch normalization \n",
    "\n",
    ".cuda 挪到gpu\n",
    "\n",
    "batch training \n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as dt\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, h1, h2, h3, output_size, lr):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, h1)\n",
    "        self.l2 = nn.Linear(h1, h2)\n",
    "        self.l3 = nn.Linear(h2, h3)\n",
    "        self.l4 = nn.Linear(h3, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l4(out)\n",
    "        return out\n",
    "    \n",
    "    #Not used. \n",
    "    def train_net(self, iterations, train_data, target):\n",
    "        for epoch in range (iterations):\n",
    "            net.optimizer.zero_grad()\n",
    "            output = net.forward(train_data)\n",
    "            loss = net.loss_fn(output, target)\n",
    "            #print(list(self.parameters()))\n",
    "            print(\"current loss is: %f\"%loss)\n",
    "            loss.backward()\n",
    "            net.optimizer.step()\n",
    "            epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "input_size = 2000\n",
    "h1 = 1000\n",
    "h2 = 500\n",
    "h3 = 100\n",
    "output_size = 1\n",
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "traning_data = torch.tensor(snp_yeast_matrix[top2000_index].values, dtype = torch.float)\n",
    "answer = torch.tensor(YPD6AU, dtype = torch.float)\n",
    "train_set = dt.TensorDataset(traning_data, answer)\n",
    "train_loader = dt.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "net = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr)\n",
    "\n",
    "total_step = len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Charles\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:443: UserWarning: Using a target size (torch.Size([200])) that is different to the input size (torch.Size([200, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.0443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Charles\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:443: UserWarning: Using a target size (torch.Size([171])) that is different to the input size (torch.Size([171, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 0.0301\n",
      "Epoch [3/100], Loss: 0.0288\n",
      "Epoch [4/100], Loss: 0.0205\n",
      "Epoch [5/100], Loss: 0.0175\n",
      "Epoch [6/100], Loss: 0.0163\n",
      "Epoch [7/100], Loss: 0.0182\n",
      "Epoch [8/100], Loss: 0.0188\n",
      "Epoch [9/100], Loss: 0.0208\n",
      "Epoch [10/100], Loss: 0.0182\n",
      "Epoch [11/100], Loss: 0.0184\n",
      "Epoch [12/100], Loss: 0.0201\n",
      "Epoch [13/100], Loss: 0.0187\n",
      "Epoch [14/100], Loss: 0.0164\n",
      "Epoch [15/100], Loss: 0.0195\n",
      "Epoch [16/100], Loss: 0.0190\n",
      "Epoch [17/100], Loss: 0.0220\n",
      "Epoch [18/100], Loss: 0.0167\n",
      "Epoch [19/100], Loss: 0.0197\n",
      "Epoch [20/100], Loss: 0.0224\n",
      "Epoch [21/100], Loss: 0.0200\n",
      "Epoch [22/100], Loss: 0.0186\n",
      "Epoch [23/100], Loss: 0.0201\n",
      "Epoch [24/100], Loss: 0.0152\n",
      "Epoch [25/100], Loss: 0.0235\n",
      "Epoch [26/100], Loss: 0.0186\n",
      "Epoch [27/100], Loss: 0.0182\n",
      "Epoch [28/100], Loss: 0.0175\n",
      "Epoch [29/100], Loss: 0.0203\n",
      "Epoch [30/100], Loss: 0.0155\n",
      "Epoch [31/100], Loss: 0.0162\n",
      "Epoch [32/100], Loss: 0.0176\n",
      "Epoch [33/100], Loss: 0.0193\n",
      "Epoch [34/100], Loss: 0.0231\n",
      "Epoch [35/100], Loss: 0.0201\n",
      "Epoch [36/100], Loss: 0.0190\n",
      "Epoch [37/100], Loss: 0.0172\n",
      "Epoch [38/100], Loss: 0.0167\n",
      "Epoch [39/100], Loss: 0.0201\n",
      "Epoch [40/100], Loss: 0.0145\n",
      "Epoch [41/100], Loss: 0.0232\n",
      "Epoch [42/100], Loss: 0.0205\n",
      "Epoch [43/100], Loss: 0.0181\n",
      "Epoch [44/100], Loss: 0.0175\n",
      "Epoch [45/100], Loss: 0.0197\n",
      "Epoch [46/100], Loss: 0.0172\n",
      "Epoch [47/100], Loss: 0.0216\n",
      "Epoch [48/100], Loss: 0.0182\n",
      "Epoch [49/100], Loss: 0.0183\n",
      "Epoch [50/100], Loss: 0.0174\n",
      "Epoch [51/100], Loss: 0.0234\n",
      "Epoch [52/100], Loss: 0.0175\n",
      "Epoch [53/100], Loss: 0.0208\n",
      "Epoch [54/100], Loss: 0.0189\n",
      "Epoch [55/100], Loss: 0.0179\n",
      "Epoch [56/100], Loss: 0.0181\n",
      "Epoch [57/100], Loss: 0.0247\n",
      "Epoch [58/100], Loss: 0.0211\n",
      "Epoch [59/100], Loss: 0.0227\n",
      "Epoch [60/100], Loss: 0.0177\n",
      "Epoch [61/100], Loss: 0.0163\n",
      "Epoch [62/100], Loss: 0.0222\n",
      "Epoch [63/100], Loss: 0.0190\n",
      "Epoch [64/100], Loss: 0.0180\n",
      "Epoch [65/100], Loss: 0.0158\n",
      "Epoch [66/100], Loss: 0.0186\n",
      "Epoch [67/100], Loss: 0.0192\n",
      "Epoch [68/100], Loss: 0.0168\n",
      "Epoch [69/100], Loss: 0.0168\n",
      "Epoch [70/100], Loss: 0.0190\n",
      "Epoch [71/100], Loss: 0.0202\n",
      "Epoch [72/100], Loss: 0.0171\n",
      "Epoch [73/100], Loss: 0.0189\n",
      "Epoch [74/100], Loss: 0.0168\n",
      "Epoch [75/100], Loss: 0.0207\n",
      "Epoch [76/100], Loss: 0.0181\n",
      "Epoch [77/100], Loss: 0.0219\n",
      "Epoch [78/100], Loss: 0.0170\n",
      "Epoch [79/100], Loss: 0.0145\n",
      "Epoch [80/100], Loss: 0.0179\n",
      "Epoch [81/100], Loss: 0.0196\n",
      "Epoch [82/100], Loss: 0.0169\n",
      "Epoch [83/100], Loss: 0.0156\n",
      "Epoch [84/100], Loss: 0.0156\n",
      "Epoch [85/100], Loss: 0.0151\n",
      "Epoch [86/100], Loss: 0.0164\n",
      "Epoch [87/100], Loss: 0.0184\n",
      "Epoch [88/100], Loss: 0.0159\n",
      "Epoch [89/100], Loss: 0.0148\n",
      "Epoch [90/100], Loss: 0.0184\n",
      "Epoch [91/100], Loss: 0.0215\n",
      "Epoch [92/100], Loss: 0.0184\n",
      "Epoch [93/100], Loss: 0.0170\n",
      "Epoch [94/100], Loss: 0.0172\n",
      "Epoch [95/100], Loss: 0.0172\n",
      "Epoch [96/100], Loss: 0.0153\n",
      "Epoch [97/100], Loss: 0.0168\n",
      "Epoch [98/100], Loss: 0.0170\n",
      "Epoch [99/100], Loss: 0.0200\n",
      "Epoch [100/100], Loss: 0.0178\n"
     ]
    }
   ],
   "source": [
    "for epoch in range (num_epochs):\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        net.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = net.forward(data).to(device)\n",
    "        loss = net.loss_fn(output, target)\n",
    "\n",
    "        #print(list(self.parameters()))\n",
    "        #print(\"current loss is: %f\"%loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch += 1\n",
    "        \n",
    "        if (i) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "split = 0.8\n",
    "train_size = int(split * len(train_set))\n",
    "print(train_size)\n",
    "test_size = len(train_set) - train_size \n",
    "print(test_size)\n",
    "\n",
    "train_dataset, test_dataset = dt.random_split(train_set, [train_size, test_size])\n",
    "\n",
    "device = torch.device('cuda')\n",
    "input_size = 2000\n",
    "h1 = 1000\n",
    "h2 = 500\n",
    "h3 = 100\n",
    "output_size = 1\n",
    "lr = 0.00001\n",
    "num_epochs = 1000\n",
    "batch_size = 200\n",
    "train_loader = dt.DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
    "\n",
    "net = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr)\n",
    "test_loader = dt.DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
    "\n",
    "def check_correlation(test_loader, net):\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        total_err = 0.0\n",
    "        for (data, target) in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.cpu().numpy()\n",
    "            output = torch.squeeze(net(data)).cpu().numpy()\n",
    "            result = stat.pearsonr(target, output)\n",
    "    print(\"Correlation: %f \\n P-Value: %f\" %(result[0], result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: -0.242963 \n",
      " P-Value: 0.000621\n",
      "Epoch [1/1000], Loss: 0.1715\n",
      "Correlation: -0.219390 \n",
      " P-Value: 0.002059\n",
      "Epoch [2/1000], Loss: 0.1486\n",
      "Correlation: -0.210183 \n",
      " P-Value: 0.003186\n",
      "Epoch [3/1000], Loss: 0.1293\n",
      "Correlation: -0.207262 \n",
      " P-Value: 0.003646\n",
      "Epoch [4/1000], Loss: 0.1132\n",
      "Correlation: -0.202118 \n",
      " P-Value: 0.004603\n",
      "Epoch [5/1000], Loss: 0.0995\n",
      "Correlation: -0.198553 \n",
      " P-Value: 0.005393\n",
      "Epoch [6/1000], Loss: 0.0876\n",
      "Correlation: -0.196516 \n",
      " P-Value: 0.005897\n",
      "Epoch [7/1000], Loss: 0.0779\n",
      "Correlation: -0.195444 \n",
      " P-Value: 0.006179\n",
      "Epoch [8/1000], Loss: 0.0704\n",
      "Correlation: -0.194631 \n",
      " P-Value: 0.006400\n",
      "Epoch [9/1000], Loss: 0.0648\n",
      "Correlation: -0.193730 \n",
      " P-Value: 0.006655\n",
      "Epoch [10/1000], Loss: 0.0602\n",
      "Correlation: -0.191898 \n",
      " P-Value: 0.007199\n",
      "Epoch [11/1000], Loss: 0.0557\n",
      "Correlation: -0.188963 \n",
      " P-Value: 0.008154\n",
      "Epoch [12/1000], Loss: 0.0512\n",
      "Correlation: -0.184145 \n",
      " P-Value: 0.009966\n",
      "Epoch [13/1000], Loss: 0.0468\n",
      "Correlation: -0.176340 \n",
      " P-Value: 0.013664\n",
      "Epoch [14/1000], Loss: 0.0426\n",
      "Correlation: -0.165562 \n",
      " P-Value: 0.020719\n",
      "Epoch [15/1000], Loss: 0.0390\n",
      "Correlation: -0.151926 \n",
      " P-Value: 0.033988\n",
      "Epoch [16/1000], Loss: 0.0358\n",
      "Correlation: -0.135724 \n",
      " P-Value: 0.058509\n",
      "Epoch [17/1000], Loss: 0.0330\n",
      "Correlation: -0.117421 \n",
      " P-Value: 0.102089\n",
      "Epoch [18/1000], Loss: 0.0306\n",
      "Correlation: -0.097932 \n",
      " P-Value: 0.173190\n",
      "Epoch [19/1000], Loss: 0.0286\n",
      "Correlation: -0.077432 \n",
      " P-Value: 0.281955\n",
      "Epoch [20/1000], Loss: 0.0271\n",
      "Correlation: -0.056106 \n",
      " P-Value: 0.435948\n",
      "Epoch [21/1000], Loss: 0.0259\n",
      "Correlation: -0.035045 \n",
      " P-Value: 0.626695\n",
      "Epoch [22/1000], Loss: 0.0251\n",
      "Correlation: -0.015673 \n",
      " P-Value: 0.827841\n",
      "Epoch [23/1000], Loss: 0.0246\n",
      "Correlation: 0.000330 \n",
      " P-Value: 0.996351\n",
      "Epoch [24/1000], Loss: 0.0242\n",
      "Correlation: 0.011490 \n",
      " P-Value: 0.873339\n",
      "Epoch [25/1000], Loss: 0.0239\n",
      "Correlation: 0.016998 \n",
      " P-Value: 0.813549\n",
      "Epoch [26/1000], Loss: 0.0236\n",
      "Correlation: 0.017314 \n",
      " P-Value: 0.810148\n",
      "Epoch [27/1000], Loss: 0.0234\n",
      "Correlation: 0.013566 \n",
      " P-Value: 0.850696\n",
      "Epoch [28/1000], Loss: 0.0231\n",
      "Correlation: 0.007152 \n",
      " P-Value: 0.920958\n",
      "Epoch [29/1000], Loss: 0.0229\n",
      "Correlation: -0.000464 \n",
      " P-Value: 0.994868\n",
      "Epoch [30/1000], Loss: 0.0226\n",
      "Correlation: -0.008184 \n",
      " P-Value: 0.909594\n",
      "Epoch [31/1000], Loss: 0.0224\n",
      "Correlation: -0.015170 \n",
      " P-Value: 0.833284\n",
      "Epoch [32/1000], Loss: 0.0222\n",
      "Correlation: -0.021252 \n",
      " P-Value: 0.768071\n",
      "Epoch [33/1000], Loss: 0.0220\n",
      "Correlation: -0.026508 \n",
      " P-Value: 0.712982\n",
      "Epoch [34/1000], Loss: 0.0218\n",
      "Correlation: -0.030974 \n",
      " P-Value: 0.667302\n",
      "Epoch [35/1000], Loss: 0.0217\n",
      "Correlation: -0.034815 \n",
      " P-Value: 0.628962\n",
      "Epoch [36/1000], Loss: 0.0215\n",
      "Correlation: -0.038123 \n",
      " P-Value: 0.596719\n",
      "Epoch [37/1000], Loss: 0.0214\n",
      "Correlation: -0.040895 \n",
      " P-Value: 0.570282\n",
      "Epoch [38/1000], Loss: 0.0212\n",
      "Correlation: -0.043208 \n",
      " P-Value: 0.548665\n",
      "Epoch [39/1000], Loss: 0.0211\n",
      "Correlation: -0.045214 \n",
      " P-Value: 0.530243\n",
      "Epoch [40/1000], Loss: 0.0210\n",
      "Correlation: -0.046942 \n",
      " P-Value: 0.514622\n",
      "Epoch [41/1000], Loss: 0.0209\n",
      "Correlation: -0.048509 \n",
      " P-Value: 0.500668\n",
      "Epoch [42/1000], Loss: 0.0208\n",
      "Correlation: -0.050052 \n",
      " P-Value: 0.487133\n",
      "Epoch [43/1000], Loss: 0.0207\n",
      "Correlation: -0.051615 \n",
      " P-Value: 0.473617\n",
      "Epoch [44/1000], Loss: 0.0206\n",
      "Correlation: -0.053198 \n",
      " P-Value: 0.460138\n",
      "Epoch [45/1000], Loss: 0.0205\n",
      "Correlation: -0.054668 \n",
      " P-Value: 0.447822\n",
      "Epoch [46/1000], Loss: 0.0204\n",
      "Correlation: -0.056144 \n",
      " P-Value: 0.435634\n",
      "Epoch [47/1000], Loss: 0.0203\n",
      "Correlation: -0.057599 \n",
      " P-Value: 0.423816\n",
      "Epoch [48/1000], Loss: 0.0203\n",
      "Correlation: -0.058966 \n",
      " P-Value: 0.412884\n",
      "Epoch [49/1000], Loss: 0.0202\n",
      "Correlation: -0.060185 \n",
      " P-Value: 0.403272\n",
      "Epoch [50/1000], Loss: 0.0201\n",
      "Correlation: -0.061327 \n",
      " P-Value: 0.394392\n",
      "Epoch [51/1000], Loss: 0.0201\n",
      "Correlation: -0.062426 \n",
      " P-Value: 0.385959\n",
      "Epoch [52/1000], Loss: 0.0200\n",
      "Correlation: -0.063404 \n",
      " P-Value: 0.378543\n",
      "Epoch [53/1000], Loss: 0.0200\n",
      "Correlation: -0.064309 \n",
      " P-Value: 0.371763\n",
      "Epoch [54/1000], Loss: 0.0199\n",
      "Correlation: -0.065058 \n",
      " P-Value: 0.366205\n",
      "Epoch [55/1000], Loss: 0.0199\n",
      "Correlation: -0.065683 \n",
      " P-Value: 0.361607\n",
      "Epoch [56/1000], Loss: 0.0198\n",
      "Correlation: -0.066200 \n",
      " P-Value: 0.357833\n",
      "Epoch [57/1000], Loss: 0.0198\n",
      "Correlation: -0.066672 \n",
      " P-Value: 0.354410\n",
      "Epoch [58/1000], Loss: 0.0197\n",
      "Correlation: -0.067134 \n",
      " P-Value: 0.351078\n",
      "Epoch [59/1000], Loss: 0.0197\n",
      "Correlation: -0.067629 \n",
      " P-Value: 0.347529\n",
      "Epoch [60/1000], Loss: 0.0197\n",
      "Correlation: -0.068131 \n",
      " P-Value: 0.343954\n",
      "Epoch [61/1000], Loss: 0.0196\n",
      "Correlation: -0.068580 \n",
      " P-Value: 0.340775\n",
      "Epoch [62/1000], Loss: 0.0196\n",
      "Correlation: -0.068967 \n",
      " P-Value: 0.338049\n",
      "Epoch [63/1000], Loss: 0.0196\n",
      "Correlation: -0.069321 \n",
      " P-Value: 0.335573\n",
      "Epoch [64/1000], Loss: 0.0195\n",
      "Correlation: -0.069608 \n",
      " P-Value: 0.333569\n",
      "Epoch [65/1000], Loss: 0.0195\n",
      "Correlation: -0.069864 \n",
      " P-Value: 0.331792\n",
      "Epoch [66/1000], Loss: 0.0195\n",
      "Correlation: -0.070114 \n",
      " P-Value: 0.330059\n",
      "Epoch [67/1000], Loss: 0.0194\n",
      "Correlation: -0.070393 \n",
      " P-Value: 0.328137\n",
      "Epoch [68/1000], Loss: 0.0194\n",
      "Correlation: -0.070632 \n",
      " P-Value: 0.326489\n",
      "Epoch [69/1000], Loss: 0.0194\n",
      "Correlation: -0.070833 \n",
      " P-Value: 0.325112\n",
      "Epoch [70/1000], Loss: 0.0194\n",
      "Correlation: -0.070960 \n",
      " P-Value: 0.324240\n",
      "Epoch [71/1000], Loss: 0.0193\n",
      "Correlation: -0.071049 \n",
      " P-Value: 0.323636\n",
      "Epoch [72/1000], Loss: 0.0193\n",
      "Correlation: -0.071087 \n",
      " P-Value: 0.323377\n",
      "Epoch [73/1000], Loss: 0.0193\n",
      "Correlation: -0.071064 \n",
      " P-Value: 0.323535\n",
      "Epoch [74/1000], Loss: 0.0193\n",
      "Correlation: -0.071023 \n",
      " P-Value: 0.323811\n",
      "Epoch [75/1000], Loss: 0.0192\n",
      "Correlation: -0.070982 \n",
      " P-Value: 0.324091\n",
      "Epoch [76/1000], Loss: 0.0192\n",
      "Correlation: -0.070964 \n",
      " P-Value: 0.324216\n",
      "Epoch [77/1000], Loss: 0.0192\n",
      "Correlation: -0.071006 \n",
      " P-Value: 0.323926\n",
      "Epoch [78/1000], Loss: 0.0192\n",
      "Correlation: -0.071054 \n",
      " P-Value: 0.323603\n",
      "Epoch [79/1000], Loss: 0.0192\n",
      "Correlation: -0.071095 \n",
      " P-Value: 0.323319\n",
      "Epoch [80/1000], Loss: 0.0191\n",
      "Correlation: -0.071074 \n",
      " P-Value: 0.323463\n",
      "Epoch [81/1000], Loss: 0.0191\n",
      "Correlation: -0.070994 \n",
      " P-Value: 0.324010\n",
      "Epoch [82/1000], Loss: 0.0191\n",
      "Correlation: -0.070869 \n",
      " P-Value: 0.324863\n",
      "Epoch [83/1000], Loss: 0.0191\n",
      "Correlation: -0.070746 \n",
      " P-Value: 0.325711\n",
      "Epoch [84/1000], Loss: 0.0191\n",
      "Correlation: -0.070636 \n",
      " P-Value: 0.326467\n",
      "Epoch [85/1000], Loss: 0.0190\n",
      "Correlation: -0.070604 \n",
      " P-Value: 0.326681\n",
      "Epoch [86/1000], Loss: 0.0190\n",
      "Correlation: -0.070626 \n",
      " P-Value: 0.326528\n",
      "Epoch [87/1000], Loss: 0.0190\n",
      "Correlation: -0.070667 \n",
      " P-Value: 0.326247\n",
      "Epoch [88/1000], Loss: 0.0190\n",
      "Correlation: -0.070664 \n",
      " P-Value: 0.326269\n",
      "Epoch [89/1000], Loss: 0.0190\n",
      "Correlation: -0.070640 \n",
      " P-Value: 0.326433\n",
      "Epoch [90/1000], Loss: 0.0190\n",
      "Correlation: -0.070589 \n",
      " P-Value: 0.326789\n",
      "Epoch [91/1000], Loss: 0.0189\n",
      "Correlation: -0.070539 \n",
      " P-Value: 0.327133\n",
      "Epoch [92/1000], Loss: 0.0189\n",
      "Correlation: -0.070470 \n",
      " P-Value: 0.327605\n",
      "Epoch [93/1000], Loss: 0.0189\n",
      "Correlation: -0.070386 \n",
      " P-Value: 0.328185\n",
      "Epoch [94/1000], Loss: 0.0189\n",
      "Correlation: -0.070352 \n",
      " P-Value: 0.328417\n",
      "Epoch [95/1000], Loss: 0.0189\n",
      "Correlation: -0.070335 \n",
      " P-Value: 0.328533\n",
      "Epoch [96/1000], Loss: 0.0189\n",
      "Correlation: -0.070252 \n",
      " P-Value: 0.329109\n",
      "Epoch [97/1000], Loss: 0.0189\n",
      "Correlation: -0.070194 \n",
      " P-Value: 0.329510\n",
      "Epoch [98/1000], Loss: 0.0189\n",
      "Correlation: -0.070158 \n",
      " P-Value: 0.329754\n",
      "Epoch [99/1000], Loss: 0.0188\n",
      "Correlation: -0.070123 \n",
      " P-Value: 0.329998\n",
      "Epoch [100/1000], Loss: 0.0188\n",
      "Correlation: -0.070108 \n",
      " P-Value: 0.330099\n",
      "Epoch [101/1000], Loss: 0.0188\n",
      "Correlation: -0.070103 \n",
      " P-Value: 0.330139\n",
      "Epoch [102/1000], Loss: 0.0188\n",
      "Correlation: -0.070096 \n",
      " P-Value: 0.330186\n",
      "Epoch [103/1000], Loss: 0.0188\n",
      "Correlation: -0.070117 \n",
      " P-Value: 0.330040\n",
      "Epoch [104/1000], Loss: 0.0188\n",
      "Correlation: -0.070173 \n",
      " P-Value: 0.329653\n",
      "Epoch [105/1000], Loss: 0.0188\n",
      "Correlation: -0.070203 \n",
      " P-Value: 0.329443\n",
      "Epoch [106/1000], Loss: 0.0188\n",
      "Correlation: -0.070202 \n",
      " P-Value: 0.329455\n",
      "Epoch [107/1000], Loss: 0.0188\n",
      "Correlation: -0.070199 \n",
      " P-Value: 0.329471\n",
      "Epoch [108/1000], Loss: 0.0187\n",
      "Correlation: -0.070187 \n",
      " P-Value: 0.329556\n",
      "Epoch [109/1000], Loss: 0.0187\n",
      "Correlation: -0.070168 \n",
      " P-Value: 0.329686\n",
      "Epoch [110/1000], Loss: 0.0187\n",
      "Correlation: -0.070135 \n",
      " P-Value: 0.329913\n",
      "Epoch [111/1000], Loss: 0.0187\n",
      "Correlation: -0.070107 \n",
      " P-Value: 0.330108\n",
      "Epoch [112/1000], Loss: 0.0187\n",
      "Correlation: -0.070122 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " P-Value: 0.330007\n",
      "Epoch [113/1000], Loss: 0.0187\n",
      "Correlation: -0.070154 \n",
      " P-Value: 0.329781\n",
      "Epoch [114/1000], Loss: 0.0187\n",
      "Correlation: -0.070141 \n",
      " P-Value: 0.329870\n",
      "Epoch [115/1000], Loss: 0.0187\n",
      "Correlation: -0.070136 \n",
      " P-Value: 0.329905\n",
      "Epoch [116/1000], Loss: 0.0187\n",
      "Correlation: -0.070148 \n",
      " P-Value: 0.329825\n",
      "Epoch [117/1000], Loss: 0.0187\n",
      "Correlation: -0.070132 \n",
      " P-Value: 0.329936\n",
      "Epoch [118/1000], Loss: 0.0187\n",
      "Correlation: -0.070097 \n",
      " P-Value: 0.330180\n",
      "Epoch [119/1000], Loss: 0.0187\n",
      "Correlation: -0.070054 \n",
      " P-Value: 0.330478\n",
      "Epoch [120/1000], Loss: 0.0186\n",
      "Correlation: -0.070017 \n",
      " P-Value: 0.330735\n",
      "Epoch [121/1000], Loss: 0.0186\n",
      "Correlation: -0.069999 \n",
      " P-Value: 0.330853\n",
      "Epoch [122/1000], Loss: 0.0186\n",
      "Correlation: -0.070030 \n",
      " P-Value: 0.330638\n",
      "Epoch [123/1000], Loss: 0.0186\n",
      "Correlation: -0.070064 \n",
      " P-Value: 0.330408\n",
      "Epoch [124/1000], Loss: 0.0186\n",
      "Correlation: -0.070085 \n",
      " P-Value: 0.330260\n",
      "Epoch [125/1000], Loss: 0.0186\n",
      "Correlation: -0.070102 \n",
      " P-Value: 0.330145\n",
      "Epoch [126/1000], Loss: 0.0186\n",
      "Correlation: -0.070128 \n",
      " P-Value: 0.329965\n",
      "Epoch [127/1000], Loss: 0.0186\n",
      "Correlation: -0.070165 \n",
      " P-Value: 0.329705\n",
      "Epoch [128/1000], Loss: 0.0186\n",
      "Correlation: -0.070230 \n",
      " P-Value: 0.329261\n",
      "Epoch [129/1000], Loss: 0.0186\n",
      "Correlation: -0.070320 \n",
      " P-Value: 0.328634\n",
      "Epoch [130/1000], Loss: 0.0186\n",
      "Correlation: -0.070385 \n",
      " P-Value: 0.328186\n",
      "Epoch [131/1000], Loss: 0.0186\n",
      "Correlation: -0.070415 \n",
      " P-Value: 0.327983\n",
      "Epoch [132/1000], Loss: 0.0186\n",
      "Correlation: -0.070440 \n",
      " P-Value: 0.327809\n",
      "Epoch [133/1000], Loss: 0.0186\n",
      "Correlation: -0.070501 \n",
      " P-Value: 0.327393\n",
      "Epoch [134/1000], Loss: 0.0186\n",
      "Correlation: -0.070536 \n",
      " P-Value: 0.327152\n",
      "Epoch [135/1000], Loss: 0.0186\n",
      "Correlation: -0.070547 \n",
      " P-Value: 0.327072\n",
      "Epoch [136/1000], Loss: 0.0186\n",
      "Correlation: -0.070565 \n",
      " P-Value: 0.326952\n",
      "Epoch [137/1000], Loss: 0.0185\n",
      "Correlation: -0.070595 \n",
      " P-Value: 0.326744\n",
      "Epoch [138/1000], Loss: 0.0185\n",
      "Correlation: -0.070593 \n",
      " P-Value: 0.326760\n",
      "Epoch [139/1000], Loss: 0.0185\n",
      "Correlation: -0.070581 \n",
      " P-Value: 0.326841\n",
      "Epoch [140/1000], Loss: 0.0185\n",
      "Correlation: -0.070585 \n",
      " P-Value: 0.326813\n",
      "Epoch [141/1000], Loss: 0.0185\n",
      "Correlation: -0.070625 \n",
      " P-Value: 0.326542\n",
      "Epoch [142/1000], Loss: 0.0185\n",
      "Correlation: -0.070656 \n",
      " P-Value: 0.326328\n",
      "Epoch [143/1000], Loss: 0.0185\n",
      "Correlation: -0.070651 \n",
      " P-Value: 0.326358\n",
      "Epoch [144/1000], Loss: 0.0185\n",
      "Correlation: -0.070608 \n",
      " P-Value: 0.326653\n",
      "Epoch [145/1000], Loss: 0.0185\n",
      "Correlation: -0.070563 \n",
      " P-Value: 0.326966\n",
      "Epoch [146/1000], Loss: 0.0185\n",
      "Correlation: -0.070514 \n",
      " P-Value: 0.327302\n",
      "Epoch [147/1000], Loss: 0.0185\n",
      "Correlation: -0.070440 \n",
      " P-Value: 0.327808\n",
      "Epoch [148/1000], Loss: 0.0185\n",
      "Correlation: -0.070368 \n",
      " P-Value: 0.328310\n",
      "Epoch [149/1000], Loss: 0.0185\n",
      "Correlation: -0.070324 \n",
      " P-Value: 0.328612\n",
      "Epoch [150/1000], Loss: 0.0185\n",
      "Correlation: -0.070298 \n",
      " P-Value: 0.328788\n",
      "Epoch [151/1000], Loss: 0.0185\n",
      "Correlation: -0.070226 \n",
      " P-Value: 0.329288\n",
      "Epoch [152/1000], Loss: 0.0185\n",
      "Correlation: -0.070147 \n",
      " P-Value: 0.329832\n",
      "Epoch [153/1000], Loss: 0.0185\n",
      "Correlation: -0.070111 \n",
      " P-Value: 0.330082\n",
      "Epoch [154/1000], Loss: 0.0185\n",
      "Correlation: -0.070061 \n",
      " P-Value: 0.330429\n",
      "Epoch [155/1000], Loss: 0.0185\n",
      "Correlation: -0.069985 \n",
      " P-Value: 0.330954\n",
      "Epoch [156/1000], Loss: 0.0185\n",
      "Correlation: -0.069926 \n",
      " P-Value: 0.331365\n",
      "Epoch [157/1000], Loss: 0.0185\n",
      "Correlation: -0.069853 \n",
      " P-Value: 0.331871\n",
      "Epoch [158/1000], Loss: 0.0185\n",
      "Correlation: -0.069771 \n",
      " P-Value: 0.332434\n",
      "Epoch [159/1000], Loss: 0.0185\n",
      "Correlation: -0.069667 \n",
      " P-Value: 0.333160\n",
      "Epoch [160/1000], Loss: 0.0185\n",
      "Correlation: -0.069577 \n",
      " P-Value: 0.333788\n",
      "Epoch [161/1000], Loss: 0.0185\n",
      "Correlation: -0.069480 \n",
      " P-Value: 0.334461\n",
      "Epoch [162/1000], Loss: 0.0185\n",
      "Correlation: -0.069370 \n",
      " P-Value: 0.335227\n",
      "Epoch [163/1000], Loss: 0.0185\n",
      "Correlation: -0.069278 \n",
      " P-Value: 0.335875\n",
      "Epoch [164/1000], Loss: 0.0184\n",
      "Correlation: -0.069180 \n",
      " P-Value: 0.336558\n",
      "Epoch [165/1000], Loss: 0.0184\n",
      "Correlation: -0.069077 \n",
      " P-Value: 0.337277\n",
      "Epoch [166/1000], Loss: 0.0184\n",
      "Correlation: -0.068994 \n",
      " P-Value: 0.337864\n",
      "Epoch [167/1000], Loss: 0.0184\n",
      "Correlation: -0.068892 \n",
      " P-Value: 0.338577\n",
      "Epoch [168/1000], Loss: 0.0184\n",
      "Correlation: -0.068796 \n",
      " P-Value: 0.339255\n",
      "Epoch [169/1000], Loss: 0.0184\n",
      "Correlation: -0.068718 \n",
      " P-Value: 0.339805\n",
      "Epoch [170/1000], Loss: 0.0184\n",
      "Correlation: -0.068619 \n",
      " P-Value: 0.340498\n",
      "Epoch [171/1000], Loss: 0.0184\n",
      "Correlation: -0.068507 \n",
      " P-Value: 0.341293\n",
      "Epoch [172/1000], Loss: 0.0184\n",
      "Correlation: -0.068393 \n",
      " P-Value: 0.342101\n",
      "Epoch [173/1000], Loss: 0.0184\n",
      "Correlation: -0.068303 \n",
      " P-Value: 0.342731\n",
      "Epoch [174/1000], Loss: 0.0184\n",
      "Correlation: -0.068219 \n",
      " P-Value: 0.343330\n",
      "Epoch [175/1000], Loss: 0.0184\n",
      "Correlation: -0.068127 \n",
      " P-Value: 0.343982\n",
      "Epoch [176/1000], Loss: 0.0184\n",
      "Correlation: -0.068009 \n",
      " P-Value: 0.344824\n",
      "Epoch [177/1000], Loss: 0.0184\n",
      "Correlation: -0.067888 \n",
      " P-Value: 0.345684\n",
      "Epoch [178/1000], Loss: 0.0184\n",
      "Correlation: -0.067764 \n",
      " P-Value: 0.346563\n",
      "Epoch [179/1000], Loss: 0.0184\n",
      "Correlation: -0.067681 \n",
      " P-Value: 0.347159\n",
      "Epoch [180/1000], Loss: 0.0184\n",
      "Correlation: -0.067581 \n",
      " P-Value: 0.347876\n",
      "Epoch [181/1000], Loss: 0.0184\n",
      "Correlation: -0.067442 \n",
      " P-Value: 0.348866\n",
      "Epoch [182/1000], Loss: 0.0184\n",
      "Correlation: -0.067312 \n",
      " P-Value: 0.349798\n",
      "Epoch [183/1000], Loss: 0.0184\n",
      "Correlation: -0.067205 \n",
      " P-Value: 0.350569\n",
      "Epoch [184/1000], Loss: 0.0184\n",
      "Correlation: -0.067082 \n",
      " P-Value: 0.351451\n",
      "Epoch [185/1000], Loss: 0.0184\n",
      "Correlation: -0.066933 \n",
      " P-Value: 0.352526\n",
      "Epoch [186/1000], Loss: 0.0184\n",
      "Correlation: -0.066773 \n",
      " P-Value: 0.353678\n",
      "Epoch [187/1000], Loss: 0.0184\n",
      "Correlation: -0.066633 \n",
      " P-Value: 0.354691\n",
      "Epoch [188/1000], Loss: 0.0184\n",
      "Correlation: -0.066517 \n",
      " P-Value: 0.355531\n",
      "Epoch [189/1000], Loss: 0.0184\n",
      "Correlation: -0.066397 \n",
      " P-Value: 0.356400\n",
      "Epoch [190/1000], Loss: 0.0184\n",
      "Correlation: -0.066256 \n",
      " P-Value: 0.357428\n",
      "Epoch [191/1000], Loss: 0.0184\n",
      "Correlation: -0.066099 \n",
      " P-Value: 0.358571\n",
      "Epoch [192/1000], Loss: 0.0184\n",
      "Correlation: -0.065968 \n",
      " P-Value: 0.359525\n",
      "Epoch [193/1000], Loss: 0.0184\n",
      "Correlation: -0.065847 \n",
      " P-Value: 0.360413\n",
      "Epoch [194/1000], Loss: 0.0184\n",
      "Correlation: -0.065711 \n",
      " P-Value: 0.361403\n",
      "Epoch [195/1000], Loss: 0.0184\n",
      "Correlation: -0.065556 \n",
      " P-Value: 0.362538\n",
      "Epoch [196/1000], Loss: 0.0184\n",
      "Correlation: -0.065400 \n",
      " P-Value: 0.363686\n",
      "Epoch [197/1000], Loss: 0.0184\n",
      "Correlation: -0.065263 \n",
      " P-Value: 0.364691\n",
      "Epoch [198/1000], Loss: 0.0184\n",
      "Correlation: -0.065146 \n",
      " P-Value: 0.365555\n",
      "Epoch [199/1000], Loss: 0.0184\n",
      "Correlation: -0.065009 \n",
      " P-Value: 0.366565\n",
      "Epoch [200/1000], Loss: 0.0184\n",
      "Correlation: -0.064839 \n",
      " P-Value: 0.367827\n",
      "Epoch [201/1000], Loss: 0.0184\n",
      "Correlation: -0.064680 \n",
      " P-Value: 0.369003\n",
      "Epoch [202/1000], Loss: 0.0184\n",
      "Correlation: -0.064542 \n",
      " P-Value: 0.370029\n",
      "Epoch [203/1000], Loss: 0.0184\n",
      "Correlation: -0.064376 \n",
      " P-Value: 0.371262\n",
      "Epoch [204/1000], Loss: 0.0184\n",
      "Correlation: -0.064247 \n",
      " P-Value: 0.372224\n",
      "Epoch [205/1000], Loss: 0.0184\n",
      "Correlation: -0.064144 \n",
      " P-Value: 0.372995\n",
      "Epoch [206/1000], Loss: 0.0184\n",
      "Correlation: -0.064034 \n",
      " P-Value: 0.373814\n",
      "Epoch [207/1000], Loss: 0.0184\n",
      "Correlation: -0.063901 \n",
      " P-Value: 0.374808\n",
      "Epoch [208/1000], Loss: 0.0184\n",
      "Correlation: -0.063739 \n",
      " P-Value: 0.376024\n",
      "Epoch [209/1000], Loss: 0.0184\n",
      "Correlation: -0.063606 \n",
      " P-Value: 0.377020\n",
      "Epoch [210/1000], Loss: 0.0184\n",
      "Correlation: -0.063506 \n",
      " P-Value: 0.377772\n",
      "Epoch [211/1000], Loss: 0.0184\n",
      "Correlation: -0.063427 \n",
      " P-Value: 0.378369\n",
      "Epoch [212/1000], Loss: 0.0184\n",
      "Correlation: -0.063305 \n",
      " P-Value: 0.379292\n",
      "Epoch [213/1000], Loss: 0.0184\n",
      "Correlation: -0.063161 \n",
      " P-Value: 0.380380\n",
      "Epoch [214/1000], Loss: 0.0184\n",
      "Correlation: -0.063037 \n",
      " P-Value: 0.381314\n",
      "Epoch [215/1000], Loss: 0.0184\n",
      "Correlation: -0.062922 \n",
      " P-Value: 0.382183\n",
      "Epoch [216/1000], Loss: 0.0184\n",
      "Correlation: -0.062803 \n",
      " P-Value: 0.383088\n",
      "Epoch [217/1000], Loss: 0.0184\n",
      "Correlation: -0.062669 \n",
      " P-Value: 0.384106\n",
      "Epoch [218/1000], Loss: 0.0184\n",
      "Correlation: -0.062517 \n",
      " P-Value: 0.385264\n",
      "Epoch [219/1000], Loss: 0.0184\n",
      "Correlation: -0.062377 \n",
      " P-Value: 0.386331\n",
      "Epoch [220/1000], Loss: 0.0184\n",
      "Correlation: -0.062274 \n",
      " P-Value: 0.387119\n",
      "Epoch [221/1000], Loss: 0.0184\n",
      "Correlation: -0.062145 \n",
      " P-Value: 0.388101\n",
      "Epoch [222/1000], Loss: 0.0184\n",
      "Correlation: -0.062041 \n",
      " P-Value: 0.388897\n",
      "Epoch [223/1000], Loss: 0.0184\n",
      "Correlation: -0.061908 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " P-Value: 0.389918\n",
      "Epoch [224/1000], Loss: 0.0184\n",
      "Correlation: -0.061769 \n",
      " P-Value: 0.390989\n",
      "Epoch [225/1000], Loss: 0.0184\n",
      "Correlation: -0.061645 \n",
      " P-Value: 0.391942\n",
      "Epoch [226/1000], Loss: 0.0184\n",
      "Correlation: -0.061492 \n",
      " P-Value: 0.393121\n",
      "Epoch [227/1000], Loss: 0.0183\n",
      "Correlation: -0.061343 \n",
      " P-Value: 0.394265\n",
      "Epoch [228/1000], Loss: 0.0183\n",
      "Correlation: -0.061220 \n",
      " P-Value: 0.395219\n",
      "Epoch [229/1000], Loss: 0.0183\n",
      "Correlation: -0.061117 \n",
      " P-Value: 0.396013\n",
      "Epoch [230/1000], Loss: 0.0183\n",
      "Correlation: -0.060966 \n",
      " P-Value: 0.397186\n",
      "Epoch [231/1000], Loss: 0.0183\n",
      "Correlation: -0.060848 \n",
      " P-Value: 0.398098\n",
      "Epoch [232/1000], Loss: 0.0183\n",
      "Correlation: -0.060739 \n",
      " P-Value: 0.398947\n",
      "Epoch [233/1000], Loss: 0.0183\n",
      "Correlation: -0.060610 \n",
      " P-Value: 0.399950\n",
      "Epoch [234/1000], Loss: 0.0183\n",
      "Correlation: -0.060465 \n",
      " P-Value: 0.401085\n",
      "Epoch [235/1000], Loss: 0.0183\n",
      "Correlation: -0.060356 \n",
      " P-Value: 0.401935\n",
      "Epoch [236/1000], Loss: 0.0183\n",
      "Correlation: -0.060254 \n",
      " P-Value: 0.402731\n",
      "Epoch [237/1000], Loss: 0.0183\n",
      "Correlation: -0.060141 \n",
      " P-Value: 0.403614\n",
      "Epoch [238/1000], Loss: 0.0183\n",
      "Correlation: -0.060015 \n",
      " P-Value: 0.404606\n",
      "Epoch [239/1000], Loss: 0.0183\n",
      "Correlation: -0.059893 \n",
      " P-Value: 0.405562\n",
      "Epoch [240/1000], Loss: 0.0183\n",
      "Correlation: -0.059746 \n",
      " P-Value: 0.406715\n",
      "Epoch [241/1000], Loss: 0.0183\n",
      "Correlation: -0.059615 \n",
      " P-Value: 0.407750\n",
      "Epoch [242/1000], Loss: 0.0183\n",
      "Correlation: -0.059544 \n",
      " P-Value: 0.408307\n",
      "Epoch [243/1000], Loss: 0.0183\n",
      "Correlation: -0.059477 \n",
      " P-Value: 0.408838\n",
      "Epoch [244/1000], Loss: 0.0183\n",
      "Correlation: -0.059363 \n",
      " P-Value: 0.409734\n",
      "Epoch [245/1000], Loss: 0.0183\n",
      "Correlation: -0.059218 \n",
      " P-Value: 0.410885\n",
      "Epoch [246/1000], Loss: 0.0183\n",
      "Correlation: -0.059041 \n",
      " P-Value: 0.412284\n",
      "Epoch [247/1000], Loss: 0.0183\n",
      "Correlation: -0.058951 \n",
      " P-Value: 0.413002\n",
      "Epoch [248/1000], Loss: 0.0183\n",
      "Correlation: -0.058887 \n",
      " P-Value: 0.413510\n",
      "Epoch [249/1000], Loss: 0.0183\n",
      "Correlation: -0.058757 \n",
      " P-Value: 0.414543\n",
      "Epoch [250/1000], Loss: 0.0183\n",
      "Correlation: -0.058638 \n",
      " P-Value: 0.415488\n",
      "Epoch [251/1000], Loss: 0.0183\n",
      "Correlation: -0.058530 \n",
      " P-Value: 0.416353\n",
      "Epoch [252/1000], Loss: 0.0183\n",
      "Correlation: -0.058471 \n",
      " P-Value: 0.416822\n",
      "Epoch [253/1000], Loss: 0.0183\n",
      "Correlation: -0.058378 \n",
      " P-Value: 0.417563\n",
      "Epoch [254/1000], Loss: 0.0183\n",
      "Correlation: -0.058247 \n",
      " P-Value: 0.418612\n",
      "Epoch [255/1000], Loss: 0.0183\n",
      "Correlation: -0.058143 \n",
      " P-Value: 0.419447\n",
      "Epoch [256/1000], Loss: 0.0183\n",
      "Correlation: -0.058021 \n",
      " P-Value: 0.420421\n",
      "Epoch [257/1000], Loss: 0.0183\n",
      "Correlation: -0.057891 \n",
      " P-Value: 0.421463\n",
      "Epoch [258/1000], Loss: 0.0183\n",
      "Correlation: -0.057808 \n",
      " P-Value: 0.422135\n",
      "Epoch [259/1000], Loss: 0.0183\n",
      "Correlation: -0.057703 \n",
      " P-Value: 0.422975\n",
      "Epoch [260/1000], Loss: 0.0183\n",
      "Correlation: -0.057619 \n",
      " P-Value: 0.423650\n",
      "Epoch [261/1000], Loss: 0.0183\n",
      "Correlation: -0.057541 \n",
      " P-Value: 0.424283\n",
      "Epoch [262/1000], Loss: 0.0183\n",
      "Correlation: -0.057403 \n",
      " P-Value: 0.425401\n",
      "Epoch [263/1000], Loss: 0.0183\n",
      "Correlation: -0.057281 \n",
      " P-Value: 0.426386\n",
      "Epoch [264/1000], Loss: 0.0183\n",
      "Correlation: -0.057175 \n",
      " P-Value: 0.427238\n",
      "Epoch [265/1000], Loss: 0.0183\n",
      "Correlation: -0.057059 \n",
      " P-Value: 0.428184\n",
      "Epoch [266/1000], Loss: 0.0183\n",
      "Correlation: -0.056948 \n",
      " P-Value: 0.429084\n",
      "Epoch [267/1000], Loss: 0.0183\n",
      "Correlation: -0.056866 \n",
      " P-Value: 0.429745\n",
      "Epoch [268/1000], Loss: 0.0183\n",
      "Correlation: -0.056743 \n",
      " P-Value: 0.430750\n",
      "Epoch [269/1000], Loss: 0.0183\n",
      "Correlation: -0.056649 \n",
      " P-Value: 0.431515\n",
      "Epoch [270/1000], Loss: 0.0183\n",
      "Correlation: -0.056580 \n",
      " P-Value: 0.432077\n",
      "Epoch [271/1000], Loss: 0.0183\n",
      "Correlation: -0.056446 \n",
      " P-Value: 0.433169\n",
      "Epoch [272/1000], Loss: 0.0183\n",
      "Correlation: -0.056344 \n",
      " P-Value: 0.434002\n",
      "Epoch [273/1000], Loss: 0.0183\n",
      "Correlation: -0.056239 \n",
      " P-Value: 0.434856\n",
      "Epoch [274/1000], Loss: 0.0183\n",
      "Correlation: -0.056182 \n",
      " P-Value: 0.435324\n",
      "Epoch [275/1000], Loss: 0.0183\n",
      "Correlation: -0.056094 \n",
      " P-Value: 0.436047\n",
      "Epoch [276/1000], Loss: 0.0183\n",
      "Correlation: -0.055945 \n",
      " P-Value: 0.437268\n",
      "Epoch [277/1000], Loss: 0.0183\n",
      "Correlation: -0.055837 \n",
      " P-Value: 0.438155\n",
      "Epoch [278/1000], Loss: 0.0183\n",
      "Correlation: -0.055777 \n",
      " P-Value: 0.438651\n",
      "Epoch [279/1000], Loss: 0.0183\n",
      "Correlation: -0.055649 \n",
      " P-Value: 0.439702\n",
      "Epoch [280/1000], Loss: 0.0183\n",
      "Correlation: -0.055557 \n",
      " P-Value: 0.440454\n",
      "Epoch [281/1000], Loss: 0.0183\n",
      "Correlation: -0.055502 \n",
      " P-Value: 0.440909\n",
      "Epoch [282/1000], Loss: 0.0183\n",
      "Correlation: -0.055420 \n",
      " P-Value: 0.441587\n",
      "Epoch [283/1000], Loss: 0.0183\n",
      "Correlation: -0.055351 \n",
      " P-Value: 0.442156\n",
      "Epoch [284/1000], Loss: 0.0183\n",
      "Correlation: -0.055264 \n",
      " P-Value: 0.442879\n",
      "Epoch [285/1000], Loss: 0.0183\n",
      "Correlation: -0.055121 \n",
      " P-Value: 0.444062\n",
      "Epoch [286/1000], Loss: 0.0183\n",
      "Correlation: -0.055020 \n",
      " P-Value: 0.444894\n",
      "Epoch [287/1000], Loss: 0.0183\n",
      "Correlation: -0.054981 \n",
      " P-Value: 0.445223\n",
      "Epoch [288/1000], Loss: 0.0183\n",
      "Correlation: -0.054937 \n",
      " P-Value: 0.445582\n",
      "Epoch [289/1000], Loss: 0.0183\n",
      "Correlation: -0.054846 \n",
      " P-Value: 0.446338\n",
      "Epoch [290/1000], Loss: 0.0183\n",
      "Correlation: -0.054743 \n",
      " P-Value: 0.447198\n",
      "Epoch [291/1000], Loss: 0.0183\n",
      "Correlation: -0.054674 \n",
      " P-Value: 0.447767\n",
      "Epoch [292/1000], Loss: 0.0183\n",
      "Correlation: -0.054592 \n",
      " P-Value: 0.448449\n",
      "Epoch [293/1000], Loss: 0.0183\n",
      "Correlation: -0.054486 \n",
      " P-Value: 0.449335\n",
      "Epoch [294/1000], Loss: 0.0183\n",
      "Correlation: -0.054395 \n",
      " P-Value: 0.450094\n",
      "Epoch [295/1000], Loss: 0.0183\n",
      "Correlation: -0.054307 \n",
      " P-Value: 0.450830\n",
      "Epoch [296/1000], Loss: 0.0183\n",
      "Correlation: -0.054227 \n",
      " P-Value: 0.451493\n",
      "Epoch [297/1000], Loss: 0.0183\n",
      "Correlation: -0.054117 \n",
      " P-Value: 0.452411\n",
      "Epoch [298/1000], Loss: 0.0183\n",
      "Correlation: -0.053985 \n",
      " P-Value: 0.453517\n",
      "Epoch [299/1000], Loss: 0.0183\n",
      "Correlation: -0.053894 \n",
      " P-Value: 0.454279\n",
      "Epoch [300/1000], Loss: 0.0183\n",
      "Correlation: -0.053796 \n",
      " P-Value: 0.455104\n",
      "Epoch [301/1000], Loss: 0.0183\n",
      "Correlation: -0.053687 \n",
      " P-Value: 0.456021\n",
      "Epoch [302/1000], Loss: 0.0183\n",
      "Correlation: -0.053623 \n",
      " P-Value: 0.456561\n",
      "Epoch [303/1000], Loss: 0.0183\n",
      "Correlation: -0.053534 \n",
      " P-Value: 0.457310\n",
      "Epoch [304/1000], Loss: 0.0183\n",
      "Correlation: -0.053447 \n",
      " P-Value: 0.458038\n",
      "Epoch [305/1000], Loss: 0.0183\n",
      "Correlation: -0.053367 \n",
      " P-Value: 0.458717\n",
      "Epoch [306/1000], Loss: 0.0183\n",
      "Correlation: -0.053288 \n",
      " P-Value: 0.459379\n",
      "Epoch [307/1000], Loss: 0.0183\n",
      "Correlation: -0.053184 \n",
      " P-Value: 0.460257\n",
      "Epoch [308/1000], Loss: 0.0183\n",
      "Correlation: -0.053115 \n",
      " P-Value: 0.460842\n",
      "Epoch [309/1000], Loss: 0.0183\n",
      "Correlation: -0.053082 \n",
      " P-Value: 0.461124\n",
      "Epoch [310/1000], Loss: 0.0183\n",
      "Correlation: -0.052964 \n",
      " P-Value: 0.462119\n",
      "Epoch [311/1000], Loss: 0.0183\n",
      "Correlation: -0.052869 \n",
      " P-Value: 0.462923\n",
      "Epoch [312/1000], Loss: 0.0183\n",
      "Correlation: -0.052811 \n",
      " P-Value: 0.463414\n",
      "Epoch [313/1000], Loss: 0.0183\n",
      "Correlation: -0.052700 \n",
      " P-Value: 0.464359\n",
      "Epoch [314/1000], Loss: 0.0183\n",
      "Correlation: -0.052589 \n",
      " P-Value: 0.465301\n",
      "Epoch [315/1000], Loss: 0.0183\n",
      "Correlation: -0.052522 \n",
      " P-Value: 0.465864\n",
      "Epoch [316/1000], Loss: 0.0183\n",
      "Correlation: -0.052466 \n",
      " P-Value: 0.466342\n",
      "Epoch [317/1000], Loss: 0.0183\n",
      "Correlation: -0.052366 \n",
      " P-Value: 0.467197\n",
      "Epoch [318/1000], Loss: 0.0183\n",
      "Correlation: -0.052294 \n",
      " P-Value: 0.467806\n",
      "Epoch [319/1000], Loss: 0.0183\n",
      "Correlation: -0.052220 \n",
      " P-Value: 0.468439\n",
      "Epoch [320/1000], Loss: 0.0183\n",
      "Correlation: -0.052155 \n",
      " P-Value: 0.468997\n",
      "Epoch [321/1000], Loss: 0.0183\n",
      "Correlation: -0.052072 \n",
      " P-Value: 0.469700\n",
      "Epoch [322/1000], Loss: 0.0183\n",
      "Correlation: -0.051979 \n",
      " P-Value: 0.470499\n",
      "Epoch [323/1000], Loss: 0.0183\n",
      "Correlation: -0.051910 \n",
      " P-Value: 0.471090\n",
      "Epoch [324/1000], Loss: 0.0183\n",
      "Correlation: -0.051857 \n",
      " P-Value: 0.471546\n",
      "Epoch [325/1000], Loss: 0.0183\n",
      "Correlation: -0.051753 \n",
      " P-Value: 0.472435\n",
      "Epoch [326/1000], Loss: 0.0183\n",
      "Correlation: -0.051696 \n",
      " P-Value: 0.472921\n",
      "Epoch [327/1000], Loss: 0.0183\n",
      "Correlation: -0.051699 \n",
      " P-Value: 0.472894\n",
      "Epoch [328/1000], Loss: 0.0183\n",
      "Correlation: -0.051646 \n",
      " P-Value: 0.473352\n",
      "Epoch [329/1000], Loss: 0.0183\n",
      "Correlation: -0.051528 \n",
      " P-Value: 0.474361\n",
      "Epoch [330/1000], Loss: 0.0183\n",
      "Correlation: -0.051456 \n",
      " P-Value: 0.474985\n",
      "Epoch [331/1000], Loss: 0.0183\n",
      "Correlation: -0.051403 \n",
      " P-Value: 0.475433\n",
      "Epoch [332/1000], Loss: 0.0183\n",
      "Correlation: -0.051386 \n",
      " P-Value: 0.475586\n",
      "Epoch [333/1000], Loss: 0.0183\n",
      "Correlation: -0.051292 \n",
      " P-Value: 0.476391\n",
      "Epoch [334/1000], Loss: 0.0183\n",
      "Correlation: -0.051209 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " P-Value: 0.477110\n",
      "Epoch [335/1000], Loss: 0.0183\n",
      "Correlation: -0.051204 \n",
      " P-Value: 0.477147\n",
      "Epoch [336/1000], Loss: 0.0183\n",
      "Correlation: -0.051174 \n",
      " P-Value: 0.477412\n",
      "Epoch [337/1000], Loss: 0.0183\n",
      "Correlation: -0.051083 \n",
      " P-Value: 0.478196\n",
      "Epoch [338/1000], Loss: 0.0183\n",
      "Correlation: -0.051030 \n",
      " P-Value: 0.478650\n",
      "Epoch [339/1000], Loss: 0.0183\n",
      "Correlation: -0.051030 \n",
      " P-Value: 0.478646\n",
      "Epoch [340/1000], Loss: 0.0183\n",
      "Correlation: -0.050944 \n",
      " P-Value: 0.479392\n",
      "Epoch [341/1000], Loss: 0.0183\n",
      "Correlation: -0.050901 \n",
      " P-Value: 0.479768\n",
      "Epoch [342/1000], Loss: 0.0183\n",
      "Correlation: -0.050857 \n",
      " P-Value: 0.480141\n",
      "Epoch [343/1000], Loss: 0.0183\n",
      "Correlation: -0.050816 \n",
      " P-Value: 0.480496\n",
      "Epoch [344/1000], Loss: 0.0183\n",
      "Correlation: -0.050765 \n",
      " P-Value: 0.480937\n",
      "Epoch [345/1000], Loss: 0.0183\n",
      "Correlation: -0.050727 \n",
      " P-Value: 0.481265\n",
      "Epoch [346/1000], Loss: 0.0183\n",
      "Correlation: -0.050652 \n",
      " P-Value: 0.481921\n",
      "Epoch [347/1000], Loss: 0.0183\n",
      "Correlation: -0.050633 \n",
      " P-Value: 0.482087\n",
      "Epoch [348/1000], Loss: 0.0183\n",
      "Correlation: -0.050612 \n",
      " P-Value: 0.482266\n",
      "Epoch [349/1000], Loss: 0.0183\n",
      "Correlation: -0.050531 \n",
      " P-Value: 0.482969\n",
      "Epoch [350/1000], Loss: 0.0183\n",
      "Correlation: -0.050477 \n",
      " P-Value: 0.483434\n",
      "Epoch [351/1000], Loss: 0.0183\n",
      "Correlation: -0.050457 \n",
      " P-Value: 0.483607\n",
      "Epoch [352/1000], Loss: 0.0183\n",
      "Correlation: -0.050405 \n",
      " P-Value: 0.484059\n",
      "Epoch [353/1000], Loss: 0.0183\n",
      "Correlation: -0.050336 \n",
      " P-Value: 0.484662\n",
      "Epoch [354/1000], Loss: 0.0183\n",
      "Correlation: -0.050328 \n",
      " P-Value: 0.484726\n",
      "Epoch [355/1000], Loss: 0.0183\n",
      "Correlation: -0.050292 \n",
      " P-Value: 0.485043\n",
      "Epoch [356/1000], Loss: 0.0183\n",
      "Correlation: -0.050238 \n",
      " P-Value: 0.485509\n",
      "Epoch [357/1000], Loss: 0.0183\n",
      "Correlation: -0.050189 \n",
      " P-Value: 0.485939\n",
      "Epoch [358/1000], Loss: 0.0183\n",
      "Correlation: -0.050149 \n",
      " P-Value: 0.486286\n",
      "Epoch [359/1000], Loss: 0.0183\n",
      "Correlation: -0.050049 \n",
      " P-Value: 0.487157\n",
      "Epoch [360/1000], Loss: 0.0183\n",
      "Correlation: -0.050027 \n",
      " P-Value: 0.487347\n",
      "Epoch [361/1000], Loss: 0.0183\n",
      "Correlation: -0.050013 \n",
      " P-Value: 0.487467\n",
      "Epoch [362/1000], Loss: 0.0183\n",
      "Correlation: -0.049977 \n",
      " P-Value: 0.487784\n",
      "Epoch [363/1000], Loss: 0.0183\n",
      "Correlation: -0.049891 \n",
      " P-Value: 0.488530\n",
      "Epoch [364/1000], Loss: 0.0183\n",
      "Correlation: -0.049834 \n",
      " P-Value: 0.489033\n",
      "Epoch [365/1000], Loss: 0.0183\n",
      "Correlation: -0.049800 \n",
      " P-Value: 0.489328\n",
      "Epoch [366/1000], Loss: 0.0183\n",
      "Correlation: -0.049742 \n",
      " P-Value: 0.489833\n",
      "Epoch [367/1000], Loss: 0.0183\n",
      "Correlation: -0.049703 \n",
      " P-Value: 0.490180\n",
      "Epoch [368/1000], Loss: 0.0183\n",
      "Correlation: -0.049710 \n",
      " P-Value: 0.490112\n",
      "Epoch [369/1000], Loss: 0.0183\n",
      "Correlation: -0.049669 \n",
      " P-Value: 0.490473\n",
      "Epoch [370/1000], Loss: 0.0183\n",
      "Correlation: -0.049607 \n",
      " P-Value: 0.491018\n",
      "Epoch [371/1000], Loss: 0.0183\n",
      "Correlation: -0.049594 \n",
      " P-Value: 0.491127\n",
      "Epoch [372/1000], Loss: 0.0183\n",
      "Correlation: -0.049604 \n",
      " P-Value: 0.491039\n",
      "Epoch [373/1000], Loss: 0.0183\n",
      "Correlation: -0.049575 \n",
      " P-Value: 0.491293\n",
      "Epoch [374/1000], Loss: 0.0183\n",
      "Correlation: -0.049508 \n",
      " P-Value: 0.491881\n",
      "Epoch [375/1000], Loss: 0.0183\n",
      "Correlation: -0.049452 \n",
      " P-Value: 0.492370\n",
      "Epoch [376/1000], Loss: 0.0183\n",
      "Correlation: -0.049434 \n",
      " P-Value: 0.492535\n",
      "Epoch [377/1000], Loss: 0.0183\n",
      "Correlation: -0.049432 \n",
      " P-Value: 0.492549\n",
      "Epoch [378/1000], Loss: 0.0183\n",
      "Correlation: -0.049393 \n",
      " P-Value: 0.492888\n",
      "Epoch [379/1000], Loss: 0.0183\n",
      "Correlation: -0.049322 \n",
      " P-Value: 0.493512\n",
      "Epoch [380/1000], Loss: 0.0183\n",
      "Correlation: -0.049292 \n",
      " P-Value: 0.493775\n",
      "Epoch [381/1000], Loss: 0.0183\n",
      "Correlation: -0.049242 \n",
      " P-Value: 0.494212\n",
      "Epoch [382/1000], Loss: 0.0183\n",
      "Correlation: -0.049191 \n",
      " P-Value: 0.494666\n",
      "Epoch [383/1000], Loss: 0.0183\n",
      "Correlation: -0.049184 \n",
      " P-Value: 0.494724\n",
      "Epoch [384/1000], Loss: 0.0183\n",
      "Correlation: -0.049179 \n",
      " P-Value: 0.494765\n",
      "Epoch [385/1000], Loss: 0.0183\n",
      "Correlation: -0.049131 \n",
      " P-Value: 0.495191\n",
      "Epoch [386/1000], Loss: 0.0183\n",
      "Correlation: -0.049100 \n",
      " P-Value: 0.495461\n",
      "Epoch [387/1000], Loss: 0.0183\n",
      "Correlation: -0.049039 \n",
      " P-Value: 0.495998\n",
      "Epoch [388/1000], Loss: 0.0183\n",
      "Correlation: -0.049035 \n",
      " P-Value: 0.496034\n",
      "Epoch [389/1000], Loss: 0.0183\n",
      "Correlation: -0.049015 \n",
      " P-Value: 0.496207\n",
      "Epoch [390/1000], Loss: 0.0183\n",
      "Correlation: -0.048950 \n",
      " P-Value: 0.496779\n",
      "Epoch [391/1000], Loss: 0.0183\n",
      "Correlation: -0.048920 \n",
      " P-Value: 0.497047\n",
      "Epoch [392/1000], Loss: 0.0183\n",
      "Correlation: -0.048928 \n",
      " P-Value: 0.496977\n",
      "Epoch [393/1000], Loss: 0.0183\n",
      "Correlation: -0.048861 \n",
      " P-Value: 0.497563\n",
      "Epoch [394/1000], Loss: 0.0183\n",
      "Correlation: -0.048827 \n",
      " P-Value: 0.497868\n",
      "Epoch [395/1000], Loss: 0.0183\n",
      "Correlation: -0.048814 \n",
      " P-Value: 0.497983\n",
      "Epoch [396/1000], Loss: 0.0183\n",
      "Correlation: -0.048765 \n",
      " P-Value: 0.498411\n",
      "Epoch [397/1000], Loss: 0.0183\n",
      "Correlation: -0.048723 \n",
      " P-Value: 0.498786\n",
      "Epoch [398/1000], Loss: 0.0183\n",
      "Correlation: -0.048718 \n",
      " P-Value: 0.498826\n",
      "Epoch [399/1000], Loss: 0.0183\n",
      "Correlation: -0.048730 \n",
      " P-Value: 0.498724\n",
      "Epoch [400/1000], Loss: 0.0183\n",
      "Correlation: -0.048701 \n",
      " P-Value: 0.498977\n",
      "Epoch [401/1000], Loss: 0.0183\n",
      "Correlation: -0.048606 \n",
      " P-Value: 0.499813\n",
      "Epoch [402/1000], Loss: 0.0183\n",
      "Correlation: -0.048633 \n",
      " P-Value: 0.499573\n",
      "Epoch [403/1000], Loss: 0.0183\n",
      "Correlation: -0.048642 \n",
      " P-Value: 0.499494\n",
      "Epoch [404/1000], Loss: 0.0183\n",
      "Correlation: -0.048576 \n",
      " P-Value: 0.500083\n",
      "Epoch [405/1000], Loss: 0.0183\n",
      "Correlation: -0.048507 \n",
      " P-Value: 0.500692\n",
      "Epoch [406/1000], Loss: 0.0183\n",
      "Correlation: -0.048524 \n",
      " P-Value: 0.500543\n",
      "Epoch [407/1000], Loss: 0.0183\n",
      "Correlation: -0.048503 \n",
      " P-Value: 0.500721\n",
      "Epoch [408/1000], Loss: 0.0183\n",
      "Correlation: -0.048464 \n",
      " P-Value: 0.501069\n",
      "Epoch [409/1000], Loss: 0.0183\n",
      "Correlation: -0.048407 \n",
      " P-Value: 0.501570\n",
      "Epoch [410/1000], Loss: 0.0183\n",
      "Correlation: -0.048383 \n",
      " P-Value: 0.501791\n",
      "Epoch [411/1000], Loss: 0.0183\n",
      "Correlation: -0.048374 \n",
      " P-Value: 0.501869\n",
      "Epoch [412/1000], Loss: 0.0183\n",
      "Correlation: -0.048354 \n",
      " P-Value: 0.502046\n",
      "Epoch [413/1000], Loss: 0.0183\n",
      "Correlation: -0.048314 \n",
      " P-Value: 0.502393\n",
      "Epoch [414/1000], Loss: 0.0183\n",
      "Correlation: -0.048270 \n",
      " P-Value: 0.502786\n",
      "Epoch [415/1000], Loss: 0.0183\n",
      "Correlation: -0.048219 \n",
      " P-Value: 0.503240\n",
      "Epoch [416/1000], Loss: 0.0183\n",
      "Correlation: -0.048234 \n",
      " P-Value: 0.503104\n",
      "Epoch [417/1000], Loss: 0.0183\n",
      "Correlation: -0.048234 \n",
      " P-Value: 0.503107\n",
      "Epoch [418/1000], Loss: 0.0183\n",
      "Correlation: -0.048159 \n",
      " P-Value: 0.503772\n",
      "Epoch [419/1000], Loss: 0.0183\n",
      "Correlation: -0.048077 \n",
      " P-Value: 0.504497\n",
      "Epoch [420/1000], Loss: 0.0183\n",
      "Correlation: -0.048087 \n",
      " P-Value: 0.504411\n",
      "Epoch [421/1000], Loss: 0.0183\n",
      "Correlation: -0.048059 \n",
      " P-Value: 0.504662\n",
      "Epoch [422/1000], Loss: 0.0183\n",
      "Correlation: -0.047995 \n",
      " P-Value: 0.505223\n",
      "Epoch [423/1000], Loss: 0.0183\n",
      "Correlation: -0.047991 \n",
      " P-Value: 0.505261\n",
      "Epoch [424/1000], Loss: 0.0183\n",
      "Correlation: -0.047946 \n",
      " P-Value: 0.505662\n",
      "Epoch [425/1000], Loss: 0.0183\n",
      "Correlation: -0.047917 \n",
      " P-Value: 0.505916\n",
      "Epoch [426/1000], Loss: 0.0183\n",
      "Correlation: -0.047884 \n",
      " P-Value: 0.506211\n",
      "Epoch [427/1000], Loss: 0.0183\n",
      "Correlation: -0.047831 \n",
      " P-Value: 0.506687\n",
      "Epoch [428/1000], Loss: 0.0183\n",
      "Correlation: -0.047844 \n",
      " P-Value: 0.506572\n",
      "Epoch [429/1000], Loss: 0.0183\n",
      "Correlation: -0.047830 \n",
      " P-Value: 0.506693\n",
      "Epoch [430/1000], Loss: 0.0183\n",
      "Correlation: -0.047751 \n",
      " P-Value: 0.507398\n",
      "Epoch [431/1000], Loss: 0.0183\n",
      "Correlation: -0.047680 \n",
      " P-Value: 0.508025\n",
      "Epoch [432/1000], Loss: 0.0183\n",
      "Correlation: -0.047666 \n",
      " P-Value: 0.508155\n",
      "Epoch [433/1000], Loss: 0.0183\n",
      "Correlation: -0.047695 \n",
      " P-Value: 0.507898\n",
      "Epoch [434/1000], Loss: 0.0183\n",
      "Correlation: -0.047675 \n",
      " P-Value: 0.508077\n",
      "Epoch [435/1000], Loss: 0.0183\n",
      "Correlation: -0.047596 \n",
      " P-Value: 0.508778\n",
      "Epoch [436/1000], Loss: 0.0183\n",
      "Correlation: -0.047528 \n",
      " P-Value: 0.509385\n",
      "Epoch [437/1000], Loss: 0.0183\n",
      "Correlation: -0.047504 \n",
      " P-Value: 0.509599\n",
      "Epoch [438/1000], Loss: 0.0183\n",
      "Correlation: -0.047499 \n",
      " P-Value: 0.509638\n",
      "Epoch [439/1000], Loss: 0.0183\n",
      "Correlation: -0.047496 \n",
      " P-Value: 0.509669\n",
      "Epoch [440/1000], Loss: 0.0183\n",
      "Correlation: -0.047502 \n",
      " P-Value: 0.509618\n",
      "Epoch [441/1000], Loss: 0.0183\n",
      "Correlation: -0.047433 \n",
      " P-Value: 0.510229\n",
      "Epoch [442/1000], Loss: 0.0183\n",
      "Correlation: -0.047400 \n",
      " P-Value: 0.510528\n",
      "Epoch [443/1000], Loss: 0.0183\n",
      "Correlation: -0.047348 \n",
      " P-Value: 0.510995\n",
      "Epoch [444/1000], Loss: 0.0183\n",
      "Correlation: -0.047356 \n",
      " P-Value: 0.510922\n",
      "Epoch [445/1000], Loss: 0.0183\n",
      "Correlation: -0.047332 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " P-Value: 0.511132\n",
      "Epoch [446/1000], Loss: 0.0183\n",
      "Correlation: -0.047243 \n",
      " P-Value: 0.511926\n",
      "Epoch [447/1000], Loss: 0.0183\n",
      "Correlation: -0.047258 \n",
      " P-Value: 0.511793\n",
      "Epoch [448/1000], Loss: 0.0183\n",
      "Correlation: -0.047235 \n",
      " P-Value: 0.512001\n",
      "Epoch [449/1000], Loss: 0.0183\n",
      "Correlation: -0.047188 \n",
      " P-Value: 0.512425\n",
      "Epoch [450/1000], Loss: 0.0183\n",
      "Correlation: -0.047162 \n",
      " P-Value: 0.512657\n",
      "Epoch [451/1000], Loss: 0.0183\n",
      "Correlation: -0.047110 \n",
      " P-Value: 0.513116\n",
      "Epoch [452/1000], Loss: 0.0183\n",
      "Correlation: -0.047101 \n",
      " P-Value: 0.513197\n",
      "Epoch [453/1000], Loss: 0.0183\n",
      "Correlation: -0.047068 \n",
      " P-Value: 0.513492\n",
      "Epoch [454/1000], Loss: 0.0183\n",
      "Correlation: -0.046970 \n",
      " P-Value: 0.514374\n",
      "Epoch [455/1000], Loss: 0.0183\n",
      "Correlation: -0.046991 \n",
      " P-Value: 0.514181\n",
      "Epoch [456/1000], Loss: 0.0183\n",
      "Correlation: -0.047041 \n",
      " P-Value: 0.513736\n",
      "Epoch [457/1000], Loss: 0.0183\n",
      "Correlation: -0.046869 \n",
      " P-Value: 0.515278\n",
      "Epoch [458/1000], Loss: 0.0183\n",
      "Correlation: -0.046815 \n",
      " P-Value: 0.515764\n",
      "Epoch [459/1000], Loss: 0.0183\n",
      "Correlation: -0.046853 \n",
      " P-Value: 0.515421\n",
      "Epoch [460/1000], Loss: 0.0183\n",
      "Correlation: -0.046738 \n",
      " P-Value: 0.516452\n",
      "Epoch [461/1000], Loss: 0.0183\n",
      "Correlation: -0.046626 \n",
      " P-Value: 0.517466\n",
      "Epoch [462/1000], Loss: 0.0183\n",
      "Correlation: -0.046542 \n",
      " P-Value: 0.518215\n",
      "Epoch [463/1000], Loss: 0.0183\n",
      "Correlation: -0.046549 \n",
      " P-Value: 0.518151\n",
      "Epoch [464/1000], Loss: 0.0183\n",
      "Correlation: -0.046488 \n",
      " P-Value: 0.518701\n",
      "Epoch [465/1000], Loss: 0.0183\n",
      "Correlation: -0.046394 \n",
      " P-Value: 0.519549\n",
      "Epoch [466/1000], Loss: 0.0183\n",
      "Correlation: -0.046268 \n",
      " P-Value: 0.520691\n",
      "Epoch [467/1000], Loss: 0.0183\n",
      "Correlation: -0.046283 \n",
      " P-Value: 0.520547\n",
      "Epoch [468/1000], Loss: 0.0183\n",
      "Correlation: -0.046294 \n",
      " P-Value: 0.520453\n",
      "Epoch [469/1000], Loss: 0.0183\n",
      "Correlation: -0.046148 \n",
      " P-Value: 0.521772\n",
      "Epoch [470/1000], Loss: 0.0183\n",
      "Correlation: -0.045989 \n",
      " P-Value: 0.523205\n",
      "Epoch [471/1000], Loss: 0.0183\n",
      "Correlation: -0.045971 \n",
      " P-Value: 0.523367\n",
      "Epoch [472/1000], Loss: 0.0183\n",
      "Correlation: -0.045927 \n",
      " P-Value: 0.523766\n",
      "Epoch [473/1000], Loss: 0.0183\n",
      "Correlation: -0.045782 \n",
      " P-Value: 0.525077\n",
      "Epoch [474/1000], Loss: 0.0183\n",
      "Correlation: -0.045706 \n",
      " P-Value: 0.525772\n",
      "Epoch [475/1000], Loss: 0.0183\n",
      "Correlation: -0.045700 \n",
      " P-Value: 0.525819\n",
      "Epoch [476/1000], Loss: 0.0183\n",
      "Correlation: -0.045653 \n",
      " P-Value: 0.526248\n",
      "Epoch [477/1000], Loss: 0.0183\n",
      "Correlation: -0.045494 \n",
      " P-Value: 0.527690\n",
      "Epoch [478/1000], Loss: 0.0183\n",
      "Correlation: -0.045498 \n",
      " P-Value: 0.527656\n",
      "Epoch [479/1000], Loss: 0.0183\n",
      "Correlation: -0.045529 \n",
      " P-Value: 0.527373\n",
      "Epoch [480/1000], Loss: 0.0183\n",
      "Correlation: -0.045406 \n",
      " P-Value: 0.528493\n",
      "Epoch [481/1000], Loss: 0.0183\n",
      "Correlation: -0.045253 \n",
      " P-Value: 0.529884\n",
      "Epoch [482/1000], Loss: 0.0183\n",
      "Correlation: -0.045264 \n",
      " P-Value: 0.529788\n",
      "Epoch [483/1000], Loss: 0.0183\n",
      "Correlation: -0.045304 \n",
      " P-Value: 0.529424\n",
      "Epoch [484/1000], Loss: 0.0183\n",
      "Correlation: -0.045179 \n",
      " P-Value: 0.530555\n",
      "Epoch [485/1000], Loss: 0.0183\n",
      "Correlation: -0.045102 \n",
      " P-Value: 0.531261\n",
      "Epoch [486/1000], Loss: 0.0183\n",
      "Correlation: -0.045159 \n",
      " P-Value: 0.530743\n",
      "Epoch [487/1000], Loss: 0.0183\n",
      "Correlation: -0.045135 \n",
      " P-Value: 0.530963\n",
      "Epoch [488/1000], Loss: 0.0183\n",
      "Correlation: -0.045042 \n",
      " P-Value: 0.531807\n",
      "Epoch [489/1000], Loss: 0.0183\n",
      "Correlation: -0.044966 \n",
      " P-Value: 0.532499\n",
      "Epoch [490/1000], Loss: 0.0183\n",
      "Correlation: -0.044947 \n",
      " P-Value: 0.532669\n",
      "Epoch [491/1000], Loss: 0.0183\n",
      "Correlation: -0.044965 \n",
      " P-Value: 0.532512\n",
      "Epoch [492/1000], Loss: 0.0183\n",
      "Correlation: -0.044971 \n",
      " P-Value: 0.532458\n",
      "Epoch [493/1000], Loss: 0.0183\n",
      "Correlation: -0.044955 \n",
      " P-Value: 0.532604\n",
      "Epoch [494/1000], Loss: 0.0183\n",
      "Correlation: -0.044946 \n",
      " P-Value: 0.532684\n",
      "Epoch [495/1000], Loss: 0.0183\n",
      "Correlation: -0.044941 \n",
      " P-Value: 0.532728\n",
      "Epoch [496/1000], Loss: 0.0183\n",
      "Correlation: -0.044927 \n",
      " P-Value: 0.532858\n",
      "Epoch [497/1000], Loss: 0.0183\n",
      "Correlation: -0.044910 \n",
      " P-Value: 0.533008\n",
      "Epoch [498/1000], Loss: 0.0183\n",
      "Correlation: -0.044842 \n",
      " P-Value: 0.533634\n",
      "Epoch [499/1000], Loss: 0.0183\n",
      "Correlation: -0.044830 \n",
      " P-Value: 0.533741\n",
      "Epoch [500/1000], Loss: 0.0183\n",
      "Correlation: -0.044844 \n",
      " P-Value: 0.533611\n",
      "Epoch [501/1000], Loss: 0.0183\n",
      "Correlation: -0.044791 \n",
      " P-Value: 0.534096\n",
      "Epoch [502/1000], Loss: 0.0183\n",
      "Correlation: -0.044667 \n",
      " P-Value: 0.535230\n",
      "Epoch [503/1000], Loss: 0.0183\n",
      "Correlation: -0.044766 \n",
      " P-Value: 0.534326\n",
      "Epoch [504/1000], Loss: 0.0183\n",
      "Correlation: -0.044728 \n",
      " P-Value: 0.534673\n",
      "Epoch [505/1000], Loss: 0.0183\n",
      "Correlation: -0.044579 \n",
      " P-Value: 0.536040\n",
      "Epoch [506/1000], Loss: 0.0183\n",
      "Correlation: -0.044587 \n",
      " P-Value: 0.535967\n",
      "Epoch [507/1000], Loss: 0.0183\n",
      "Correlation: -0.044633 \n",
      " P-Value: 0.535539\n",
      "Epoch [508/1000], Loss: 0.0183\n",
      "Correlation: -0.044600 \n",
      " P-Value: 0.535846\n",
      "Epoch [509/1000], Loss: 0.0183\n",
      "Correlation: -0.044549 \n",
      " P-Value: 0.536316\n",
      "Epoch [510/1000], Loss: 0.0183\n",
      "Correlation: -0.044477 \n",
      " P-Value: 0.536967\n",
      "Epoch [511/1000], Loss: 0.0183\n",
      "Correlation: -0.044458 \n",
      " P-Value: 0.537148\n",
      "Epoch [512/1000], Loss: 0.0183\n",
      "Correlation: -0.044476 \n",
      " P-Value: 0.536978\n",
      "Epoch [513/1000], Loss: 0.0183\n",
      "Correlation: -0.044441 \n",
      " P-Value: 0.537304\n",
      "Epoch [514/1000], Loss: 0.0183\n",
      "Correlation: -0.044306 \n",
      " P-Value: 0.538537\n",
      "Epoch [515/1000], Loss: 0.0183\n",
      "Correlation: -0.044366 \n",
      " P-Value: 0.537989\n",
      "Epoch [516/1000], Loss: 0.0183\n",
      "Correlation: -0.044348 \n",
      " P-Value: 0.538150\n",
      "Epoch [517/1000], Loss: 0.0183\n",
      "Correlation: -0.044282 \n",
      " P-Value: 0.538758\n",
      "Epoch [518/1000], Loss: 0.0183\n",
      "Correlation: -0.044257 \n",
      " P-Value: 0.538992\n",
      "Epoch [519/1000], Loss: 0.0183\n",
      "Correlation: -0.044154 \n",
      " P-Value: 0.539934\n",
      "Epoch [520/1000], Loss: 0.0183\n",
      "Correlation: -0.044173 \n",
      " P-Value: 0.539762\n",
      "Epoch [521/1000], Loss: 0.0183\n",
      "Correlation: -0.044168 \n",
      " P-Value: 0.539804\n",
      "Epoch [522/1000], Loss: 0.0183\n",
      "Correlation: -0.044119 \n",
      " P-Value: 0.540257\n",
      "Epoch [523/1000], Loss: 0.0183\n",
      "Correlation: -0.044163 \n",
      " P-Value: 0.539854\n",
      "Epoch [524/1000], Loss: 0.0183\n",
      "Correlation: -0.044161 \n",
      " P-Value: 0.539873\n",
      "Epoch [525/1000], Loss: 0.0183\n",
      "Correlation: -0.044087 \n",
      " P-Value: 0.540552\n",
      "Epoch [526/1000], Loss: 0.0183\n",
      "Correlation: -0.044039 \n",
      " P-Value: 0.540995\n",
      "Epoch [527/1000], Loss: 0.0183\n",
      "Correlation: -0.044053 \n",
      " P-Value: 0.540863\n",
      "Epoch [528/1000], Loss: 0.0183\n",
      "Correlation: -0.044018 \n",
      " P-Value: 0.541187\n",
      "Epoch [529/1000], Loss: 0.0183\n",
      "Correlation: -0.043986 \n",
      " P-Value: 0.541477\n",
      "Epoch [530/1000], Loss: 0.0183\n",
      "Correlation: -0.043888 \n",
      " P-Value: 0.542384\n",
      "Epoch [531/1000], Loss: 0.0183\n",
      "Correlation: -0.043946 \n",
      " P-Value: 0.541852\n",
      "Epoch [532/1000], Loss: 0.0183\n",
      "Correlation: -0.043971 \n",
      " P-Value: 0.541616\n",
      "Epoch [533/1000], Loss: 0.0183\n",
      "Correlation: -0.043840 \n",
      " P-Value: 0.542821\n",
      "Epoch [534/1000], Loss: 0.0183\n",
      "Correlation: -0.043791 \n",
      " P-Value: 0.543278\n",
      "Epoch [535/1000], Loss: 0.0183\n",
      "Correlation: -0.043779 \n",
      " P-Value: 0.543382\n",
      "Epoch [536/1000], Loss: 0.0183\n",
      "Correlation: -0.043847 \n",
      " P-Value: 0.542758\n",
      "Epoch [537/1000], Loss: 0.0183\n",
      "Correlation: -0.043814 \n",
      " P-Value: 0.543066\n",
      "Epoch [538/1000], Loss: 0.0183\n",
      "Correlation: -0.043792 \n",
      " P-Value: 0.543270\n",
      "Epoch [539/1000], Loss: 0.0183\n",
      "Correlation: -0.043703 \n",
      " P-Value: 0.544087\n",
      "Epoch [540/1000], Loss: 0.0183\n",
      "Correlation: -0.043705 \n",
      " P-Value: 0.544065\n",
      "Epoch [541/1000], Loss: 0.0183\n",
      "Correlation: -0.043701 \n",
      " P-Value: 0.544103\n",
      "Epoch [542/1000], Loss: 0.0183\n",
      "Correlation: -0.043638 \n",
      " P-Value: 0.544686\n",
      "Epoch [543/1000], Loss: 0.0183\n",
      "Correlation: -0.043529 \n",
      " P-Value: 0.545693\n",
      "Epoch [544/1000], Loss: 0.0183\n",
      "Correlation: -0.043594 \n",
      " P-Value: 0.545092\n",
      "Epoch [545/1000], Loss: 0.0183\n",
      "Correlation: -0.043630 \n",
      " P-Value: 0.544760\n",
      "Epoch [546/1000], Loss: 0.0183\n",
      "Correlation: -0.043557 \n",
      " P-Value: 0.545437\n",
      "Epoch [547/1000], Loss: 0.0183\n",
      "Correlation: -0.043462 \n",
      " P-Value: 0.546315\n",
      "Epoch [548/1000], Loss: 0.0183\n",
      "Correlation: -0.043520 \n",
      " P-Value: 0.545778\n",
      "Epoch [549/1000], Loss: 0.0183\n",
      "Correlation: -0.043588 \n",
      " P-Value: 0.545149\n",
      "Epoch [550/1000], Loss: 0.0183\n",
      "Correlation: -0.043491 \n",
      " P-Value: 0.546042\n",
      "Epoch [551/1000], Loss: 0.0183\n",
      "Correlation: -0.043396 \n",
      " P-Value: 0.546917\n",
      "Epoch [552/1000], Loss: 0.0183\n",
      "Correlation: -0.043428 \n",
      " P-Value: 0.546623\n",
      "Epoch [553/1000], Loss: 0.0183\n",
      "Correlation: -0.043421 \n",
      " P-Value: 0.546694\n",
      "Epoch [554/1000], Loss: 0.0183\n",
      "Correlation: -0.043385 \n",
      " P-Value: 0.547024\n",
      "Epoch [555/1000], Loss: 0.0183\n",
      "Correlation: -0.043302 \n",
      " P-Value: 0.547792\n",
      "Epoch [556/1000], Loss: 0.0183\n",
      "Correlation: -0.043363 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " P-Value: 0.547229\n",
      "Epoch [557/1000], Loss: 0.0183\n",
      "Correlation: -0.043334 \n",
      " P-Value: 0.547493\n",
      "Epoch [558/1000], Loss: 0.0183\n",
      "Correlation: -0.043244 \n",
      " P-Value: 0.548328\n",
      "Epoch [559/1000], Loss: 0.0183\n",
      "Correlation: -0.043217 \n",
      " P-Value: 0.548577\n",
      "Epoch [560/1000], Loss: 0.0183\n",
      "Correlation: -0.043164 \n",
      " P-Value: 0.549066\n",
      "Epoch [561/1000], Loss: 0.0183\n",
      "Correlation: -0.043160 \n",
      " P-Value: 0.549106\n",
      "Epoch [562/1000], Loss: 0.0183\n",
      "Correlation: -0.043169 \n",
      " P-Value: 0.549022\n",
      "Epoch [563/1000], Loss: 0.0183\n",
      "Correlation: -0.043172 \n",
      " P-Value: 0.548991\n",
      "Epoch [564/1000], Loss: 0.0183\n",
      "Correlation: -0.043181 \n",
      " P-Value: 0.548912\n",
      "Epoch [565/1000], Loss: 0.0183\n",
      "Correlation: -0.043121 \n",
      " P-Value: 0.549469\n",
      "Epoch [566/1000], Loss: 0.0183\n",
      "Correlation: -0.043104 \n",
      " P-Value: 0.549628\n",
      "Epoch [567/1000], Loss: 0.0183\n",
      "Correlation: -0.043143 \n",
      " P-Value: 0.549263\n",
      "Epoch [568/1000], Loss: 0.0183\n",
      "Correlation: -0.043113 \n",
      " P-Value: 0.549537\n",
      "Epoch [569/1000], Loss: 0.0183\n",
      "Correlation: -0.043115 \n",
      " P-Value: 0.549519\n",
      "Epoch [570/1000], Loss: 0.0183\n",
      "Correlation: -0.043029 \n",
      " P-Value: 0.550324\n",
      "Epoch [571/1000], Loss: 0.0183\n",
      "Correlation: -0.043017 \n",
      " P-Value: 0.550428\n",
      "Epoch [572/1000], Loss: 0.0183\n",
      "Correlation: -0.043010 \n",
      " P-Value: 0.550501\n",
      "Epoch [573/1000], Loss: 0.0183\n",
      "Correlation: -0.043014 \n",
      " P-Value: 0.550462\n",
      "Epoch [574/1000], Loss: 0.0183\n",
      "Correlation: -0.043007 \n",
      " P-Value: 0.550525\n",
      "Epoch [575/1000], Loss: 0.0183\n",
      "Correlation: -0.042951 \n",
      " P-Value: 0.551040\n",
      "Epoch [576/1000], Loss: 0.0183\n",
      "Correlation: -0.042912 \n",
      " P-Value: 0.551405\n",
      "Epoch [577/1000], Loss: 0.0183\n",
      "Correlation: -0.042969 \n",
      " P-Value: 0.550874\n",
      "Epoch [578/1000], Loss: 0.0183\n",
      "Correlation: -0.042944 \n",
      " P-Value: 0.551107\n",
      "Epoch [579/1000], Loss: 0.0183\n",
      "Correlation: -0.042900 \n",
      " P-Value: 0.551520\n",
      "Epoch [580/1000], Loss: 0.0183\n",
      "Correlation: -0.042877 \n",
      " P-Value: 0.551727\n",
      "Epoch [581/1000], Loss: 0.0183\n",
      "Correlation: -0.042820 \n",
      " P-Value: 0.552261\n",
      "Epoch [582/1000], Loss: 0.0183\n",
      "Correlation: -0.042869 \n",
      " P-Value: 0.551801\n",
      "Epoch [583/1000], Loss: 0.0183\n",
      "Correlation: -0.042843 \n",
      " P-Value: 0.552050\n",
      "Epoch [584/1000], Loss: 0.0183\n",
      "Correlation: -0.042769 \n",
      " P-Value: 0.552738\n",
      "Epoch [585/1000], Loss: 0.0183\n",
      "Correlation: -0.042760 \n",
      " P-Value: 0.552817\n",
      "Epoch [586/1000], Loss: 0.0183\n",
      "Correlation: -0.042744 \n",
      " P-Value: 0.552966\n",
      "Epoch [587/1000], Loss: 0.0183\n",
      "Correlation: -0.042729 \n",
      " P-Value: 0.553105\n",
      "Epoch [588/1000], Loss: 0.0183\n",
      "Correlation: -0.042789 \n",
      " P-Value: 0.552552\n",
      "Epoch [589/1000], Loss: 0.0183\n",
      "Correlation: -0.042735 \n",
      " P-Value: 0.553047\n",
      "Epoch [590/1000], Loss: 0.0183\n",
      "Correlation: -0.042615 \n",
      " P-Value: 0.554168\n",
      "Epoch [591/1000], Loss: 0.0183\n",
      "Correlation: -0.042710 \n",
      " P-Value: 0.553282\n",
      "Epoch [592/1000], Loss: 0.0183\n",
      "Correlation: -0.042706 \n",
      " P-Value: 0.553321\n",
      "Epoch [593/1000], Loss: 0.0183\n",
      "Correlation: -0.042559 \n",
      " P-Value: 0.554687\n",
      "Epoch [594/1000], Loss: 0.0183\n",
      "Correlation: -0.042581 \n",
      " P-Value: 0.554484\n",
      "Epoch [595/1000], Loss: 0.0183\n",
      "Correlation: -0.042611 \n",
      " P-Value: 0.554200\n",
      "Epoch [596/1000], Loss: 0.0183\n",
      "Correlation: -0.042672 \n",
      " P-Value: 0.553631\n",
      "Epoch [597/1000], Loss: 0.0183\n",
      "Correlation: -0.042578 \n",
      " P-Value: 0.554507\n",
      "Epoch [598/1000], Loss: 0.0183\n",
      "Correlation: -0.042480 \n",
      " P-Value: 0.555420\n",
      "Epoch [599/1000], Loss: 0.0183\n",
      "Correlation: -0.042418 \n",
      " P-Value: 0.555999\n",
      "Epoch [600/1000], Loss: 0.0183\n",
      "Correlation: -0.042523 \n",
      " P-Value: 0.555019\n",
      "Epoch [601/1000], Loss: 0.0183\n",
      "Correlation: -0.042547 \n",
      " P-Value: 0.554799\n",
      "Epoch [602/1000], Loss: 0.0183\n",
      "Correlation: -0.042479 \n",
      " P-Value: 0.555436\n",
      "Epoch [603/1000], Loss: 0.0183\n",
      "Correlation: -0.042457 \n",
      " P-Value: 0.555635\n",
      "Epoch [604/1000], Loss: 0.0183\n",
      "Correlation: -0.042488 \n",
      " P-Value: 0.555345\n",
      "Epoch [605/1000], Loss: 0.0183\n",
      "Correlation: -0.042412 \n",
      " P-Value: 0.556059\n",
      "Epoch [606/1000], Loss: 0.0183\n",
      "Correlation: -0.042330 \n",
      " P-Value: 0.556821\n",
      "Epoch [607/1000], Loss: 0.0183\n",
      "Correlation: -0.042385 \n",
      " P-Value: 0.556311\n",
      "Epoch [608/1000], Loss: 0.0183\n",
      "Correlation: -0.042408 \n",
      " P-Value: 0.556090\n",
      "Epoch [609/1000], Loss: 0.0183\n",
      "Correlation: -0.042342 \n",
      " P-Value: 0.556711\n",
      "Epoch [610/1000], Loss: 0.0183\n",
      "Correlation: -0.042359 \n",
      " P-Value: 0.556546\n",
      "Epoch [611/1000], Loss: 0.0183\n",
      "Correlation: -0.042273 \n",
      " P-Value: 0.557353\n",
      "Epoch [612/1000], Loss: 0.0183\n",
      "Correlation: -0.042241 \n",
      " P-Value: 0.557648\n",
      "Epoch [613/1000], Loss: 0.0183\n",
      "Correlation: -0.042286 \n",
      " P-Value: 0.557230\n",
      "Epoch [614/1000], Loss: 0.0183\n",
      "Correlation: -0.042318 \n",
      " P-Value: 0.556932\n",
      "Epoch [615/1000], Loss: 0.0183\n",
      "Correlation: -0.042166 \n",
      " P-Value: 0.558353\n",
      "Epoch [616/1000], Loss: 0.0183\n",
      "Correlation: -0.042261 \n",
      " P-Value: 0.557461\n",
      "Epoch [617/1000], Loss: 0.0183\n",
      "Correlation: -0.042291 \n",
      " P-Value: 0.557188\n",
      "Epoch [618/1000], Loss: 0.0183\n",
      "Correlation: -0.042105 \n",
      " P-Value: 0.558921\n",
      "Epoch [619/1000], Loss: 0.0183\n",
      "Correlation: -0.042106 \n",
      " P-Value: 0.558912\n",
      "Epoch [620/1000], Loss: 0.0183\n",
      "Correlation: -0.042214 \n",
      " P-Value: 0.557900\n",
      "Epoch [621/1000], Loss: 0.0183\n",
      "Correlation: -0.042133 \n",
      " P-Value: 0.558664\n",
      "Epoch [622/1000], Loss: 0.0183\n",
      "Correlation: -0.042074 \n",
      " P-Value: 0.559216\n",
      "Epoch [623/1000], Loss: 0.0183\n",
      "Correlation: -0.042050 \n",
      " P-Value: 0.559437\n",
      "Epoch [624/1000], Loss: 0.0183\n",
      "Correlation: -0.042087 \n",
      " P-Value: 0.559089\n",
      "Epoch [625/1000], Loss: 0.0183\n",
      "Correlation: -0.042155 \n",
      " P-Value: 0.558454\n",
      "Epoch [626/1000], Loss: 0.0183\n",
      "Correlation: -0.042033 \n",
      " P-Value: 0.559594\n",
      "Epoch [627/1000], Loss: 0.0183\n",
      "Correlation: -0.041941 \n",
      " P-Value: 0.560456\n",
      "Epoch [628/1000], Loss: 0.0183\n",
      "Correlation: -0.042044 \n",
      " P-Value: 0.559495\n",
      "Epoch [629/1000], Loss: 0.0183\n",
      "Correlation: -0.042083 \n",
      " P-Value: 0.559132\n",
      "Epoch [630/1000], Loss: 0.0183\n",
      "Correlation: -0.042029 \n",
      " P-Value: 0.559637\n",
      "Epoch [631/1000], Loss: 0.0183\n",
      "Correlation: -0.041896 \n",
      " P-Value: 0.560875\n",
      "Epoch [632/1000], Loss: 0.0183\n",
      "Correlation: -0.041951 \n",
      " P-Value: 0.560360\n",
      "Epoch [633/1000], Loss: 0.0183\n",
      "Correlation: -0.042057 \n",
      " P-Value: 0.559368\n",
      "Epoch [634/1000], Loss: 0.0183\n",
      "Correlation: -0.041958 \n",
      " P-Value: 0.560301\n",
      "Epoch [635/1000], Loss: 0.0183\n",
      "Correlation: -0.041885 \n",
      " P-Value: 0.560984\n",
      "Epoch [636/1000], Loss: 0.0183\n",
      "Correlation: -0.041934 \n",
      " P-Value: 0.560517\n",
      "Epoch [637/1000], Loss: 0.0183\n",
      "Correlation: -0.041917 \n",
      " P-Value: 0.560682\n",
      "Epoch [638/1000], Loss: 0.0183\n",
      "Correlation: -0.041805 \n",
      " P-Value: 0.561730\n",
      "Epoch [639/1000], Loss: 0.0183\n",
      "Correlation: -0.041783 \n",
      " P-Value: 0.561936\n",
      "Epoch [640/1000], Loss: 0.0183\n",
      "Correlation: -0.041864 \n",
      " P-Value: 0.561179\n",
      "Epoch [641/1000], Loss: 0.0183\n",
      "Correlation: -0.041802 \n",
      " P-Value: 0.561757\n",
      "Epoch [642/1000], Loss: 0.0183\n",
      "Correlation: -0.041719 \n",
      " P-Value: 0.562532\n",
      "Epoch [643/1000], Loss: 0.0183\n",
      "Correlation: -0.041702 \n",
      " P-Value: 0.562691\n",
      "Epoch [644/1000], Loss: 0.0183\n",
      "Correlation: -0.041720 \n",
      " P-Value: 0.562528\n",
      "Epoch [645/1000], Loss: 0.0183\n",
      "Correlation: -0.041642 \n",
      " P-Value: 0.563253\n",
      "Epoch [646/1000], Loss: 0.0183\n",
      "Correlation: -0.041645 \n",
      " P-Value: 0.563229\n",
      "Epoch [647/1000], Loss: 0.0183\n",
      "Correlation: -0.041650 \n",
      " P-Value: 0.563182\n",
      "Epoch [648/1000], Loss: 0.0183\n",
      "Correlation: -0.041562 \n",
      " P-Value: 0.564011\n",
      "Epoch [649/1000], Loss: 0.0183\n",
      "Correlation: -0.041566 \n",
      " P-Value: 0.563966\n",
      "Epoch [650/1000], Loss: 0.0183\n",
      "Correlation: -0.041534 \n",
      " P-Value: 0.564273\n",
      "Epoch [651/1000], Loss: 0.0183\n",
      "Correlation: -0.041517 \n",
      " P-Value: 0.564434\n",
      "Epoch [652/1000], Loss: 0.0183\n",
      "Correlation: -0.041579 \n",
      " P-Value: 0.563844\n",
      "Epoch [653/1000], Loss: 0.0183\n",
      "Correlation: -0.041484 \n",
      " P-Value: 0.564740\n",
      "Epoch [654/1000], Loss: 0.0183\n",
      "Correlation: -0.041338 \n",
      " P-Value: 0.566114\n",
      "Epoch [655/1000], Loss: 0.0183\n",
      "Correlation: -0.041477 \n",
      " P-Value: 0.564801\n",
      "Epoch [656/1000], Loss: 0.0183\n",
      "Correlation: -0.041454 \n",
      " P-Value: 0.565023\n",
      "Epoch [657/1000], Loss: 0.0183\n",
      "Correlation: -0.041297 \n",
      " P-Value: 0.566499\n",
      "Epoch [658/1000], Loss: 0.0183\n",
      "Correlation: -0.041378 \n",
      " P-Value: 0.565739\n",
      "Epoch [659/1000], Loss: 0.0183\n",
      "Correlation: -0.041410 \n",
      " P-Value: 0.565432\n",
      "Epoch [660/1000], Loss: 0.0183\n",
      "Correlation: -0.041271 \n",
      " P-Value: 0.566744\n",
      "Epoch [661/1000], Loss: 0.0183\n",
      "Correlation: -0.041325 \n",
      " P-Value: 0.566237\n",
      "Epoch [662/1000], Loss: 0.0183\n",
      "Correlation: -0.041298 \n",
      " P-Value: 0.566491\n",
      "Epoch [663/1000], Loss: 0.0183\n",
      "Correlation: -0.041178 \n",
      " P-Value: 0.567620\n",
      "Epoch [664/1000], Loss: 0.0183\n",
      "Correlation: -0.041245 \n",
      " P-Value: 0.566991\n",
      "Epoch [665/1000], Loss: 0.0183\n",
      "Correlation: -0.041252 \n",
      " P-Value: 0.566917\n",
      "Epoch [666/1000], Loss: 0.0183\n",
      "Correlation: -0.041199 \n",
      " P-Value: 0.567423\n",
      "Epoch [667/1000], Loss: 0.0183\n",
      "Correlation: -0.041226 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " P-Value: 0.567164\n",
      "Epoch [668/1000], Loss: 0.0183\n",
      "Correlation: -0.041188 \n",
      " P-Value: 0.567522\n",
      "Epoch [669/1000], Loss: 0.0183\n",
      "Correlation: -0.041013 \n",
      " P-Value: 0.569176\n",
      "Epoch [670/1000], Loss: 0.0183\n",
      "Correlation: -0.041088 \n",
      " P-Value: 0.568460\n",
      "Epoch [671/1000], Loss: 0.0183\n",
      "Correlation: -0.041087 \n",
      " P-Value: 0.568471\n",
      "Epoch [672/1000], Loss: 0.0183\n",
      "Correlation: -0.041006 \n",
      " P-Value: 0.569235\n",
      "Epoch [673/1000], Loss: 0.0183\n",
      "Correlation: -0.041116 \n",
      " P-Value: 0.568199\n",
      "Epoch [674/1000], Loss: 0.0183\n",
      "Correlation: -0.041074 \n",
      " P-Value: 0.568595\n",
      "Epoch [675/1000], Loss: 0.0183\n",
      "Correlation: -0.040949 \n",
      " P-Value: 0.569771\n",
      "Epoch [676/1000], Loss: 0.0183\n",
      "Correlation: -0.041048 \n",
      " P-Value: 0.568844\n",
      "Epoch [677/1000], Loss: 0.0183\n",
      "Correlation: -0.040969 \n",
      " P-Value: 0.569586\n",
      "Epoch [678/1000], Loss: 0.0183\n",
      "Correlation: -0.040847 \n",
      " P-Value: 0.570740\n",
      "Epoch [679/1000], Loss: 0.0183\n",
      "Correlation: -0.041000 \n",
      " P-Value: 0.569290\n",
      "Epoch [680/1000], Loss: 0.0183\n",
      "Correlation: -0.040965 \n",
      " P-Value: 0.569621\n",
      "Epoch [681/1000], Loss: 0.0183\n",
      "Correlation: -0.040744 \n",
      " P-Value: 0.571712\n",
      "Epoch [682/1000], Loss: 0.0183\n",
      "Correlation: -0.040876 \n",
      " P-Value: 0.570464\n",
      "Epoch [683/1000], Loss: 0.0183\n",
      "Correlation: -0.040905 \n",
      " P-Value: 0.570189\n",
      "Epoch [684/1000], Loss: 0.0183\n",
      "Correlation: -0.040792 \n",
      " P-Value: 0.571256\n",
      "Epoch [685/1000], Loss: 0.0183\n",
      "Correlation: -0.040827 \n",
      " P-Value: 0.570924\n",
      "Epoch [686/1000], Loss: 0.0183\n",
      "Correlation: -0.040726 \n",
      " P-Value: 0.571879\n",
      "Epoch [687/1000], Loss: 0.0183\n",
      "Correlation: -0.040683 \n",
      " P-Value: 0.572282\n",
      "Epoch [688/1000], Loss: 0.0183\n",
      "Correlation: -0.040702 \n",
      " P-Value: 0.572105\n",
      "Epoch [689/1000], Loss: 0.0183\n",
      "Correlation: -0.040803 \n",
      " P-Value: 0.571155\n",
      "Epoch [690/1000], Loss: 0.0183\n",
      "Correlation: -0.040690 \n",
      " P-Value: 0.572222\n",
      "Epoch [691/1000], Loss: 0.0183\n",
      "Correlation: -0.040519 \n",
      " P-Value: 0.573835\n",
      "Epoch [692/1000], Loss: 0.0183\n",
      "Correlation: -0.040630 \n",
      " P-Value: 0.572791\n",
      "Epoch [693/1000], Loss: 0.0183\n",
      "Correlation: -0.040648 \n",
      " P-Value: 0.572619\n",
      "Epoch [694/1000], Loss: 0.0183\n",
      "Correlation: -0.040445 \n",
      " P-Value: 0.574536\n",
      "Epoch [695/1000], Loss: 0.0183\n",
      "Correlation: -0.040540 \n",
      " P-Value: 0.573641\n",
      "Epoch [696/1000], Loss: 0.0183\n",
      "Correlation: -0.040533 \n",
      " P-Value: 0.573703\n",
      "Epoch [697/1000], Loss: 0.0183\n",
      "Correlation: -0.040424 \n",
      " P-Value: 0.574735\n",
      "Epoch [698/1000], Loss: 0.0183\n",
      "Correlation: -0.040345 \n",
      " P-Value: 0.575482\n",
      "Epoch [699/1000], Loss: 0.0183\n",
      "Correlation: -0.040436 \n",
      " P-Value: 0.574624\n",
      "Epoch [700/1000], Loss: 0.0183\n",
      "Correlation: -0.040477 \n",
      " P-Value: 0.574230\n",
      "Epoch [701/1000], Loss: 0.0183\n",
      "Correlation: -0.040364 \n",
      " P-Value: 0.575301\n",
      "Epoch [702/1000], Loss: 0.0183\n",
      "Correlation: -0.040228 \n",
      " P-Value: 0.576596\n",
      "Epoch [703/1000], Loss: 0.0183\n",
      "Correlation: -0.040353 \n",
      " P-Value: 0.575412\n",
      "Epoch [704/1000], Loss: 0.0183\n",
      "Correlation: -0.040341 \n",
      " P-Value: 0.575518\n",
      "Epoch [705/1000], Loss: 0.0183\n",
      "Correlation: -0.040268 \n",
      " P-Value: 0.576210\n",
      "Epoch [706/1000], Loss: 0.0183\n",
      "Correlation: -0.040115 \n",
      " P-Value: 0.577661\n",
      "Epoch [707/1000], Loss: 0.0183\n",
      "Correlation: -0.040195 \n",
      " P-Value: 0.576905\n",
      "Epoch [708/1000], Loss: 0.0183\n",
      "Correlation: -0.040333 \n",
      " P-Value: 0.575600\n",
      "Epoch [709/1000], Loss: 0.0183\n",
      "Correlation: -0.040287 \n",
      " P-Value: 0.576031\n",
      "Epoch [710/1000], Loss: 0.0183\n",
      "Correlation: -0.040136 \n",
      " P-Value: 0.577466\n",
      "Epoch [711/1000], Loss: 0.0183\n",
      "Correlation: -0.040083 \n",
      " P-Value: 0.577973\n",
      "Epoch [712/1000], Loss: 0.0183\n",
      "Correlation: -0.040075 \n",
      " P-Value: 0.578043\n",
      "Epoch [713/1000], Loss: 0.0183\n",
      "Correlation: -0.040217 \n",
      " P-Value: 0.576694\n",
      "Epoch [714/1000], Loss: 0.0183\n",
      "Correlation: -0.040176 \n",
      " P-Value: 0.577083\n",
      "Epoch [715/1000], Loss: 0.0183\n",
      "Correlation: -0.039991 \n",
      " P-Value: 0.578843\n",
      "Epoch [716/1000], Loss: 0.0183\n",
      "Correlation: -0.040067 \n",
      " P-Value: 0.578119\n",
      "Epoch [717/1000], Loss: 0.0183\n",
      "Correlation: -0.040099 \n",
      " P-Value: 0.577815\n",
      "Epoch [718/1000], Loss: 0.0183\n",
      "Correlation: -0.039966 \n",
      " P-Value: 0.579077\n",
      "Epoch [719/1000], Loss: 0.0183\n",
      "Correlation: -0.040031 \n",
      " P-Value: 0.578461\n",
      "Epoch [720/1000], Loss: 0.0183\n",
      "Correlation: -0.040076 \n",
      " P-Value: 0.578039\n",
      "Epoch [721/1000], Loss: 0.0183\n",
      "Correlation: -0.039917 \n",
      " P-Value: 0.579543\n",
      "Epoch [722/1000], Loss: 0.0183\n",
      "Correlation: -0.039914 \n",
      " P-Value: 0.579576\n",
      "Epoch [723/1000], Loss: 0.0183\n",
      "Correlation: -0.040002 \n",
      " P-Value: 0.578740\n",
      "Epoch [724/1000], Loss: 0.0183\n",
      "Correlation: -0.039868 \n",
      " P-Value: 0.580009\n",
      "Epoch [725/1000], Loss: 0.0183\n",
      "Correlation: -0.039883 \n",
      " P-Value: 0.579870\n",
      "Epoch [726/1000], Loss: 0.0183\n",
      "Correlation: -0.039898 \n",
      " P-Value: 0.579724\n",
      "Epoch [727/1000], Loss: 0.0183\n",
      "Correlation: -0.039795 \n",
      " P-Value: 0.580706\n",
      "Epoch [728/1000], Loss: 0.0183\n",
      "Correlation: -0.039789 \n",
      " P-Value: 0.580760\n",
      "Epoch [729/1000], Loss: 0.0183\n",
      "Correlation: -0.039875 \n",
      " P-Value: 0.579941\n",
      "Epoch [730/1000], Loss: 0.0183\n",
      "Correlation: -0.039750 \n",
      " P-Value: 0.581134\n",
      "Epoch [731/1000], Loss: 0.0183\n",
      "Correlation: -0.039828 \n",
      " P-Value: 0.580393\n",
      "Epoch [732/1000], Loss: 0.0183\n",
      "Correlation: -0.039745 \n",
      " P-Value: 0.581186\n",
      "Epoch [733/1000], Loss: 0.0183\n",
      "Correlation: -0.039596 \n",
      " P-Value: 0.582600\n",
      "Epoch [734/1000], Loss: 0.0183\n",
      "Correlation: -0.039724 \n",
      " P-Value: 0.581380\n",
      "Epoch [735/1000], Loss: 0.0183\n",
      "Correlation: -0.039778 \n",
      " P-Value: 0.580868\n",
      "Epoch [736/1000], Loss: 0.0183\n",
      "Correlation: -0.039626 \n",
      " P-Value: 0.582315\n",
      "Epoch [737/1000], Loss: 0.0183\n",
      "Correlation: -0.039576 \n",
      " P-Value: 0.582788\n",
      "Epoch [738/1000], Loss: 0.0183\n",
      "Correlation: -0.039607 \n",
      " P-Value: 0.582494\n",
      "Epoch [739/1000], Loss: 0.0183\n",
      "Correlation: -0.039561 \n",
      " P-Value: 0.582935\n",
      "Epoch [740/1000], Loss: 0.0183\n",
      "Correlation: -0.039533 \n",
      " P-Value: 0.583206\n",
      "Epoch [741/1000], Loss: 0.0183\n",
      "Correlation: -0.039616 \n",
      " P-Value: 0.582415\n",
      "Epoch [742/1000], Loss: 0.0183\n",
      "Correlation: -0.039509 \n",
      " P-Value: 0.583432\n",
      "Epoch [743/1000], Loss: 0.0183\n",
      "Correlation: -0.039508 \n",
      " P-Value: 0.583441\n",
      "Epoch [744/1000], Loss: 0.0183\n",
      "Correlation: -0.039565 \n",
      " P-Value: 0.582899\n",
      "Epoch [745/1000], Loss: 0.0183\n",
      "Correlation: -0.039502 \n",
      " P-Value: 0.583499\n",
      "Epoch [746/1000], Loss: 0.0183\n",
      "Correlation: -0.039359 \n",
      " P-Value: 0.584863\n",
      "Epoch [747/1000], Loss: 0.0183\n",
      "Correlation: -0.039520 \n",
      " P-Value: 0.583324\n",
      "Epoch [748/1000], Loss: 0.0183\n",
      "Correlation: -0.039461 \n",
      " P-Value: 0.583893\n",
      "Epoch [749/1000], Loss: 0.0183\n",
      "Correlation: -0.039248 \n",
      " P-Value: 0.585924\n",
      "Epoch [750/1000], Loss: 0.0183\n",
      "Correlation: -0.039387 \n",
      " P-Value: 0.584598\n",
      "Epoch [751/1000], Loss: 0.0183\n",
      "Correlation: -0.039452 \n",
      " P-Value: 0.583978\n",
      "Epoch [752/1000], Loss: 0.0183\n",
      "Correlation: -0.039363 \n",
      " P-Value: 0.584821\n",
      "Epoch [753/1000], Loss: 0.0183\n",
      "Correlation: -0.039317 \n",
      " P-Value: 0.585261\n",
      "Epoch [754/1000], Loss: 0.0183\n",
      "Correlation: -0.039257 \n",
      " P-Value: 0.585835\n",
      "Epoch [755/1000], Loss: 0.0183\n",
      "Correlation: -0.039348 \n",
      " P-Value: 0.584970\n",
      "Epoch [756/1000], Loss: 0.0183\n",
      "Correlation: -0.039422 \n",
      " P-Value: 0.584262\n",
      "Epoch [757/1000], Loss: 0.0183\n",
      "Correlation: -0.039247 \n",
      " P-Value: 0.585933\n",
      "Epoch [758/1000], Loss: 0.0183\n",
      "Correlation: -0.039123 \n",
      " P-Value: 0.587120\n",
      "Epoch [759/1000], Loss: 0.0183\n",
      "Correlation: -0.039285 \n",
      " P-Value: 0.585567\n",
      "Epoch [760/1000], Loss: 0.0183\n",
      "Correlation: -0.039284 \n",
      " P-Value: 0.585580\n",
      "Epoch [761/1000], Loss: 0.0183\n",
      "Correlation: -0.039119 \n",
      " P-Value: 0.587157\n",
      "Epoch [762/1000], Loss: 0.0183\n",
      "Correlation: -0.039114 \n",
      " P-Value: 0.587207\n",
      "Epoch [763/1000], Loss: 0.0183\n",
      "Correlation: -0.039226 \n",
      " P-Value: 0.586136\n",
      "Epoch [764/1000], Loss: 0.0183\n",
      "Correlation: -0.039177 \n",
      " P-Value: 0.586598\n",
      "Epoch [765/1000], Loss: 0.0183\n",
      "Correlation: -0.039066 \n",
      " P-Value: 0.587659\n",
      "Epoch [766/1000], Loss: 0.0183\n",
      "Correlation: -0.039104 \n",
      " P-Value: 0.587300\n",
      "Epoch [767/1000], Loss: 0.0183\n",
      "Correlation: -0.039090 \n",
      " P-Value: 0.587434\n",
      "Epoch [768/1000], Loss: 0.0183\n",
      "Correlation: -0.039115 \n",
      " P-Value: 0.587194\n",
      "Epoch [769/1000], Loss: 0.0183\n",
      "Correlation: -0.039026 \n",
      " P-Value: 0.588041\n",
      "Epoch [770/1000], Loss: 0.0183\n",
      "Correlation: -0.038965 \n",
      " P-Value: 0.588626\n",
      "Epoch [771/1000], Loss: 0.0183\n",
      "Correlation: -0.039037 \n",
      " P-Value: 0.587938\n",
      "Epoch [772/1000], Loss: 0.0183\n",
      "Correlation: -0.039132 \n",
      " P-Value: 0.587034\n",
      "Epoch [773/1000], Loss: 0.0183\n",
      "Correlation: -0.039036 \n",
      " P-Value: 0.587947\n",
      "Epoch [774/1000], Loss: 0.0183\n",
      "Correlation: -0.038891 \n",
      " P-Value: 0.589335\n",
      "Epoch [775/1000], Loss: 0.0183\n",
      "Correlation: -0.038984 \n",
      " P-Value: 0.588450\n",
      "Epoch [776/1000], Loss: 0.0183\n",
      "Correlation: -0.038995 \n",
      " P-Value: 0.588337\n",
      "Epoch [777/1000], Loss: 0.0183\n",
      "Correlation: -0.038918 \n",
      " P-Value: 0.589083\n",
      "Epoch [778/1000], Loss: 0.0183\n",
      "Correlation: -0.038894 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " P-Value: 0.589310\n",
      "Epoch [779/1000], Loss: 0.0183\n",
      "Correlation: -0.038961 \n",
      " P-Value: 0.588666\n",
      "Epoch [780/1000], Loss: 0.0183\n",
      "Correlation: -0.038855 \n",
      " P-Value: 0.589679\n",
      "Epoch [781/1000], Loss: 0.0183\n",
      "Correlation: -0.038806 \n",
      " P-Value: 0.590153\n",
      "Epoch [782/1000], Loss: 0.0183\n",
      "Correlation: -0.038920 \n",
      " P-Value: 0.589063\n",
      "Epoch [783/1000], Loss: 0.0183\n",
      "Correlation: -0.038891 \n",
      " P-Value: 0.589342\n",
      "Epoch [784/1000], Loss: 0.0183\n",
      "Correlation: -0.038724 \n",
      " P-Value: 0.590940\n",
      "Epoch [785/1000], Loss: 0.0183\n",
      "Correlation: -0.038821 \n",
      " P-Value: 0.590012\n",
      "Epoch [786/1000], Loss: 0.0183\n",
      "Correlation: -0.038947 \n",
      " P-Value: 0.588797\n",
      "Epoch [787/1000], Loss: 0.0183\n",
      "Correlation: -0.038747 \n",
      " P-Value: 0.590721\n",
      "Epoch [788/1000], Loss: 0.0183\n",
      "Correlation: -0.038542 \n",
      " P-Value: 0.592682\n",
      "Epoch [789/1000], Loss: 0.0183\n",
      "Correlation: -0.038781 \n",
      " P-Value: 0.590393\n",
      "Epoch [790/1000], Loss: 0.0183\n",
      "Correlation: -0.038788 \n",
      " P-Value: 0.590329\n",
      "Epoch [791/1000], Loss: 0.0183\n",
      "Correlation: -0.038584 \n",
      " P-Value: 0.592286\n",
      "Epoch [792/1000], Loss: 0.0183\n",
      "Correlation: -0.038667 \n",
      " P-Value: 0.591490\n",
      "Epoch [793/1000], Loss: 0.0183\n",
      "Correlation: -0.038776 \n",
      " P-Value: 0.590436\n",
      "Epoch [794/1000], Loss: 0.0183\n",
      "Correlation: -0.038636 \n",
      " P-Value: 0.591788\n",
      "Epoch [795/1000], Loss: 0.0183\n",
      "Correlation: -0.038504 \n",
      " P-Value: 0.593049\n",
      "Epoch [796/1000], Loss: 0.0183\n",
      "Correlation: -0.038673 \n",
      " P-Value: 0.591427\n",
      "Epoch [797/1000], Loss: 0.0183\n",
      "Correlation: -0.038605 \n",
      " P-Value: 0.592078\n",
      "Epoch [798/1000], Loss: 0.0183\n",
      "Correlation: -0.038568 \n",
      " P-Value: 0.592438\n",
      "Epoch [799/1000], Loss: 0.0183\n",
      "Correlation: -0.038600 \n",
      " P-Value: 0.592125\n",
      "Epoch [800/1000], Loss: 0.0183\n",
      "Correlation: -0.038519 \n",
      " P-Value: 0.592911\n",
      "Epoch [801/1000], Loss: 0.0183\n",
      "Correlation: -0.038428 \n",
      " P-Value: 0.593780\n",
      "Epoch [802/1000], Loss: 0.0183\n",
      "Correlation: -0.038416 \n",
      " P-Value: 0.593895\n",
      "Epoch [803/1000], Loss: 0.0183\n",
      "Correlation: -0.038543 \n",
      " P-Value: 0.592680\n",
      "Epoch [804/1000], Loss: 0.0183\n",
      "Correlation: -0.038399 \n",
      " P-Value: 0.594056\n",
      "Epoch [805/1000], Loss: 0.0183\n",
      "Correlation: -0.038388 \n",
      " P-Value: 0.594169\n",
      "Epoch [806/1000], Loss: 0.0183\n",
      "Correlation: -0.038343 \n",
      " P-Value: 0.594594\n",
      "Epoch [807/1000], Loss: 0.0183\n",
      "Correlation: -0.038366 \n",
      " P-Value: 0.594375\n",
      "Epoch [808/1000], Loss: 0.0183\n",
      "Correlation: -0.038363 \n",
      " P-Value: 0.594409\n",
      "Epoch [809/1000], Loss: 0.0183\n",
      "Correlation: -0.038300 \n",
      " P-Value: 0.595014\n",
      "Epoch [810/1000], Loss: 0.0183\n",
      "Correlation: -0.038321 \n",
      " P-Value: 0.594809\n",
      "Epoch [811/1000], Loss: 0.0183\n",
      "Correlation: -0.038414 \n",
      " P-Value: 0.593911\n",
      "Epoch [812/1000], Loss: 0.0183\n",
      "Correlation: -0.038273 \n",
      " P-Value: 0.595271\n",
      "Epoch [813/1000], Loss: 0.0183\n",
      "Correlation: -0.038109 \n",
      " P-Value: 0.596847\n",
      "Epoch [814/1000], Loss: 0.0183\n",
      "Correlation: -0.038171 \n",
      " P-Value: 0.596254\n",
      "Epoch [815/1000], Loss: 0.0183\n",
      "Correlation: -0.038361 \n",
      " P-Value: 0.594427\n",
      "Epoch [816/1000], Loss: 0.0183\n",
      "Correlation: -0.038297 \n",
      " P-Value: 0.595043\n",
      "Epoch [817/1000], Loss: 0.0183\n",
      "Correlation: -0.038067 \n",
      " P-Value: 0.597256\n",
      "Epoch [818/1000], Loss: 0.0183\n",
      "Correlation: -0.038105 \n",
      " P-Value: 0.596890\n",
      "Epoch [819/1000], Loss: 0.0183\n",
      "Correlation: -0.038212 \n",
      " P-Value: 0.595863\n",
      "Epoch [820/1000], Loss: 0.0183\n",
      "Correlation: -0.038162 \n",
      " P-Value: 0.596345\n",
      "Epoch [821/1000], Loss: 0.0183\n",
      "Correlation: -0.038150 \n",
      " P-Value: 0.596453\n",
      "Epoch [822/1000], Loss: 0.0183\n",
      "Correlation: -0.038023 \n",
      " P-Value: 0.597677\n",
      "Epoch [823/1000], Loss: 0.0183\n",
      "Correlation: -0.037948 \n",
      " P-Value: 0.598402\n",
      "Epoch [824/1000], Loss: 0.0183\n",
      "Correlation: -0.038132 \n",
      " P-Value: 0.596630\n",
      "Epoch [825/1000], Loss: 0.0183\n",
      "Correlation: -0.038063 \n",
      " P-Value: 0.597299\n",
      "Epoch [826/1000], Loss: 0.0183\n",
      "Correlation: -0.037980 \n",
      " P-Value: 0.598099\n",
      "Epoch [827/1000], Loss: 0.0183\n",
      "Correlation: -0.038048 \n",
      " P-Value: 0.597436\n",
      "Epoch [828/1000], Loss: 0.0183\n",
      "Correlation: -0.037980 \n",
      " P-Value: 0.598093\n",
      "Epoch [829/1000], Loss: 0.0183\n",
      "Correlation: -0.037897 \n",
      " P-Value: 0.598898\n",
      "Epoch [830/1000], Loss: 0.0183\n",
      "Correlation: -0.037890 \n",
      " P-Value: 0.598964\n",
      "Epoch [831/1000], Loss: 0.0183\n",
      "Correlation: -0.037982 \n",
      " P-Value: 0.598075\n",
      "Epoch [832/1000], Loss: 0.0183\n",
      "Correlation: -0.037953 \n",
      " P-Value: 0.598355\n",
      "Epoch [833/1000], Loss: 0.0183\n",
      "Correlation: -0.037872 \n",
      " P-Value: 0.599138\n",
      "Epoch [834/1000], Loss: 0.0183\n",
      "Correlation: -0.037767 \n",
      " P-Value: 0.600154\n",
      "Epoch [835/1000], Loss: 0.0183\n",
      "Correlation: -0.037797 \n",
      " P-Value: 0.599863\n",
      "Epoch [836/1000], Loss: 0.0183\n",
      "Correlation: -0.037922 \n",
      " P-Value: 0.598653\n",
      "Epoch [837/1000], Loss: 0.0183\n",
      "Correlation: -0.037894 \n",
      " P-Value: 0.598928\n",
      "Epoch [838/1000], Loss: 0.0183\n",
      "Correlation: -0.037772 \n",
      " P-Value: 0.600105\n",
      "Epoch [839/1000], Loss: 0.0183\n",
      "Correlation: -0.037693 \n",
      " P-Value: 0.600870\n",
      "Epoch [840/1000], Loss: 0.0183\n",
      "Correlation: -0.037832 \n",
      " P-Value: 0.599520\n",
      "Epoch [841/1000], Loss: 0.0183\n",
      "Correlation: -0.037860 \n",
      " P-Value: 0.599257\n",
      "Epoch [842/1000], Loss: 0.0183\n",
      "Correlation: -0.037836 \n",
      " P-Value: 0.599486\n",
      "Epoch [843/1000], Loss: 0.0183\n",
      "Correlation: -0.037727 \n",
      " P-Value: 0.600533\n",
      "Epoch [844/1000], Loss: 0.0183\n",
      "Correlation: -0.037677 \n",
      " P-Value: 0.601024\n",
      "Epoch [845/1000], Loss: 0.0183\n",
      "Correlation: -0.037795 \n",
      " P-Value: 0.599884\n",
      "Epoch [846/1000], Loss: 0.0183\n",
      "Correlation: -0.037766 \n",
      " P-Value: 0.600162\n",
      "Epoch [847/1000], Loss: 0.0183\n",
      "Correlation: -0.037626 \n",
      " P-Value: 0.601518\n",
      "Epoch [848/1000], Loss: 0.0183\n",
      "Correlation: -0.037674 \n",
      " P-Value: 0.601048\n",
      "Epoch [849/1000], Loss: 0.0183\n",
      "Correlation: -0.037616 \n",
      " P-Value: 0.601614\n",
      "Epoch [850/1000], Loss: 0.0183\n",
      "Correlation: -0.037657 \n",
      " P-Value: 0.601215\n",
      "Epoch [851/1000], Loss: 0.0183\n",
      "Correlation: -0.037688 \n",
      " P-Value: 0.600914\n",
      "Epoch [852/1000], Loss: 0.0183\n",
      "Correlation: -0.037493 \n",
      " P-Value: 0.602796\n",
      "Epoch [853/1000], Loss: 0.0183\n",
      "Correlation: -0.037427 \n",
      " P-Value: 0.603436\n",
      "Epoch [854/1000], Loss: 0.0183\n",
      "Correlation: -0.037585 \n",
      " P-Value: 0.601906\n",
      "Epoch [855/1000], Loss: 0.0183\n",
      "Correlation: -0.037606 \n",
      " P-Value: 0.601709\n",
      "Epoch [856/1000], Loss: 0.0183\n",
      "Correlation: -0.037496 \n",
      " P-Value: 0.602768\n",
      "Epoch [857/1000], Loss: 0.0183\n",
      "Correlation: -0.037497 \n",
      " P-Value: 0.602763\n",
      "Epoch [858/1000], Loss: 0.0183\n",
      "Correlation: -0.037463 \n",
      " P-Value: 0.603093\n",
      "Epoch [859/1000], Loss: 0.0183\n",
      "Correlation: -0.037401 \n",
      " P-Value: 0.603693\n",
      "Epoch [860/1000], Loss: 0.0183\n",
      "Correlation: -0.037423 \n",
      " P-Value: 0.603473\n",
      "Epoch [861/1000], Loss: 0.0183\n",
      "Correlation: -0.037412 \n",
      " P-Value: 0.603584\n",
      "Epoch [862/1000], Loss: 0.0183\n",
      "Correlation: -0.037327 \n",
      " P-Value: 0.604408\n",
      "Epoch [863/1000], Loss: 0.0183\n",
      "Correlation: -0.037325 \n",
      " P-Value: 0.604427\n",
      "Epoch [864/1000], Loss: 0.0183\n",
      "Correlation: -0.037347 \n",
      " P-Value: 0.604215\n",
      "Epoch [865/1000], Loss: 0.0183\n",
      "Correlation: -0.037322 \n",
      " P-Value: 0.604459\n",
      "Epoch [866/1000], Loss: 0.0183\n",
      "Correlation: -0.037273 \n",
      " P-Value: 0.604934\n",
      "Epoch [867/1000], Loss: 0.0183\n",
      "Correlation: -0.037350 \n",
      " P-Value: 0.604187\n",
      "Epoch [868/1000], Loss: 0.0183\n",
      "Correlation: -0.037356 \n",
      " P-Value: 0.604125\n",
      "Epoch [869/1000], Loss: 0.0183\n",
      "Correlation: -0.037226 \n",
      " P-Value: 0.605387\n",
      "Epoch [870/1000], Loss: 0.0183\n",
      "Correlation: -0.037149 \n",
      " P-Value: 0.606132\n",
      "Epoch [871/1000], Loss: 0.0183\n",
      "Correlation: -0.037237 \n",
      " P-Value: 0.605284\n",
      "Epoch [872/1000], Loss: 0.0183\n",
      "Correlation: -0.037290 \n",
      " P-Value: 0.604767\n",
      "Epoch [873/1000], Loss: 0.0183\n",
      "Correlation: -0.037195 \n",
      " P-Value: 0.605691\n",
      "Epoch [874/1000], Loss: 0.0183\n",
      "Correlation: -0.037181 \n",
      " P-Value: 0.605820\n",
      "Epoch [875/1000], Loss: 0.0183\n",
      "Correlation: -0.037055 \n",
      " P-Value: 0.607042\n",
      "Epoch [876/1000], Loss: 0.0183\n",
      "Correlation: -0.037065 \n",
      " P-Value: 0.606948\n",
      "Epoch [877/1000], Loss: 0.0183\n",
      "Correlation: -0.037209 \n",
      " P-Value: 0.605548\n",
      "Epoch [878/1000], Loss: 0.0183\n",
      "Correlation: -0.037172 \n",
      " P-Value: 0.605914\n",
      "Epoch [879/1000], Loss: 0.0183\n",
      "Correlation: -0.036948 \n",
      " P-Value: 0.608083\n",
      "Epoch [880/1000], Loss: 0.0183\n",
      "Correlation: -0.037004 \n",
      " P-Value: 0.607540\n",
      "Epoch [881/1000], Loss: 0.0183\n",
      "Correlation: -0.037139 \n",
      " P-Value: 0.606232\n",
      "Epoch [882/1000], Loss: 0.0183\n",
      "Correlation: -0.037098 \n",
      " P-Value: 0.606624\n",
      "Epoch [883/1000], Loss: 0.0183\n",
      "Correlation: -0.037054 \n",
      " P-Value: 0.607058\n",
      "Epoch [884/1000], Loss: 0.0183\n",
      "Correlation: -0.037023 \n",
      " P-Value: 0.607358\n",
      "Epoch [885/1000], Loss: 0.0183\n",
      "Correlation: -0.036918 \n",
      " P-Value: 0.608373\n",
      "Epoch [886/1000], Loss: 0.0183\n",
      "Correlation: -0.037005 \n",
      " P-Value: 0.607528\n",
      "Epoch [887/1000], Loss: 0.0183\n",
      "Correlation: -0.036983 \n",
      " P-Value: 0.607741\n",
      "Epoch [888/1000], Loss: 0.0183\n",
      "Correlation: -0.036946 \n",
      " P-Value: 0.608107\n",
      "Epoch [889/1000], Loss: 0.0183\n",
      "Correlation: -0.036919 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " P-Value: 0.608362\n",
      "Epoch [890/1000], Loss: 0.0183\n",
      "Correlation: -0.036835 \n",
      " P-Value: 0.609185\n",
      "Epoch [891/1000], Loss: 0.0183\n",
      "Correlation: -0.036885 \n",
      " P-Value: 0.608698\n",
      "Epoch [892/1000], Loss: 0.0183\n",
      "Correlation: -0.036939 \n",
      " P-Value: 0.608175\n",
      "Epoch [893/1000], Loss: 0.0183\n",
      "Correlation: -0.036828 \n",
      " P-Value: 0.609255\n",
      "Epoch [894/1000], Loss: 0.0183\n",
      "Correlation: -0.036620 \n",
      " P-Value: 0.611273\n",
      "Epoch [895/1000], Loss: 0.0183\n",
      "Correlation: -0.036795 \n",
      " P-Value: 0.609573\n",
      "Epoch [896/1000], Loss: 0.0183\n",
      "Correlation: -0.036945 \n",
      " P-Value: 0.608111\n",
      "Epoch [897/1000], Loss: 0.0183\n",
      "Correlation: -0.036818 \n",
      " P-Value: 0.609346\n",
      "Epoch [898/1000], Loss: 0.0183\n",
      "Correlation: -0.036701 \n",
      " P-Value: 0.610489\n",
      "Epoch [899/1000], Loss: 0.0183\n",
      "Correlation: -0.036645 \n",
      " P-Value: 0.611034\n",
      "Epoch [900/1000], Loss: 0.0183\n",
      "Correlation: -0.036695 \n",
      " P-Value: 0.610541\n",
      "Epoch [901/1000], Loss: 0.0183\n",
      "Correlation: -0.036788 \n",
      " P-Value: 0.609644\n",
      "Epoch [902/1000], Loss: 0.0183\n",
      "Correlation: -0.036655 \n",
      " P-Value: 0.610931\n",
      "Epoch [903/1000], Loss: 0.0183\n",
      "Correlation: -0.036545 \n",
      " P-Value: 0.612003\n",
      "Epoch [904/1000], Loss: 0.0183\n",
      "Correlation: -0.036540 \n",
      " P-Value: 0.612054\n",
      "Epoch [905/1000], Loss: 0.0183\n",
      "Correlation: -0.036693 \n",
      " P-Value: 0.610564\n",
      "Epoch [906/1000], Loss: 0.0183\n",
      "Correlation: -0.036734 \n",
      " P-Value: 0.610167\n",
      "Epoch [907/1000], Loss: 0.0183\n",
      "Correlation: -0.036523 \n",
      " P-Value: 0.612221\n",
      "Epoch [908/1000], Loss: 0.0183\n",
      "Correlation: -0.036542 \n",
      " P-Value: 0.612036\n",
      "Epoch [909/1000], Loss: 0.0183\n",
      "Correlation: -0.036586 \n",
      " P-Value: 0.611606\n",
      "Epoch [910/1000], Loss: 0.0183\n",
      "Correlation: -0.036560 \n",
      " P-Value: 0.611862\n",
      "Epoch [911/1000], Loss: 0.0183\n",
      "Correlation: -0.036457 \n",
      " P-Value: 0.612867\n",
      "Epoch [912/1000], Loss: 0.0183\n",
      "Correlation: -0.036469 \n",
      " P-Value: 0.612750\n",
      "Epoch [913/1000], Loss: 0.0183\n",
      "Correlation: -0.036474 \n",
      " P-Value: 0.612697\n",
      "Epoch [914/1000], Loss: 0.0183\n",
      "Correlation: -0.036446 \n",
      " P-Value: 0.612968\n",
      "Epoch [915/1000], Loss: 0.0183\n",
      "Correlation: -0.036433 \n",
      " P-Value: 0.613100\n",
      "Epoch [916/1000], Loss: 0.0183\n",
      "Correlation: -0.036393 \n",
      " P-Value: 0.613486\n",
      "Epoch [917/1000], Loss: 0.0183\n",
      "Correlation: -0.036329 \n",
      " P-Value: 0.614110\n",
      "Epoch [918/1000], Loss: 0.0183\n",
      "Correlation: -0.036473 \n",
      " P-Value: 0.612711\n",
      "Epoch [919/1000], Loss: 0.0183\n",
      "Correlation: -0.036446 \n",
      " P-Value: 0.612974\n",
      "Epoch [920/1000], Loss: 0.0183\n",
      "Correlation: -0.036435 \n",
      " P-Value: 0.613078\n",
      "Epoch [921/1000], Loss: 0.0183\n",
      "Correlation: -0.036237 \n",
      " P-Value: 0.615006\n",
      "Epoch [922/1000], Loss: 0.0183\n",
      "Correlation: -0.036299 \n",
      " P-Value: 0.614409\n",
      "Epoch [923/1000], Loss: 0.0183\n",
      "Correlation: -0.036364 \n",
      " P-Value: 0.613768\n",
      "Epoch [924/1000], Loss: 0.0183\n",
      "Correlation: -0.036191 \n",
      " P-Value: 0.615457\n",
      "Epoch [925/1000], Loss: 0.0183\n",
      "Correlation: -0.036263 \n",
      " P-Value: 0.614761\n",
      "Epoch [926/1000], Loss: 0.0183\n",
      "Correlation: -0.036185 \n",
      " P-Value: 0.615519\n",
      "Epoch [927/1000], Loss: 0.0183\n",
      "Correlation: -0.036071 \n",
      " P-Value: 0.616633\n",
      "Epoch [928/1000], Loss: 0.0183\n",
      "Correlation: -0.036289 \n",
      " P-Value: 0.614498\n",
      "Epoch [929/1000], Loss: 0.0183\n",
      "Correlation: -0.036125 \n",
      " P-Value: 0.616102\n",
      "Epoch [930/1000], Loss: 0.0183\n",
      "Correlation: -0.035977 \n",
      " P-Value: 0.617556\n",
      "Epoch [931/1000], Loss: 0.0183\n",
      "Correlation: -0.036181 \n",
      " P-Value: 0.615554\n",
      "Epoch [932/1000], Loss: 0.0183\n",
      "Correlation: -0.036112 \n",
      " P-Value: 0.616232\n",
      "Epoch [933/1000], Loss: 0.0183\n",
      "Correlation: -0.036016 \n",
      " P-Value: 0.617167\n",
      "Epoch [934/1000], Loss: 0.0183\n",
      "Correlation: -0.036126 \n",
      " P-Value: 0.616092\n",
      "Epoch [935/1000], Loss: 0.0183\n",
      "Correlation: -0.035938 \n",
      " P-Value: 0.617935\n",
      "Epoch [936/1000], Loss: 0.0183\n",
      "Correlation: -0.036018 \n",
      " P-Value: 0.617150\n",
      "Epoch [937/1000], Loss: 0.0183\n",
      "Correlation: -0.036104 \n",
      " P-Value: 0.616311\n",
      "Epoch [938/1000], Loss: 0.0183\n",
      "Correlation: -0.035936 \n",
      " P-Value: 0.617948\n",
      "Epoch [939/1000], Loss: 0.0183\n",
      "Correlation: -0.035992 \n",
      " P-Value: 0.617403\n",
      "Epoch [940/1000], Loss: 0.0183\n",
      "Correlation: -0.035920 \n",
      " P-Value: 0.618113\n",
      "Epoch [941/1000], Loss: 0.0183\n",
      "Correlation: -0.036008 \n",
      " P-Value: 0.617246\n",
      "Epoch [942/1000], Loss: 0.0183\n",
      "Correlation: -0.036006 \n",
      " P-Value: 0.617269\n",
      "Epoch [943/1000], Loss: 0.0183\n",
      "Correlation: -0.035880 \n",
      " P-Value: 0.618496\n",
      "Epoch [944/1000], Loss: 0.0183\n",
      "Correlation: -0.035876 \n",
      " P-Value: 0.618538\n",
      "Epoch [945/1000], Loss: 0.0183\n",
      "Correlation: -0.035913 \n",
      " P-Value: 0.618182\n",
      "Epoch [946/1000], Loss: 0.0183\n",
      "Correlation: -0.035879 \n",
      " P-Value: 0.618509\n",
      "Epoch [947/1000], Loss: 0.0183\n",
      "Correlation: -0.035769 \n",
      " P-Value: 0.619583\n",
      "Epoch [948/1000], Loss: 0.0183\n",
      "Correlation: -0.035730 \n",
      " P-Value: 0.619968\n",
      "Epoch [949/1000], Loss: 0.0183\n",
      "Correlation: -0.035855 \n",
      " P-Value: 0.618746\n",
      "Epoch [950/1000], Loss: 0.0183\n",
      "Correlation: -0.035781 \n",
      " P-Value: 0.619470\n",
      "Epoch [951/1000], Loss: 0.0183\n",
      "Correlation: -0.035768 \n",
      " P-Value: 0.619601\n",
      "Epoch [952/1000], Loss: 0.0183\n",
      "Correlation: -0.035702 \n",
      " P-Value: 0.620241\n",
      "Epoch [953/1000], Loss: 0.0183\n",
      "Correlation: -0.035715 \n",
      " P-Value: 0.620113\n",
      "Epoch [954/1000], Loss: 0.0183\n",
      "Correlation: -0.035742 \n",
      " P-Value: 0.619856\n",
      "Epoch [955/1000], Loss: 0.0183\n",
      "Correlation: -0.035636 \n",
      " P-Value: 0.620888\n",
      "Epoch [956/1000], Loss: 0.0183\n",
      "Correlation: -0.035569 \n",
      " P-Value: 0.621546\n",
      "Epoch [957/1000], Loss: 0.0183\n",
      "Correlation: -0.035710 \n",
      " P-Value: 0.620168\n",
      "Epoch [958/1000], Loss: 0.0183\n",
      "Correlation: -0.035537 \n",
      " P-Value: 0.621864\n",
      "Epoch [959/1000], Loss: 0.0183\n",
      "Correlation: -0.035539 \n",
      " P-Value: 0.621843\n",
      "Epoch [960/1000], Loss: 0.0183\n",
      "Correlation: -0.035471 \n",
      " P-Value: 0.622511\n",
      "Epoch [961/1000], Loss: 0.0183\n",
      "Correlation: -0.035582 \n",
      " P-Value: 0.621416\n",
      "Epoch [962/1000], Loss: 0.0183\n",
      "Correlation: -0.035513 \n",
      " P-Value: 0.622101\n",
      "Epoch [963/1000], Loss: 0.0183\n",
      "Correlation: -0.035454 \n",
      " P-Value: 0.622675\n",
      "Epoch [964/1000], Loss: 0.0183\n",
      "Correlation: -0.035424 \n",
      " P-Value: 0.622967\n",
      "Epoch [965/1000], Loss: 0.0183\n",
      "Correlation: -0.035534 \n",
      " P-Value: 0.621893\n",
      "Epoch [966/1000], Loss: 0.0183\n",
      "Correlation: -0.035444 \n",
      " P-Value: 0.622777\n",
      "Epoch [967/1000], Loss: 0.0183\n",
      "Correlation: -0.035502 \n",
      " P-Value: 0.622210\n",
      "Epoch [968/1000], Loss: 0.0183\n",
      "Correlation: -0.035352 \n",
      " P-Value: 0.623680\n",
      "Epoch [969/1000], Loss: 0.0183\n",
      "Correlation: -0.035329 \n",
      " P-Value: 0.623902\n",
      "Epoch [970/1000], Loss: 0.0183\n",
      "Correlation: -0.035387 \n",
      " P-Value: 0.623333\n",
      "Epoch [971/1000], Loss: 0.0183\n",
      "Correlation: -0.035379 \n",
      " P-Value: 0.623415\n",
      "Epoch [972/1000], Loss: 0.0183\n",
      "Correlation: -0.035384 \n",
      " P-Value: 0.623362\n",
      "Epoch [973/1000], Loss: 0.0183\n",
      "Correlation: -0.035405 \n",
      " P-Value: 0.623157\n",
      "Epoch [974/1000], Loss: 0.0183\n",
      "Correlation: -0.035308 \n",
      " P-Value: 0.624114\n",
      "Epoch [975/1000], Loss: 0.0183\n",
      "Correlation: -0.035208 \n",
      " P-Value: 0.625097\n",
      "Epoch [976/1000], Loss: 0.0183\n",
      "Correlation: -0.035378 \n",
      " P-Value: 0.623423\n",
      "Epoch [977/1000], Loss: 0.0183\n",
      "Correlation: -0.035316 \n",
      " P-Value: 0.624029\n",
      "Epoch [978/1000], Loss: 0.0183\n",
      "Correlation: -0.035259 \n",
      " P-Value: 0.624592\n",
      "Epoch [979/1000], Loss: 0.0183\n",
      "Correlation: -0.035226 \n",
      " P-Value: 0.624919\n",
      "Epoch [980/1000], Loss: 0.0183\n",
      "Correlation: -0.035202 \n",
      " P-Value: 0.625152\n",
      "Epoch [981/1000], Loss: 0.0183\n",
      "Correlation: -0.035158 \n",
      " P-Value: 0.625590\n",
      "Epoch [982/1000], Loss: 0.0183\n",
      "Correlation: -0.035108 \n",
      " P-Value: 0.626075\n",
      "Epoch [983/1000], Loss: 0.0183\n",
      "Correlation: -0.035246 \n",
      " P-Value: 0.624724\n",
      "Epoch [984/1000], Loss: 0.0183\n",
      "Correlation: -0.035182 \n",
      " P-Value: 0.625347\n",
      "Epoch [985/1000], Loss: 0.0183\n",
      "Correlation: -0.035093 \n",
      " P-Value: 0.626229\n",
      "Epoch [986/1000], Loss: 0.0183\n",
      "Correlation: -0.035179 \n",
      " P-Value: 0.625379\n",
      "Epoch [987/1000], Loss: 0.0183\n",
      "Correlation: -0.035142 \n",
      " P-Value: 0.625738\n",
      "Epoch [988/1000], Loss: 0.0183\n",
      "Correlation: -0.035135 \n",
      " P-Value: 0.625815\n",
      "Epoch [989/1000], Loss: 0.0183\n",
      "Correlation: -0.035238 \n",
      " P-Value: 0.624796\n",
      "Epoch [990/1000], Loss: 0.0183\n",
      "Correlation: -0.035126 \n",
      " P-Value: 0.625902\n",
      "Epoch [991/1000], Loss: 0.0183\n",
      "Correlation: -0.035059 \n",
      " P-Value: 0.626561\n",
      "Epoch [992/1000], Loss: 0.0183\n",
      "Correlation: -0.034986 \n",
      " P-Value: 0.627275\n",
      "Epoch [993/1000], Loss: 0.0183\n",
      "Correlation: -0.035014 \n",
      " P-Value: 0.627007\n",
      "Epoch [994/1000], Loss: 0.0183\n",
      "Correlation: -0.035083 \n",
      " P-Value: 0.626328\n",
      "Epoch [995/1000], Loss: 0.0183\n",
      "Correlation: -0.035088 \n",
      " P-Value: 0.626271\n",
      "Epoch [996/1000], Loss: 0.0183\n",
      "Correlation: -0.034893 \n",
      " P-Value: 0.628195\n",
      "Epoch [997/1000], Loss: 0.0183\n",
      "Correlation: -0.034990 \n",
      " P-Value: 0.627235\n",
      "Epoch [998/1000], Loss: 0.0183\n",
      "Correlation: -0.035069 \n",
      " P-Value: 0.626458\n",
      "Epoch [999/1000], Loss: 0.0183\n",
      "Correlation: -0.034851 \n",
      " P-Value: 0.628611\n",
      "Epoch [1000/1000], Loss: 0.0183\n"
     ]
    }
   ],
   "source": [
    "for epoch in range (num_epochs):\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        net.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = net.forward(data).to(device)\n",
    "        loss = net.loss_fn(output, target)\n",
    "        total_loss += loss\n",
    "        counter += 1\n",
    "        #print(list(self.parameters()))\n",
    "        #print(\"current loss is: %f\"%loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    check_correlation(test_loader, net)    \n",
    "    epoch += 1\n",
    "\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "            .format(epoch, num_epochs, total_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
