{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions ##\n",
    "971 yeast\n",
    "\n",
    "linear regression, snp weight vector, 选top 10%， 20%， 30%。。\n",
    "\n",
    "分别用这些snp来再跑linear regression的model( no gene)\n",
    "\n",
    "再把这些snp放到gene上，再跑一次linear regression model\n",
    "\n",
    " \n",
    "\n",
    "神经网络input snp\n",
    "\n",
    "最多两千个feature，用linear regression来筛选\n",
    "\n",
    " \n",
    "\n",
    "还要考虑promoter， enhancer（先不搞）\n",
    "\n",
    " \n",
    "\n",
    "1. 先选snp （Science paper：看snp preproccesing的procedure）<5 删掉\n",
    "\n",
    "2. fit linear regression model\n",
    "\n",
    "3. 选大约2000个\n",
    "\n",
    "4. 再用神经网络 （pytorch）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import bisect as bs\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import scipy.stats as stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18921</th>\n",
       "      <th>18922</th>\n",
       "      <th>18923</th>\n",
       "      <th>18924</th>\n",
       "      <th>18925</th>\n",
       "      <th>18926</th>\n",
       "      <th>18927</th>\n",
       "      <th>18928</th>\n",
       "      <th>18929</th>\n",
       "      <th>18930</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAA_AAA</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAB_AAB</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAC_AAC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAD_AAD</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAE_AAE</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 18931 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4      5      6      7      8      9      \\\n",
       "AAA_AAA      0      0      0      1      0      0      0      0      0      0   \n",
       "AAB_AAB      0      0      0      0      1      0      0      0      0      0   \n",
       "AAC_AAC      0      0      0      1      0      0      0      0      0      0   \n",
       "AAD_AAD      0      0      0      0      1      1      1      0      0      0   \n",
       "AAE_AAE      0      0      0      1      0      0      0      0      0      0   \n",
       "\n",
       "         ...  18921  18922  18923  18924  18925  18926  18927  18928  18929  \\\n",
       "AAA_AAA  ...      0      0      0      0      0      0      1      0      0   \n",
       "AAB_AAB  ...      0      0      0      0      0      0      1      0      0   \n",
       "AAC_AAC  ...      0      0      0      0      0      0      0      0      0   \n",
       "AAD_AAD  ...      0      0      1      1      1      1      0      0      1   \n",
       "AAE_AAE  ...      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "         18930  \n",
       "AAA_AAA      0  \n",
       "AAB_AAB      0  \n",
       "AAC_AAC      0  \n",
       "AAD_AAD      1  \n",
       "AAE_AAE      0  \n",
       "\n",
       "[5 rows x 18931 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNP_url = \"./1011geneSNP.csv\"\n",
    "Gene_url = \"./GCF_000146045.2_R64_genomic.gff\"\n",
    "\n",
    "snp_yeast_matrix = pd.read_csv(SNP_url, sep = ',' , error_bad_lines=False)\n",
    "del snp_yeast_matrix['#CHROM']\n",
    "del snp_yeast_matrix['POS']\n",
    "del snp_yeast_matrix['REF']\n",
    "del snp_yeast_matrix['ALT']\n",
    "del snp_yeast_matrix['ANN[*].GENE']\n",
    "del snp_yeast_matrix['ANN[*].GENEID']\n",
    "index_list = snp_yeast_matrix[\"ID\"].tolist()\n",
    "del snp_yeast_matrix['ID']\n",
    "snp_yeast_matrix = snp_yeast_matrix.transpose()\n",
    "column_names = list(snp_yeast_matrix.index)\n",
    "to_delete = [\"ABC\", \"ABF\", \"ASP\", \"BGS\",\"BGF\", \"BHL\",\"BHQ\",\"BID\",\"BIR\",\"BKG\",\"BKN\",\"CFH\",\"SACE_YAL\",\"SACE_YBA\",\n",
    "             \"SACE_YBB\",\"SACE_YBM\",\"SACE_YBN\",\"SACE_YBO\",\"SACE_YBP\",\"SACE_YCJ\",\"SACE_YCS\",\"SACE_YCT\",\"SACE_YCU\",\"SACE_YCV\",\"SACE_YCW\",\n",
    "             \"SACE_YCX\",\"SACE_YCY\",\"SACE_YCZ\",\"SACE_YDA\",\"SACE_YDB\",\"SACE_YDC\",\"SACE_YDD\", \"SACE_YDE\",\"SACE_YDF\",\"SACE_YDG\",\"SACE_YDH\",\"SACE_YDI\",\n",
    "             \"SACE_YDJ\", \"SACE_YDK\", \"SACE_YDL\"]\n",
    "print(len(to_delete))\n",
    "index = 0\n",
    "for index in range(len(to_delete)):\n",
    "    curr = to_delete[index]\n",
    "    curr = curr+\"_\"+curr\n",
    "    to_delete[index] = curr\n",
    "    index+=1\n",
    "to_delete_index = []\n",
    "for item in to_delete:\n",
    "    curr_index = column_names.index(item)\n",
    "    to_delete_index.append(curr_index)\n",
    "snp_yeast_matrix = snp_yeast_matrix.drop(to_delete)\n",
    "snp_yeast_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_url = \"./phenoMatrix_35ConditionsNormalizedByYPD.csv\"\n",
    "pheno_yeast_matrix = pd.read_csv(pheno_url, delimiter='\\t', error_bad_lines=False)\n",
    "YPD6AU = pheno_yeast_matrix[\"YPD6AU\"]\n",
    "YPDBENOMYL200 = pheno_yeast_matrix[\"YPDBENOMYL200\"]\n",
    "YPDNACL1M = pheno_yeast_matrix[\"YPDNACL1M\"]\n",
    "YPDSDS = pheno_yeast_matrix[\"YPDSDS\"]\n",
    "YPGALACTOSE = pheno_yeast_matrix[\"YPGALACTOSE\"]\n",
    "YPGLYCEROL = pheno_yeast_matrix[\"YPGLYCEROL\"]\n",
    "YPRIBOSE = pheno_yeast_matrix[\"YPRIBOSE\"]\n",
    "YPSORBITOL = pheno_yeast_matrix[\"YPSORBITOL\"]\n",
    "total_run = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017625507644391997\n",
      "[-2.40193329e+08 -9.51329668e+07 -6.91509560e+07 ... -2.07819641e+05\n",
      "  1.72123812e+06 -3.65489209e+06]\n",
      "[   22    40   177 ... 12580 13439  1064]\n",
      "[35909712.86755651 30969496.06911329 30172606.27411069 ...\n",
      "  2057249.79204882  2056066.05203741  2055314.69394521]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_1 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPD6AU, test_size=0.2)\n",
    "    lr1 = linear_model.LinearRegression()\n",
    "    lr1.fit(x_train, y_train)\n",
    "    temp = np.array(lr1.coef_)\n",
    "    weight_vector_1 = [x + y for x, y in zip(weight_vector_1, temp)]\n",
    "    predictions_1 = lr1.predict(x_test)\n",
    "    pcc_1 = stat.pearsonr(predictions_1, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_1[0]\n",
    "\n",
    "avg_assoc_1 = sum/total_run\n",
    "avg_weight_vector_1 = np.divide(weight_vector_1, total_run)\n",
    "print(avg_assoc_1)\n",
    "print(avg_weight_vector_1)\n",
    "top2000_index = (-avg_weight_vector_1).argsort()[:2000]\n",
    "print (top2000_index)\n",
    "print (avg_weight_vector_1[top2000_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.037436911180733945\n",
      "[-3.37849673e+05  6.46576234e+07 -1.76284387e+07 ...  5.74838207e+04\n",
      "  6.64331609e+05 -1.09117706e+06]\n",
      "[    1     4     7 ... 17459  8807   579]\n",
      "[64657623.42498443 43433330.5155755  14566282.72546469 ...\n",
      "   724000.37730427   723879.2780653    723841.16101986]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_3 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDNACL1M, test_size=0.2)\n",
    "    lr3 = linear_model.LinearRegression()\n",
    "    lr3.fit(x_train, y_train)\n",
    "    temp = np.array(lr3.coef_)\n",
    "    weight_vector_3 = [x + y for x, y in zip(weight_vector_3, temp)]\n",
    "    predictions_3 = lr3.predict(x_test)\n",
    "    pcc_3 = stat.pearsonr(predictions_3, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_3[0]\n",
    "\n",
    "avg_assoc_3 = sum/total_run\n",
    "avg_weight_vector_3 = np.divide(weight_vector_3, total_run)\n",
    "print(avg_assoc_3)\n",
    "print(avg_weight_vector_3)\n",
    "top2000_index_3 = (-avg_weight_vector_3).argsort()[:2000]\n",
    "print (top2000_index_3)\n",
    "print (avg_weight_vector_3[top2000_index_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.050776698519380235\n",
      "[ 1.80293179e+08  3.00320564e+07  1.00394208e+07 ... -3.23214271e+04\n",
      " -1.37441458e+06 -3.48155840e+05]\n",
      "[   0    9    1 ... 8013 2623 8329]\n",
      "[1.80293179e+08 3.77394678e+07 3.00320564e+07 ... 7.83034738e+05\n",
      " 7.82003570e+05 7.81750319e+05]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_4 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDSDS, test_size=0.2)\n",
    "    lr4 = linear_model.LinearRegression()\n",
    "    lr4.fit(x_train, y_train)\n",
    "    temp = np.array(lr4.coef_)\n",
    "    weight_vector_4 = [x + y for x, y in zip(weight_vector_4, temp)]\n",
    "    predictions_4 = lr4.predict(x_test)\n",
    "    pcc_4 = stat.pearsonr(predictions_4, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_4[0]\n",
    "\n",
    "avg_assoc_4 = sum/total_run\n",
    "avg_weight_vector_4 = np.divide(weight_vector_4, total_run)\n",
    "print(avg_assoc_4)\n",
    "print(avg_weight_vector_4)\n",
    "top2000_index_4 = (-avg_weight_vector_4).argsort()[:2000]\n",
    "print (top2000_index_4)\n",
    "print (avg_weight_vector_4[top2000_index_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01487416324101254\n",
      "[ -7086718.755928    19818053.69945217 -10674090.39858287 ...\n",
      "    368086.6244495    -112899.05171859   -325806.63563745]\n",
      "[    4     6     1 ... 15876 13231  7014]\n",
      "[28971723.24723437 20022340.91732469 19818053.69945217 ...\n",
      "   698925.65741911   698919.24679987   698725.96386096]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_2 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDBENOMYL200, test_size=0.2)\n",
    "    lr2 = linear_model.LinearRegression()\n",
    "    lr2.fit(x_train, y_train)\n",
    "    temp = np.array(lr2.coef_)\n",
    "    weight_vector_2 = [x + y for x, y in zip(weight_vector_2, temp)]\n",
    "    predictions_2 = lr2.predict(x_test)\n",
    "    pcc_2 = stat.pearsonr(predictions_2, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_2[0]\n",
    "\n",
    "avg_assoc_2 = sum/total_run\n",
    "avg_weight_vector_2 = np.divide(weight_vector_2, total_run)\n",
    "print(avg_assoc_2)\n",
    "print(avg_weight_vector_2)\n",
    "top2000_index_2 = (-avg_weight_vector_2).argsort()[:2000]\n",
    "print (top2000_index_2)\n",
    "print (avg_weight_vector_2[top2000_index_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027988724790353886\n",
      "[-3.55220592e+08  1.09278129e+08 -2.61123680e+08 ... -4.55756319e+05\n",
      "  2.54604957e+06 -5.72904128e+04]\n",
      "[    1    11    10 ...  3537 12134 11862]\n",
      "[1.09278129e+08 7.41107208e+07 6.94411198e+07 ... 1.69495390e+06\n",
      " 1.69492387e+06 1.69119076e+06]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_5 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPGALACTOSE, test_size=0.2)\n",
    "    lr5 = linear_model.LinearRegression()\n",
    "    lr5.fit(x_train, y_train)\n",
    "    temp = np.array(lr5.coef_)\n",
    "    weight_vector_5 = [x + y for x, y in zip(weight_vector_5, temp)]\n",
    "    predictions_5 = lr5.predict(x_test)\n",
    "    pcc_5 = stat.pearsonr(predictions_5, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_5[0]\n",
    "\n",
    "avg_assoc_5 = sum/total_run\n",
    "avg_weight_vector_5 = np.divide(weight_vector_5, total_run)\n",
    "print(avg_assoc_5)\n",
    "print(avg_weight_vector_5)\n",
    "top2000_index_5 = (-avg_weight_vector_5).argsort()[:2000]\n",
    "print (top2000_index_5)\n",
    "print (avg_weight_vector_5[top2000_index_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01669042277109073\n",
      "[ 9.57655442e+07  8.30312598e+07 -5.45238087e+07 ... -6.39761225e+03\n",
      " -4.70708416e+06  2.03677672e+06]\n",
      "[    0     1     4 ... 13165 16181  7643]\n",
      "[95765544.23048124 83031259.82149577 63159836.09703083 ...\n",
      "  4184377.91238532  4183117.47498691  4180022.78792203]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_6 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPGLYCEROL, test_size=0.2)\n",
    "    lr6 = linear_model.LinearRegression()\n",
    "    lr6.fit(x_train, y_train)\n",
    "    temp = np.array(lr6.coef_)\n",
    "    weight_vector_6 = [x + y for x, y in zip(weight_vector_6, temp)]\n",
    "    predictions_6 = lr6.predict(x_test)\n",
    "    pcc_6 = stat.pearsonr(predictions_6, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_6[0]\n",
    "\n",
    "avg_assoc_6 = sum/total_run\n",
    "avg_weight_vector_6 = np.divide(weight_vector_6, total_run)\n",
    "print(avg_assoc_6)\n",
    "print(avg_weight_vector_6)\n",
    "top2000_index_6 = (-avg_weight_vector_6).argsort()[:2000]\n",
    "print (top2000_index_6)\n",
    "print (avg_weight_vector_6[top2000_index_6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012708775898420057\n",
      "[ 1.37363610e+08  2.55347170e+07 -2.27010896e+07 ... -5.95999093e+05\n",
      " -1.03600312e+04  1.72148497e+06]\n",
      "[    0     3    22 ...  9194  6502 15553]\n",
      "[1.37363610e+08 3.90485854e+07 3.55737953e+07 ... 1.04004830e+06\n",
      " 1.03954627e+06 1.03873070e+06]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_7 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPRIBOSE, test_size=0.2)\n",
    "    lr7 = linear_model.LinearRegression()\n",
    "    lr7.fit(x_train, y_train)\n",
    "    temp = np.array(lr7.coef_)\n",
    "    weight_vector_7 = [x + y for x, y in zip(weight_vector_7, temp)]\n",
    "    predictions_7 = lr7.predict(x_test)\n",
    "    pcc_7 = stat.pearsonr(predictions_7, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_7[0]\n",
    "\n",
    "avg_assoc_7 = sum/total_run\n",
    "avg_weight_vector_7 = np.divide(weight_vector_7, total_run)\n",
    "print(avg_assoc_7)\n",
    "print(avg_weight_vector_7)\n",
    "top2000_index_7 = (-avg_weight_vector_7).argsort()[:2000]\n",
    "print (top2000_index_7)\n",
    "print (avg_weight_vector_7[top2000_index_7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00658958047780311\n",
      "[-1.98248928e+08  2.49733290e+07 -2.84093445e+07 ...  6.25713982e+05\n",
      "  3.72645493e+05 -1.22592640e+06]\n",
      "[  14   18   19 ... 3716 2050 3460]\n",
      "[56802836.23059125 42934110.51236925 37816279.76193699 ...\n",
      "  2528406.61915241  2527849.51669522  2527272.76368826]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_8 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPSORBITOL, test_size=0.2)\n",
    "    lr8 = linear_model.LinearRegression()\n",
    "    lr8.fit(x_train, y_train)\n",
    "    temp = np.array(lr8.coef_)\n",
    "    weight_vector_8 = [x + y for x, y in zip(weight_vector_8, temp)]\n",
    "    predictions_8 = lr8.predict(x_test)\n",
    "    pcc_8 = stat.pearsonr(predictions_8, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_8[0]\n",
    "\n",
    "avg_assoc_8 = sum/total_run\n",
    "avg_weight_vector_8 = np.divide(weight_vector_8, total_run)\n",
    "print(avg_assoc_8)\n",
    "print(avg_weight_vector_8)\n",
    "top2000_index_8 = (-avg_weight_vector_8).argsort()[:2000]\n",
    "print (top2000_index_8)\n",
    "print (avg_weight_vector_8[top2000_index_8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = [\"YPD6AU\", \"YPDBENOMYL200\", \"YPDNACL1M\", \"YPDSDS\", \"YPGALACTOSE\", \"YPGLYCEROL\", \"YPRIBOSE\", \"YPSORBITOL\"]\n",
    "index_df = pd.DataFrame(list(zip(top2000_index, top2000_index_2, top2000_index_3, top2000_index_4, top2000_index_5, top2000_index_6, top2000_index_7, top2000_index_8)), columns = df_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一层hiddeen nueron 数量为一\n",
    "\n",
    "2000 1000 500 100 1 \n",
    "\n",
    "adam optimize 要\n",
    "\n",
    "batch normalization \n",
    "\n",
    ".cuda 挪到gpu\n",
    "\n",
    "batch training \n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as dt\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, h1, h2, h3, output_size, lr):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, h1)\n",
    "        self.l2 = nn.Linear(h1, h2)\n",
    "        self.l3 = nn.Linear(h2, h3)\n",
    "        self.l4 = nn.Linear(h3, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l4(out)\n",
    "        return out\n",
    "    \n",
    "    #Not used. \n",
    "    def train_net(self, iterations, train_data, target):\n",
    "        for epoch in range (iterations):\n",
    "            net.optimizer.zero_grad()\n",
    "            output = net.forward(train_data)\n",
    "            loss = net.loss_fn(output, target)\n",
    "            #print(list(self.parameters()))\n",
    "            print(\"current loss is: %f\"%loss)\n",
    "            loss.backward()\n",
    "            net.optimizer.step()\n",
    "            epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "input_size = 2000\n",
    "h1 = 1000\n",
    "h2 = 500\n",
    "h3 = 100\n",
    "output_size = 1\n",
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "traning_data = torch.tensor(snp_yeast_matrix[top2000_index].values, dtype = torch.float)\n",
    "answer = torch.tensor(YPD6AU, dtype = torch.float)\n",
    "train_set = dt.TensorDataset(traning_data, answer)\n",
    "train_loader = dt.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "net = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr)\n",
    "\n",
    "total_step = len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Charles\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:443: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.0732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Charles\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:443: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 0.0227\n",
      "Epoch [3/100], Loss: 0.0103\n",
      "Epoch [4/100], Loss: 0.0163\n",
      "Epoch [5/100], Loss: 0.0195\n",
      "Epoch [6/100], Loss: 0.0141\n",
      "Epoch [7/100], Loss: 0.0148\n",
      "Epoch [8/100], Loss: 0.0220\n",
      "Epoch [9/100], Loss: 0.0137\n",
      "Epoch [10/100], Loss: 0.0261\n",
      "Epoch [11/100], Loss: 0.0060\n",
      "Epoch [12/100], Loss: 0.0138\n",
      "Epoch [13/100], Loss: 0.0108\n",
      "Epoch [14/100], Loss: 0.0248\n",
      "Epoch [15/100], Loss: 0.0124\n",
      "Epoch [16/100], Loss: 0.0120\n",
      "Epoch [17/100], Loss: 0.0145\n",
      "Epoch [18/100], Loss: 0.0152\n",
      "Epoch [19/100], Loss: 0.0348\n",
      "Epoch [20/100], Loss: 0.0227\n",
      "Epoch [21/100], Loss: 0.0141\n",
      "Epoch [22/100], Loss: 0.0138\n",
      "Epoch [23/100], Loss: 0.0283\n",
      "Epoch [24/100], Loss: 0.0127\n",
      "Epoch [25/100], Loss: 0.0147\n",
      "Epoch [26/100], Loss: 0.0174\n",
      "Epoch [27/100], Loss: 0.0074\n",
      "Epoch [28/100], Loss: 0.0194\n",
      "Epoch [29/100], Loss: 0.0149\n",
      "Epoch [30/100], Loss: 0.0195\n",
      "Epoch [31/100], Loss: 0.0184\n",
      "Epoch [32/100], Loss: 0.0089\n",
      "Epoch [33/100], Loss: 0.0185\n",
      "Epoch [34/100], Loss: 0.0262\n",
      "Epoch [35/100], Loss: 0.0174\n",
      "Epoch [36/100], Loss: 0.0282\n",
      "Epoch [37/100], Loss: 0.0192\n",
      "Epoch [38/100], Loss: 0.0185\n",
      "Epoch [39/100], Loss: 0.0069\n",
      "Epoch [40/100], Loss: 0.0127\n",
      "Epoch [41/100], Loss: 0.0197\n",
      "Epoch [42/100], Loss: 0.0124\n",
      "Epoch [43/100], Loss: 0.0182\n",
      "Epoch [44/100], Loss: 0.0399\n",
      "Epoch [45/100], Loss: 0.0200\n",
      "Epoch [46/100], Loss: 0.0126\n",
      "Epoch [47/100], Loss: 0.0142\n",
      "Epoch [48/100], Loss: 0.0260\n",
      "Epoch [49/100], Loss: 0.0222\n",
      "Epoch [50/100], Loss: 0.0273\n",
      "Epoch [51/100], Loss: 0.0176\n",
      "Epoch [52/100], Loss: 0.0184\n",
      "Epoch [53/100], Loss: 0.0021\n",
      "Epoch [54/100], Loss: 0.0059\n",
      "Epoch [55/100], Loss: 0.0239\n",
      "Epoch [56/100], Loss: 0.0968\n",
      "Epoch [57/100], Loss: 0.1096\n",
      "Epoch [58/100], Loss: 0.0141\n",
      "Epoch [59/100], Loss: 0.0138\n",
      "Epoch [60/100], Loss: 0.0091\n",
      "Epoch [61/100], Loss: 0.0120\n",
      "Epoch [62/100], Loss: 0.0321\n",
      "Epoch [63/100], Loss: 0.0106\n",
      "Epoch [64/100], Loss: 0.0220\n",
      "Epoch [65/100], Loss: 0.0208\n",
      "Epoch [66/100], Loss: 0.0157\n",
      "Epoch [67/100], Loss: 0.0153\n",
      "Epoch [68/100], Loss: 0.0143\n",
      "Epoch [69/100], Loss: 0.0077\n",
      "Epoch [70/100], Loss: 0.0105\n",
      "Epoch [71/100], Loss: 0.0221\n",
      "Epoch [72/100], Loss: 0.0149\n",
      "Epoch [73/100], Loss: 0.0185\n",
      "Epoch [74/100], Loss: 0.0134\n",
      "Epoch [75/100], Loss: 0.0217\n",
      "Epoch [76/100], Loss: 0.0149\n",
      "Epoch [77/100], Loss: 0.0246\n",
      "Epoch [78/100], Loss: 0.0181\n",
      "Epoch [79/100], Loss: 0.0066\n",
      "Epoch [80/100], Loss: 0.0151\n",
      "Epoch [81/100], Loss: 0.0254\n",
      "Epoch [82/100], Loss: 0.0165\n",
      "Epoch [83/100], Loss: 0.0145\n",
      "Epoch [84/100], Loss: 0.0177\n",
      "Epoch [85/100], Loss: 0.0176\n",
      "Epoch [86/100], Loss: 0.0299\n",
      "Epoch [87/100], Loss: 0.0121\n",
      "Epoch [88/100], Loss: 0.0079\n",
      "Epoch [89/100], Loss: 0.0256\n",
      "Epoch [90/100], Loss: 0.0366\n",
      "Epoch [91/100], Loss: 0.0070\n",
      "Epoch [92/100], Loss: 0.0160\n",
      "Epoch [93/100], Loss: 0.0086\n",
      "Epoch [94/100], Loss: 0.0155\n",
      "Epoch [95/100], Loss: 0.0148\n",
      "Epoch [96/100], Loss: 0.0152\n",
      "Epoch [97/100], Loss: 0.0224\n",
      "Epoch [98/100], Loss: 0.0086\n",
      "Epoch [99/100], Loss: 0.0203\n",
      "Epoch [100/100], Loss: 0.0123\n"
     ]
    }
   ],
   "source": [
    "for epoch in range (num_epochs):\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        net.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = net.forward(data).to(device)\n",
    "        loss = net.loss_fn(output, target)\n",
    "\n",
    "        #print(list(self.parameters()))\n",
    "        #print(\"current loss is: %f\"%loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch += 1\n",
    "        \n",
    "        if (i) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "split = 0.8\n",
    "train_size = int(split * len(train_set))\n",
    "print(train_size)\n",
    "test_size = len(train_set) - train_size \n",
    "print(test_size)\n",
    "\n",
    "train_dataset, test_dataset = dt.random_split(train_set, [train_size, test_size])\n",
    "\n",
    "device = torch.device('cuda')\n",
    "input_size = 2000\n",
    "h1 = 1000\n",
    "h2 = 500\n",
    "h3 = 100\n",
    "output_size = 1\n",
    "lr = 0.0001\n",
    "num_epochs = 80\n",
    "batch_size = 10\n",
    "train_loader = dt.DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
    "\n",
    "net = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Loss: 0.1647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Charles\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:443: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/80], Loss: 0.0166\n",
      "Epoch [3/80], Loss: 0.0144\n",
      "Epoch [4/80], Loss: 0.0145\n",
      "Epoch [5/80], Loss: 0.0146\n",
      "Epoch [6/80], Loss: 0.0145\n",
      "Epoch [7/80], Loss: 0.0144\n",
      "Epoch [8/80], Loss: 0.0144\n",
      "Epoch [9/80], Loss: 0.0142\n",
      "Epoch [10/80], Loss: 0.0141\n",
      "Epoch [11/80], Loss: 0.0141\n",
      "Epoch [12/80], Loss: 0.0141\n",
      "Epoch [13/80], Loss: 0.0142\n",
      "Epoch [14/80], Loss: 0.0142\n",
      "Epoch [15/80], Loss: 0.0142\n",
      "Epoch [16/80], Loss: 0.0142\n",
      "Epoch [17/80], Loss: 0.0143\n",
      "Epoch [18/80], Loss: 0.0141\n",
      "Epoch [19/80], Loss: 0.0141\n",
      "Epoch [20/80], Loss: 0.0141\n",
      "Epoch [21/80], Loss: 0.0142\n",
      "Epoch [22/80], Loss: 0.0142\n",
      "Epoch [23/80], Loss: 0.0142\n",
      "Epoch [24/80], Loss: 0.0142\n",
      "Epoch [25/80], Loss: 0.0143\n",
      "Epoch [26/80], Loss: 0.0147\n",
      "Epoch [27/80], Loss: 0.0148\n",
      "Epoch [28/80], Loss: 0.0143\n",
      "Epoch [29/80], Loss: 0.0142\n",
      "Epoch [30/80], Loss: 0.0142\n",
      "Epoch [31/80], Loss: 0.0147\n",
      "Epoch [32/80], Loss: 0.0144\n",
      "Epoch [33/80], Loss: 0.0146\n",
      "Epoch [34/80], Loss: 0.0142\n",
      "Epoch [35/80], Loss: 0.0141\n",
      "Epoch [36/80], Loss: 0.0141\n",
      "Epoch [37/80], Loss: 0.0150\n",
      "Epoch [38/80], Loss: 0.0142\n",
      "Epoch [39/80], Loss: 0.0145\n",
      "Epoch [40/80], Loss: 0.0143\n",
      "Epoch [41/80], Loss: 0.0142\n",
      "Epoch [42/80], Loss: 0.0143\n",
      "Epoch [43/80], Loss: 0.0143\n",
      "Epoch [44/80], Loss: 0.0143\n",
      "Epoch [45/80], Loss: 0.0144\n",
      "Epoch [46/80], Loss: 0.0143\n",
      "Epoch [47/80], Loss: 0.0141\n",
      "Epoch [48/80], Loss: 0.0141\n",
      "Epoch [49/80], Loss: 0.0141\n",
      "Epoch [50/80], Loss: 0.0143\n",
      "Epoch [51/80], Loss: 0.0142\n",
      "Epoch [52/80], Loss: 0.0141\n",
      "Epoch [53/80], Loss: 0.0142\n",
      "Epoch [54/80], Loss: 0.0144\n",
      "Epoch [55/80], Loss: 0.0143\n",
      "Epoch [56/80], Loss: 0.0141\n",
      "Epoch [57/80], Loss: 0.0141\n",
      "Epoch [58/80], Loss: 0.0142\n",
      "Epoch [59/80], Loss: 0.0141\n",
      "Epoch [60/80], Loss: 0.0142\n",
      "Epoch [61/80], Loss: 0.0142\n",
      "Epoch [62/80], Loss: 0.0141\n",
      "Epoch [63/80], Loss: 0.0140\n",
      "Epoch [64/80], Loss: 0.0141\n",
      "Epoch [65/80], Loss: 0.0141\n",
      "Epoch [66/80], Loss: 0.0141\n",
      "Epoch [67/80], Loss: 0.0142\n",
      "Epoch [68/80], Loss: 0.0142\n",
      "Epoch [69/80], Loss: 0.0140\n",
      "Epoch [70/80], Loss: 0.0142\n",
      "Epoch [71/80], Loss: 0.0142\n",
      "Epoch [72/80], Loss: 0.0141\n",
      "Epoch [73/80], Loss: 0.0141\n",
      "Epoch [74/80], Loss: 0.0142\n",
      "Epoch [75/80], Loss: 0.0141\n",
      "Epoch [76/80], Loss: 0.0142\n",
      "Epoch [77/80], Loss: 0.0143\n",
      "Epoch [78/80], Loss: 0.0141\n",
      "Epoch [79/80], Loss: 0.0142\n",
      "Epoch [80/80], Loss: 0.0141\n"
     ]
    }
   ],
   "source": [
    "for epoch in range (num_epochs):\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        net.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = net.forward(data).to(device)\n",
    "        loss = net.loss_fn(output, target)\n",
    "\n",
    "        #print(list(self.parameters()))\n",
    "        #print(\"current loss is: %f\"%loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch += 1\n",
    "        \n",
    "        if (i) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error is: 21.971487 \n",
      " Avg error is: 0.112674\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "counter = 0\n",
    "test_loader = dt.DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
    "with torch.no_grad():\n",
    "    total_err = 0.0\n",
    "    for (data, target) in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = net(data)\n",
    "        for out, tar in zip(target, output):\n",
    "            curr_err = abs(out - tar)\n",
    "            total_err += curr_err\n",
    "            counter += 1\n",
    "    print(\"Total error is: %f \\n Avg error is: %f\" %(total_err, total_err/counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
