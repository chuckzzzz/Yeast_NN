{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions ##\n",
    "1. Map top 10% snps to genes\n",
    "2. Use additive model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import bisect as bs\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import scipy.stats as stat\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18921</th>\n",
       "      <th>18922</th>\n",
       "      <th>18923</th>\n",
       "      <th>18924</th>\n",
       "      <th>18925</th>\n",
       "      <th>18926</th>\n",
       "      <th>18927</th>\n",
       "      <th>18928</th>\n",
       "      <th>18929</th>\n",
       "      <th>18930</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAA_AAA</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAB_AAB</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAC_AAC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAD_AAD</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAE_AAE</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 18931 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4      5      6      7      8      9      \\\n",
       "AAA_AAA      0      0      0      1      0      0      0      0      0      0   \n",
       "AAB_AAB      0      0      0      0      1      0      0      0      0      0   \n",
       "AAC_AAC      0      0      0      1      0      0      0      0      0      0   \n",
       "AAD_AAD      0      0      0      0      1      1      1      0      0      0   \n",
       "AAE_AAE      0      0      0      1      0      0      0      0      0      0   \n",
       "\n",
       "         ...  18921  18922  18923  18924  18925  18926  18927  18928  18929  \\\n",
       "AAA_AAA  ...      0      0      0      0      0      0      1      0      0   \n",
       "AAB_AAB  ...      0      0      0      0      0      0      1      0      0   \n",
       "AAC_AAC  ...      0      0      0      0      0      0      0      0      0   \n",
       "AAD_AAD  ...      0      0      1      1      1      1      0      0      1   \n",
       "AAE_AAE  ...      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "         18930  \n",
       "AAA_AAA      0  \n",
       "AAB_AAB      0  \n",
       "AAC_AAC      0  \n",
       "AAD_AAD      1  \n",
       "AAE_AAE      0  \n",
       "\n",
       "[5 rows x 18931 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNP_url = \"./1011geneSNP.csv\"\n",
    "Gene_url = \"./GCF_000146045.2_R64_genomic.gff\"\n",
    "\n",
    "snp_yeast_matrix = pd.read_csv(SNP_url, sep = ',' , error_bad_lines=False)\n",
    "del snp_yeast_matrix['#CHROM']\n",
    "pos_list = list(snp_yeast_matrix['POS'])\n",
    "del snp_yeast_matrix['POS']\n",
    "del snp_yeast_matrix['REF']\n",
    "del snp_yeast_matrix['ALT']\n",
    "del snp_yeast_matrix['ANN[*].GENE']\n",
    "del snp_yeast_matrix['ANN[*].GENEID']\n",
    "index_list = snp_yeast_matrix[\"ID\"].tolist()\n",
    "del snp_yeast_matrix['ID']\n",
    "snp_yeast_matrix = snp_yeast_matrix.transpose()\n",
    "column_names = list(snp_yeast_matrix.index)\n",
    "to_delete = [\"ABC\", \"ABF\", \"ASP\", \"BGS\",\"BGF\", \"BHL\",\"BHQ\",\"BID\",\"BIR\",\"BKG\",\"BKN\",\"CFH\",\"SACE_YAL\",\"SACE_YBA\",\n",
    "             \"SACE_YBB\",\"SACE_YBM\",\"SACE_YBN\",\"SACE_YBO\",\"SACE_YBP\",\"SACE_YCJ\",\"SACE_YCS\",\"SACE_YCT\",\"SACE_YCU\",\"SACE_YCV\",\"SACE_YCW\",\n",
    "             \"SACE_YCX\",\"SACE_YCY\",\"SACE_YCZ\",\"SACE_YDA\",\"SACE_YDB\",\"SACE_YDC\",\"SACE_YDD\", \"SACE_YDE\",\"SACE_YDF\",\"SACE_YDG\",\"SACE_YDH\",\"SACE_YDI\",\n",
    "             \"SACE_YDJ\", \"SACE_YDK\", \"SACE_YDL\"]\n",
    "print(len(to_delete))\n",
    "index = 0\n",
    "for index in range(len(to_delete)):\n",
    "    curr = to_delete[index]\n",
    "    curr = curr+\"_\"+curr\n",
    "    to_delete[index] = curr\n",
    "    index+=1\n",
    "to_delete_index = []\n",
    "for item in to_delete:\n",
    "    curr_index = column_names.index(item)\n",
    "    to_delete_index.append(curr_index)\n",
    "snp_yeast_matrix = snp_yeast_matrix.drop(to_delete)\n",
    "snp_yeast_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_url = \"./phenoMatrix_35ConditionsNormalizedByYPD.csv\"\n",
    "pheno_yeast_matrix = pd.read_csv(pheno_url, delimiter='\\t', error_bad_lines=False)\n",
    "YPD6AU = pheno_yeast_matrix[\"YPD6AU\"]\n",
    "YPDBENOMYL200 = pheno_yeast_matrix[\"YPDBENOMYL200\"]\n",
    "YPDNACL1M = pheno_yeast_matrix[\"YPDNACL1M\"]\n",
    "YPDSDS = pheno_yeast_matrix[\"YPDSDS\"]\n",
    "YPGALACTOSE = pheno_yeast_matrix[\"YPGALACTOSE\"]\n",
    "YPGLYCEROL = pheno_yeast_matrix[\"YPGLYCEROL\"]\n",
    "YPRIBOSE = pheno_yeast_matrix[\"YPRIBOSE\"]\n",
    "YPSORBITOL = pheno_yeast_matrix[\"YPSORBITOL\"]\n",
    "total_run = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate index of top associated snps in 100 runs ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012257347815486095\n",
      "[-4.70382797e+07 -6.89749386e+07 -1.83195975e+07 ...  2.27410609e+05\n",
      "  1.05730682e+04 -2.23611876e+06]\n",
      "[    3    21    10 ... 14519 14741  1267]\n",
      "[63489555.34764801 37168502.67938235 25187533.21800183 ...\n",
      "   966291.92206568   966192.06802127   966080.69411545]\n",
      "0.02928149642082002\n",
      "[11362998.05945773 28713208.33432954 13627430.90108617 ...\n",
      "   -40442.2075441    165859.89389064   -39600.91799881]\n",
      "[   1    3    7 ... 5896 1129 4482]\n",
      "[28713208.33432954 23918107.46240122 22478385.36228473 ...\n",
      "   322581.45664118   322579.76917957   322544.18034099]\n",
      "0.03055584375431484\n",
      "[-1.30255456e+07  1.16265475e+07  7.70332847e+07 ...  6.63462302e+04\n",
      "  2.67797084e+05  1.03873146e+06]\n",
      "[    2     5    11 ... 13456  9317 17605]\n",
      "[77033284.7110372  34113975.27897751 28712000.58812311 ...\n",
      "   985321.52233595   985242.63674778   984991.83188687]\n",
      "0.02986436825179075\n",
      "[-31589016.07814947 -32537335.13906072 -32012055.36644748 ...\n",
      "    -50114.80156448   -819854.95537944   -350007.71854033]\n",
      "[    4    37    12 ... 16097  6437  2089]\n",
      "[21165602.61480784 20843123.0516498  20276192.70727721 ...\n",
      "   637939.48879862   637814.05869804   637282.26414056]\n",
      "0.026770273423110634\n",
      "[-71481987.88642606  17174529.77613132 -59147369.21048155 ...\n",
      "    371590.27775718  -1964216.62093946  -4752374.63636292]\n",
      "[    4    19    13 ... 17851  6138 16731]\n",
      "[90739831.80296534 70743129.31193127 51387931.88249398 ...\n",
      "  1741249.1324289   1740814.05549015  1740175.89062396]\n",
      "0.011781339932532088\n",
      "[-4194653.11724212 48673137.07161298 16891541.77723872 ...\n",
      "  -277769.64717834    76209.49268603  2498787.84096578]\n",
      "[    1     3     9 ... 13950 12643  9140]\n",
      "[48673137.07161298 48659819.5903003  42163759.72037011 ...\n",
      "  1092693.61702056  1091647.38100573  1091398.26219651]\n",
      "0.020967674289920638\n",
      "[ 1.92152663e+08  2.02541566e+07 -5.64396107e+06 ... -2.67101077e+05\n",
      "  1.09653873e+06 -2.77311977e+06]\n",
      "[    0     7     6 ... 18857  4819 16847]\n",
      "[1.92152663e+08 6.31532385e+07 6.00849243e+07 ... 3.04051239e+06\n",
      " 3.04037542e+06 3.04021103e+06]\n",
      "-0.0008324804522335593\n",
      "[ 1.28849337e+09  6.27748969e+07  2.77675781e+08 ...  3.05645157e+05\n",
      "  2.67787235e+07 -1.56707847e+07]\n",
      "[   0    2    6 ... 4134 6782 3716]\n",
      "[1.28849337e+09 2.77675781e+08 1.75685079e+08 ... 1.25043884e+07\n",
      " 1.25037916e+07 1.25017774e+07]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_1 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPD6AU, test_size=0.2)\n",
    "    lr1 = linear_model.LinearRegression()\n",
    "    lr1.fit(x_train, y_train)\n",
    "    temp = np.array(lr1.coef_)\n",
    "    weight_vector_1 = [x + y for x, y in zip(weight_vector_1, temp)]\n",
    "    predictions_1 = lr1.predict(x_test)\n",
    "    pcc_1 = stat.pearsonr(predictions_1, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_1[0]\n",
    "\n",
    "avg_assoc_1 = sum/total_run\n",
    "avg_weight_vector_1 = np.divide(weight_vector_1, total_run)\n",
    "print(avg_assoc_1)\n",
    "print(avg_weight_vector_1)\n",
    "top2000_index = (-avg_weight_vector_1).argsort()[:2000]\n",
    "print (top2000_index)\n",
    "print (avg_weight_vector_1[top2000_index])\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "sum = 0\n",
    "weight_vector_2 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDBENOMYL200, test_size=0.2)\n",
    "    lr2 = linear_model.LinearRegression()\n",
    "    lr2.fit(x_train, y_train)\n",
    "    temp = np.array(lr2.coef_)\n",
    "    weight_vector_2 = [x + y for x, y in zip(weight_vector_2, temp)]\n",
    "    predictions_2 = lr2.predict(x_test)\n",
    "    pcc_2 = stat.pearsonr(predictions_2, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_2[0]\n",
    "\n",
    "avg_assoc_2 = sum/total_run\n",
    "avg_weight_vector_2 = np.divide(weight_vector_2, total_run)\n",
    "print(avg_assoc_2)\n",
    "print(avg_weight_vector_2)\n",
    "top2000_index_2 = (-avg_weight_vector_2).argsort()[:2000]\n",
    "print (top2000_index_2)\n",
    "print (avg_weight_vector_2[top2000_index_2])\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "sum = 0\n",
    "weight_vector_3 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDNACL1M, test_size=0.2)\n",
    "    lr3 = linear_model.LinearRegression()\n",
    "    lr3.fit(x_train, y_train)\n",
    "    temp = np.array(lr3.coef_)\n",
    "    weight_vector_3 = [x + y for x, y in zip(weight_vector_3, temp)]\n",
    "    predictions_3 = lr3.predict(x_test)\n",
    "    pcc_3 = stat.pearsonr(predictions_3, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_3[0]\n",
    "\n",
    "avg_assoc_3 = sum/total_run\n",
    "avg_weight_vector_3 = np.divide(weight_vector_3, total_run)\n",
    "print(avg_assoc_3)\n",
    "print(avg_weight_vector_3)\n",
    "top2000_index_3 = (-avg_weight_vector_3).argsort()[:2000]\n",
    "print (top2000_index_3)\n",
    "print (avg_weight_vector_3[top2000_index_3])\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "sum = 0\n",
    "weight_vector_4 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDSDS, test_size=0.2)\n",
    "    lr4 = linear_model.LinearRegression()\n",
    "    lr4.fit(x_train, y_train)\n",
    "    temp = np.array(lr4.coef_)\n",
    "    weight_vector_4 = [x + y for x, y in zip(weight_vector_4, temp)]\n",
    "    predictions_4 = lr4.predict(x_test)\n",
    "    pcc_4 = stat.pearsonr(predictions_4, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_4[0]\n",
    "\n",
    "avg_assoc_4 = sum/total_run\n",
    "avg_weight_vector_4 = np.divide(weight_vector_4, total_run)\n",
    "print(avg_assoc_4)\n",
    "print(avg_weight_vector_4)\n",
    "top2000_index_4 = (-avg_weight_vector_4).argsort()[:2000]\n",
    "print (top2000_index_4)\n",
    "print (avg_weight_vector_4[top2000_index_4])\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "sum = 0\n",
    "weight_vector_5 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPGALACTOSE, test_size=0.2)\n",
    "    lr5 = linear_model.LinearRegression()\n",
    "    lr5.fit(x_train, y_train)\n",
    "    temp = np.array(lr5.coef_)\n",
    "    weight_vector_5 = [x + y for x, y in zip(weight_vector_5, temp)]\n",
    "    predictions_5 = lr5.predict(x_test)\n",
    "    pcc_5 = stat.pearsonr(predictions_5, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_5[0]\n",
    "\n",
    "avg_assoc_5 = sum/total_run\n",
    "avg_weight_vector_5 = np.divide(weight_vector_5, total_run)\n",
    "print(avg_assoc_5)\n",
    "print(avg_weight_vector_5)\n",
    "top2000_index_5 = (-avg_weight_vector_5).argsort()[:2000]\n",
    "print (top2000_index_5)\n",
    "print (avg_weight_vector_5[top2000_index_5])\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "sum = 0\n",
    "weight_vector_6 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPGLYCEROL, test_size=0.2)\n",
    "    lr6 = linear_model.LinearRegression()\n",
    "    lr6.fit(x_train, y_train)\n",
    "    temp = np.array(lr6.coef_)\n",
    "    weight_vector_6 = [x + y for x, y in zip(weight_vector_6, temp)]\n",
    "    predictions_6 = lr6.predict(x_test)\n",
    "    pcc_6 = stat.pearsonr(predictions_6, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_6[0]\n",
    "\n",
    "avg_assoc_6 = sum/total_run\n",
    "avg_weight_vector_6 = np.divide(weight_vector_6, total_run)\n",
    "print(avg_assoc_6)\n",
    "print(avg_weight_vector_6)\n",
    "top2000_index_6 = (-avg_weight_vector_6).argsort()[:2000]\n",
    "print (top2000_index_6)\n",
    "print (avg_weight_vector_6[top2000_index_6])\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "sum = 0\n",
    "weight_vector_7 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPRIBOSE, test_size=0.2)\n",
    "    lr7 = linear_model.LinearRegression()\n",
    "    lr7.fit(x_train, y_train)\n",
    "    temp = np.array(lr7.coef_)\n",
    "    weight_vector_7 = [x + y for x, y in zip(weight_vector_7, temp)]\n",
    "    predictions_7 = lr7.predict(x_test)\n",
    "    pcc_7 = stat.pearsonr(predictions_7, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_7[0]\n",
    "\n",
    "avg_assoc_7 = sum/total_run\n",
    "avg_weight_vector_7 = np.divide(weight_vector_7, total_run)\n",
    "print(avg_assoc_7)\n",
    "print(avg_weight_vector_7)\n",
    "top2000_index_7 = (-avg_weight_vector_7).argsort()[:2000]\n",
    "print (top2000_index_7)\n",
    "print (avg_weight_vector_7[top2000_index_7])\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "sum = 0\n",
    "weight_vector_8 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPSORBITOL, test_size=0.2)\n",
    "    lr8 = linear_model.LinearRegression()\n",
    "    lr8.fit(x_train, y_train)\n",
    "    temp = np.array(lr8.coef_)\n",
    "    weight_vector_8 = [x + y for x, y in zip(weight_vector_8, temp)]\n",
    "    predictions_8 = lr8.predict(x_test)\n",
    "    pcc_8 = stat.pearsonr(predictions_8, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_8[0]\n",
    "\n",
    "avg_assoc_8 = sum/total_run\n",
    "avg_weight_vector_8 = np.divide(weight_vector_8, total_run)\n",
    "print(avg_assoc_8)\n",
    "print(avg_weight_vector_8)\n",
    "top2000_index_8 = (-avg_weight_vector_8).argsort()[:2000]\n",
    "print (top2000_index_8)\n",
    "print (avg_weight_vector_8[top2000_index_8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_snp_list=[top2000_index,top2000_index_2,top2000_index_3,top2000_index_4,top2000_index_5,top2000_index_6,top2000_index_7,top2000_index_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict finished. There are 6376 genes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAU8</th>\n",
       "      <th>YAL067W-A</th>\n",
       "      <th>SEO1</th>\n",
       "      <th>YAL065C</th>\n",
       "      <th>YAL064W-B</th>\n",
       "      <th>TDA8</th>\n",
       "      <th>YAL064W</th>\n",
       "      <th>YAL063C-A</th>\n",
       "      <th>FLO9</th>\n",
       "      <th>GDH3</th>\n",
       "      <th>...</th>\n",
       "      <th>tN(GUU)Q</th>\n",
       "      <th>tM(CAU)Q1</th>\n",
       "      <th>COX2</th>\n",
       "      <th>Q0255</th>\n",
       "      <th>tF(GAA)Q</th>\n",
       "      <th>tT(UAG)Q2</th>\n",
       "      <th>tV(UAC)Q</th>\n",
       "      <th>COX3</th>\n",
       "      <th>tM(CAU)Q2</th>\n",
       "      <th>RPM1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAA_AAA</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAB_AAB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAC_AAC</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAD_AAD</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAE_AAE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6427 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PAU8  YAL067W-A  SEO1  YAL065C  YAL064W-B  TDA8  YAL064W  YAL063C-A  \\\n",
       "AAA_AAA   0.0        0.0   0.0      0.0        0.0   0.0      0.0        0.0   \n",
       "AAB_AAB   0.0        0.0   0.0      0.0        0.0   0.0      0.0        0.0   \n",
       "AAC_AAC   0.0        0.0   0.0      0.0        0.0   0.0      0.0        0.0   \n",
       "AAD_AAD   0.0        0.0   0.0      0.0        0.0   0.0      0.0        0.0   \n",
       "AAE_AAE   0.0        0.0   0.0      0.0        0.0   0.0      0.0        0.0   \n",
       "\n",
       "         FLO9  GDH3  ...  tN(GUU)Q  tM(CAU)Q1  COX2  Q0255  tF(GAA)Q  \\\n",
       "AAA_AAA   0.0   0.0  ...       0.0        0.0   0.0    0.0       0.0   \n",
       "AAB_AAB   0.0   0.0  ...       0.0        0.0   0.0    0.0       0.0   \n",
       "AAC_AAC   0.0   0.0  ...       0.0        0.0   0.0    0.0       0.0   \n",
       "AAD_AAD   0.0   0.0  ...       0.0        0.0   0.0    0.0       0.0   \n",
       "AAE_AAE   0.0   0.0  ...       0.0        0.0   0.0    0.0       0.0   \n",
       "\n",
       "         tT(UAG)Q2  tV(UAC)Q  COX3  tM(CAU)Q2  RPM1  \n",
       "AAA_AAA        0.0       0.0   0.0        0.0   0.0  \n",
       "AAB_AAB        0.0       0.0   0.0        0.0   0.0  \n",
       "AAC_AAC        0.0       0.0   0.0        0.0   0.0  \n",
       "AAD_AAD        0.0       0.0   0.0        0.0   0.0  \n",
       "AAE_AAE        0.0       0.0   0.0        0.0   0.0  \n",
       "\n",
       "[5 rows x 6427 columns]"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_gene(input):\n",
    "    words = input.split(';')\n",
    "    gene_tuple = words[2]\n",
    "    words = gene_tuple.split('=')\n",
    "    gene = words[1]\n",
    "    return gene\n",
    "\n",
    "def snp_pos_to_gene(pos):\n",
    "    index = bs.bisect(start_pos, pos)\n",
    "    curr_start = start_pos[index-1]\n",
    "    curr_tuple = dict[curr_start]\n",
    "    curr_end = curr_tuple[2]\n",
    "    if pos in range ((int)(curr_start), (int)(curr_end)):\n",
    "        return curr_tuple[0]\n",
    "\n",
    "names=[\"Contig\", \"Seq\",\"Type\", \"Start\", \"End\", \"N/A\", \"N/A1\", \"N/A2\", \"Info\"]\n",
    "gene_matrix = pd.read_csv(Gene_url, sep = '\\t', error_bad_lines=False, skiprows = 8, names = names)\n",
    "gene_matrix.head()\n",
    "\n",
    "dict = {}\n",
    "gene_list = []\n",
    "for row in gene_matrix.itertuples():\n",
    "    if(row.Type == \"gene\"):\n",
    "        curr_info = row.Info\n",
    "        curr_gene = get_gene(curr_info)\n",
    "        gene_list.append(curr_gene)\n",
    "        curr_start = row.Start\n",
    "        curr_end = row.End\n",
    "        tuple = (curr_gene, curr_start, curr_end)\n",
    "        dict[curr_start] = tuple\n",
    "print(\"Dict finished. There are %u genes\" %len(dict))\n",
    "start_pos = list(dict.keys())\n",
    "rows = snp_yeast_matrix.index.values\n",
    "rows = rows[0:]\n",
    "array = np.zeros([len(rows),len(gene_list)])\n",
    "\n",
    "yeast_gene_matrix_1 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "yeast_gene_matrix_2 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "yeast_gene_matrix_3 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "yeast_gene_matrix_4 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "yeast_gene_matrix_5 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "yeast_gene_matrix_6 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "yeast_gene_matrix_7 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "yeast_gene_matrix_8 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "\n",
    "yeast_gene_matrix_list = [yeast_gene_matrix_1,yeast_gene_matrix_2,yeast_gene_matrix_3,yeast_gene_matrix_4,yeast_gene_matrix_5,yeast_gene_matrix_6,yeast_gene_matrix_7,yeast_gene_matrix_8]\n",
    "yeast_gene_matrix_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        RefSeq\n",
      "1        RefSeq\n",
      "2        RefSeq\n",
      "3        RefSeq\n",
      "4        RefSeq\n",
      "5        RefSeq\n",
      "6        RefSeq\n",
      "7        RefSeq\n",
      "8        RefSeq\n",
      "9        RefSeq\n",
      "10       RefSeq\n",
      "11       RefSeq\n",
      "12       RefSeq\n",
      "13       RefSeq\n",
      "14       RefSeq\n",
      "15       RefSeq\n",
      "16       RefSeq\n",
      "17       RefSeq\n",
      "18       RefSeq\n",
      "19       RefSeq\n",
      "20       RefSeq\n",
      "21       RefSeq\n",
      "22       RefSeq\n",
      "23       RefSeq\n",
      "24       RefSeq\n",
      "25       RefSeq\n",
      "26       RefSeq\n",
      "27       RefSeq\n",
      "28       RefSeq\n",
      "29       RefSeq\n",
      "          ...  \n",
      "26915    RefSeq\n",
      "26916    RefSeq\n",
      "26917    RefSeq\n",
      "26918    RefSeq\n",
      "26919    RefSeq\n",
      "26920    RefSeq\n",
      "26921    RefSeq\n",
      "26922    RefSeq\n",
      "26923    RefSeq\n",
      "26924    RefSeq\n",
      "26925    RefSeq\n",
      "26926    RefSeq\n",
      "26927    RefSeq\n",
      "26928    RefSeq\n",
      "26929    RefSeq\n",
      "26930    RefSeq\n",
      "26931    RefSeq\n",
      "26932    RefSeq\n",
      "26933    RefSeq\n",
      "26934    RefSeq\n",
      "26935    RefSeq\n",
      "26936    RefSeq\n",
      "26937    RefSeq\n",
      "26938    RefSeq\n",
      "26939    RefSeq\n",
      "26940    RefSeq\n",
      "26941    RefSeq\n",
      "26942    RefSeq\n",
      "26943    RefSeq\n",
      "26944       NaN\n",
      "Name: Seq, Length: 26945, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(gene_matrix[\"Seq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snp_list_to_gene(valid_snp_list, snp_yeast_matrix, yeast_gene_matrix):\n",
    "    max_snp = 18931\n",
    "    max_yeast = 971\n",
    "    snp_counter = 0\n",
    "    for snp_counter in range(max_snp):\n",
    "        if(snp_counter %2000 == 0):\n",
    "            print(snp_counter)\n",
    "        curr_column = snp_yeast_matrix[snp_counter].tolist()\n",
    "        curr_pos = pos_list[snp_counter]\n",
    "        yeast_counter = 0\n",
    "        for yeast_counter in range (max_yeast):\n",
    "            curr_val = curr_column[yeast_counter]\n",
    "            if(curr_val)!=0:\n",
    "                if snp_counter in valid_snp_list:\n",
    "                    gene = snp_pos_to_gene(curr_pos)\n",
    "                    if(gene is None):\n",
    "                        continue\n",
    "                    yeast_name = rows[yeast_counter]\n",
    "                    yeast_gene_matrix[gene][yeast_name] += 1\n",
    "            yeast_counter += 1\n",
    "        snp_counter += 1\n",
    "    print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for valid_snp, yg_matrix in zip(valid_snp_list, yeast_gene_matrix_list):\n",
    "    snp_list_to_gene(valid_snp, snp_yeast_matrix, yg_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(971, 376)\n",
      "(971, 371)\n",
      "(971, 377)\n",
      "(971, 369)\n",
      "(971, 370)\n",
      "(971, 361)\n",
      "(971, 369)\n",
      "(971, 368)\n"
     ]
    }
   ],
   "source": [
    "min_yeast_with_gene = 5\n",
    "min_total_gene = 10\n",
    "for matrix in yeast_gene_matrix_list:\n",
    "    for column in matrix:\n",
    "        counter = 0\n",
    "        value = 0\n",
    "        temp = matrix[column]\n",
    "        for element in temp:\n",
    "            if element != 0:\n",
    "                counter += 1\n",
    "                value += 4\n",
    "        if(counter < min_yeast_with_gene and value <min_total_gene ):\n",
    "            del matrix[column]\n",
    "            \n",
    "for matrix in yeast_gene_matrix_list:\n",
    "    print(matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YAL064W</th>\n",
       "      <th>YAL063C-A</th>\n",
       "      <th>FLO9</th>\n",
       "      <th>GDH3</th>\n",
       "      <th>BDH2</th>\n",
       "      <th>BDH1</th>\n",
       "      <th>CNE1</th>\n",
       "      <th>GPB2</th>\n",
       "      <th>PEX22</th>\n",
       "      <th>ACS1</th>\n",
       "      <th>...</th>\n",
       "      <th>HDA3</th>\n",
       "      <th>AOS1</th>\n",
       "      <th>SEC23</th>\n",
       "      <th>DPM1</th>\n",
       "      <th>GDB1</th>\n",
       "      <th>ATG13</th>\n",
       "      <th>PZF1</th>\n",
       "      <th>SKI3</th>\n",
       "      <th>RPC82</th>\n",
       "      <th>QCR2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAA_AAA</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAB_AAB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAC_AAC</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAD_AAD</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAE_AAE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAG_AAG</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAH_AAH</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAI_AAI</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAK_AAK</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAL_AAL</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAM_AAM</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAN_AAN</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAP_AAP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAQ_AAQ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAR_AAR</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAS_AAS</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAT_AAT</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAV_AAV</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABA_ABA</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABB_ABB</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABD_ABD</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABE_ABE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABG_ABG</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABH_ABH</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABI_ABI</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABK_ABK</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABL_ABL</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABM_ABM</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABP_ABP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABQ_ABQ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBQ_SACE_YBQ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBR_SACE_YBR</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBS_SACE_YBS</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBT_SACE_YBT</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBU_SACE_YBU</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBV_SACE_YBV</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBW_SACE_YBW</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBX_SACE_YBX</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBY_SACE_YBY</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBZ_SACE_YBZ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCA_SACE_YCA</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCB_SACE_YCB</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCC_SACE_YCC</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCD_SACE_YCD</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCE_SACE_YCE</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCF_SACE_YCF</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCG_SACE_YCG</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCH_SACE_YCH</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCI_SACE_YCI</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCK_SACE_YCK</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCL_SACE_YCL</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCM_SACE_YCM</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCN_SACE_YCN</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCO_SACE_YCO</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCP_SACE_YCP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCQ_SACE_YCQ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCR_SACE_YCR</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YDM_SACE_YDM</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YDN_SACE_YDN</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YDO_SACE_YDO</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>971 rows Ã— 376 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   YAL064W  YAL063C-A  FLO9  GDH3  BDH2  BDH1  CNE1  GPB2  \\\n",
       "AAA_AAA                0.0        0.0   1.0   0.0   0.0   1.0   0.0   1.0   \n",
       "AAB_AAB                0.0        1.0   1.0   0.0   0.0   1.0   1.0   2.0   \n",
       "AAC_AAC                1.0        0.0   4.0   1.0   1.0   1.0   1.0   1.0   \n",
       "AAD_AAD                1.0        1.0   8.0   1.0   1.0   4.0   6.0   6.0   \n",
       "AAE_AAE                0.0        0.0   1.0   0.0   1.0   1.0   1.0   2.0   \n",
       "AAG_AAG                0.0        1.0  10.0   1.0   0.0   3.0   3.0   2.0   \n",
       "AAH_AAH                1.0        1.0   8.0   1.0   1.0   2.0   4.0   5.0   \n",
       "AAI_AAI                0.0        0.0   1.0   0.0   1.0   0.0   0.0   1.0   \n",
       "AAK_AAK                0.0        0.0   1.0   0.0   1.0   0.0   1.0   2.0   \n",
       "AAL_AAL                0.0        0.0   1.0   0.0   1.0   0.0   0.0   0.0   \n",
       "AAM_AAM                0.0        0.0   2.0   1.0   1.0   3.0   2.0   4.0   \n",
       "AAN_AAN                1.0        0.0   6.0   0.0   0.0   1.0   3.0   2.0   \n",
       "AAP_AAP                0.0        0.0   2.0   0.0   0.0   3.0   3.0   0.0   \n",
       "AAQ_AAQ                0.0        1.0   2.0   0.0   0.0   1.0   1.0   0.0   \n",
       "AAR_AAR                0.0        0.0   3.0   0.0   0.0   1.0   2.0   1.0   \n",
       "AAS_AAS                1.0        0.0   6.0   1.0   1.0   1.0   2.0   1.0   \n",
       "AAT_AAT                0.0        1.0   5.0   0.0   1.0   4.0   6.0   6.0   \n",
       "AAV_AAV                0.0        0.0   2.0   0.0   1.0   5.0   2.0   3.0   \n",
       "ABA_ABA                0.0        0.0   2.0   0.0   1.0   1.0   4.0   6.0   \n",
       "ABB_ABB                1.0        1.0   9.0   1.0   1.0   4.0   6.0   6.0   \n",
       "ABD_ABD                1.0        1.0   4.0   0.0   1.0   3.0   3.0   4.0   \n",
       "ABE_ABE                0.0        0.0   1.0   0.0   1.0   1.0   0.0   1.0   \n",
       "ABG_ABG                1.0        1.0   7.0   1.0   1.0   2.0   4.0   4.0   \n",
       "ABH_ABH                0.0        1.0   2.0   1.0   2.0   4.0   7.0   6.0   \n",
       "ABI_ABI                1.0        0.0   6.0   0.0   0.0   1.0   5.0   6.0   \n",
       "ABK_ABK                0.0        0.0   2.0   1.0   2.0   1.0   4.0   2.0   \n",
       "ABL_ABL                1.0        1.0   8.0   0.0   1.0   4.0   5.0   5.0   \n",
       "ABM_ABM                0.0        0.0   1.0   0.0   0.0   0.0   0.0   2.0   \n",
       "ABP_ABP                0.0        0.0   1.0   0.0   1.0   0.0   3.0   1.0   \n",
       "ABQ_ABQ                0.0        0.0   1.0   0.0   1.0   1.0   0.0   1.0   \n",
       "...                    ...        ...   ...   ...   ...   ...   ...   ...   \n",
       "SACE_YBQ_SACE_YBQ      0.0        0.0   3.0   0.0   0.0   1.0   0.0   2.0   \n",
       "SACE_YBR_SACE_YBR      0.0        0.0   1.0   0.0   0.0   0.0   0.0   1.0   \n",
       "SACE_YBS_SACE_YBS      1.0        1.0   8.0   0.0   1.0   4.0   6.0   6.0   \n",
       "SACE_YBT_SACE_YBT      1.0        0.0   4.0   0.0   1.0   3.0   3.0   3.0   \n",
       "SACE_YBU_SACE_YBU      1.0        1.0   6.0   1.0   1.0   2.0   5.0   4.0   \n",
       "SACE_YBV_SACE_YBV      0.0        1.0   9.0   0.0   0.0   2.0   1.0   0.0   \n",
       "SACE_YBW_SACE_YBW      0.0        0.0   3.0   0.0   0.0   4.0   1.0   6.0   \n",
       "SACE_YBX_SACE_YBX      0.0        0.0   2.0   0.0   0.0   0.0   0.0   2.0   \n",
       "SACE_YBY_SACE_YBY      0.0        0.0   1.0   0.0   1.0   1.0   0.0   1.0   \n",
       "SACE_YBZ_SACE_YBZ      0.0        1.0   2.0   0.0   1.0   2.0   3.0   3.0   \n",
       "SACE_YCA_SACE_YCA      0.0        0.0   1.0   0.0   1.0   1.0   0.0   1.0   \n",
       "SACE_YCB_SACE_YCB      1.0        1.0   8.0   0.0   1.0   4.0   5.0   5.0   \n",
       "SACE_YCC_SACE_YCC      0.0        1.0   8.0   0.0   0.0   3.0   3.0   0.0   \n",
       "SACE_YCD_SACE_YCD      0.0        0.0   1.0   0.0   1.0   1.0   0.0   1.0   \n",
       "SACE_YCE_SACE_YCE      1.0        1.0   9.0   0.0   0.0   4.0   2.0   4.0   \n",
       "SACE_YCF_SACE_YCF      0.0        1.0   4.0   0.0   0.0   2.0   1.0   3.0   \n",
       "SACE_YCG_SACE_YCG      1.0        1.0   7.0   0.0   0.0   5.0   3.0   4.0   \n",
       "SACE_YCH_SACE_YCH      1.0        1.0   3.0   0.0   0.0   4.0   4.0   5.0   \n",
       "SACE_YCI_SACE_YCI      0.0        0.0   2.0   0.0   1.0   0.0   1.0   0.0   \n",
       "SACE_YCK_SACE_YCK      1.0        1.0   9.0   1.0   1.0   4.0   6.0   6.0   \n",
       "SACE_YCL_SACE_YCL      1.0        1.0   3.0   0.0   0.0   2.0   3.0   5.0   \n",
       "SACE_YCM_SACE_YCM      1.0        1.0   8.0   0.0   1.0   4.0   5.0   6.0   \n",
       "SACE_YCN_SACE_YCN      1.0        1.0   8.0   1.0   1.0   4.0   6.0   6.0   \n",
       "SACE_YCO_SACE_YCO      1.0        1.0   5.0   0.0   1.0   2.0   4.0   6.0   \n",
       "SACE_YCP_SACE_YCP      0.0        0.0   1.0   0.0   1.0   1.0   0.0   1.0   \n",
       "SACE_YCQ_SACE_YCQ      0.0        0.0   2.0   0.0   1.0   0.0   0.0   1.0   \n",
       "SACE_YCR_SACE_YCR      1.0        1.0   6.0   0.0   0.0   4.0   6.0   6.0   \n",
       "SACE_YDM_SACE_YDM      0.0        0.0   1.0   0.0   0.0   0.0   0.0   1.0   \n",
       "SACE_YDN_SACE_YDN      1.0        1.0   7.0   1.0   0.0   5.0   2.0   1.0   \n",
       "SACE_YDO_SACE_YDO      0.0        0.0   1.0   0.0   0.0   1.0   1.0   1.0   \n",
       "\n",
       "                   PEX22  ACS1  ...  HDA3  AOS1  SEC23  DPM1  GDB1  ATG13  \\\n",
       "AAA_AAA              0.0   3.0  ...   1.0   0.0    0.0   0.0   1.0    0.0   \n",
       "AAB_AAB              1.0   3.0  ...   2.0   0.0    0.0   0.0   1.0    0.0   \n",
       "AAC_AAC              0.0   2.0  ...   2.0   1.0    2.0   0.0   0.0    2.0   \n",
       "AAD_AAD              1.0   8.0  ...   2.0   2.0    4.0   1.0   5.0    5.0   \n",
       "AAE_AAE              0.0   2.0  ...   2.0   0.0    0.0   1.0   2.0    0.0   \n",
       "AAG_AAG              1.0   6.0  ...   1.0   2.0    2.0   1.0   2.0    4.0   \n",
       "AAH_AAH              1.0   6.0  ...   2.0   1.0    0.0   0.0   3.0    3.0   \n",
       "AAI_AAI              0.0   4.0  ...   2.0   0.0    0.0   0.0   1.0    0.0   \n",
       "AAK_AAK              0.0   1.0  ...   2.0   0.0    0.0   1.0   2.0    0.0   \n",
       "AAL_AAL              1.0   0.0  ...   2.0   0.0    0.0   0.0   2.0    2.0   \n",
       "AAM_AAM              2.0   9.0  ...   3.0   0.0    2.0   0.0   4.0    4.0   \n",
       "AAN_AAN              0.0   0.0  ...   2.0   0.0    2.0   0.0   2.0    5.0   \n",
       "AAP_AAP              0.0   0.0  ...   2.0   0.0    2.0   0.0   1.0    3.0   \n",
       "AAQ_AAQ              0.0   4.0  ...   2.0   0.0    0.0   0.0   2.0    2.0   \n",
       "AAR_AAR              0.0   5.0  ...   2.0   0.0    0.0   0.0   2.0    0.0   \n",
       "AAS_AAS              1.0   5.0  ...   4.0   0.0    2.0   1.0   4.0    1.0   \n",
       "AAT_AAT              1.0   6.0  ...   2.0   0.0    0.0   0.0   5.0    1.0   \n",
       "AAV_AAV              0.0   5.0  ...   1.0   2.0    2.0   1.0   3.0    2.0   \n",
       "ABA_ABA              0.0   2.0  ...   3.0   0.0    2.0   0.0   2.0    2.0   \n",
       "ABB_ABB              1.0   8.0  ...   2.0   2.0    4.0   1.0   5.0    5.0   \n",
       "ABD_ABD              0.0   6.0  ...   1.0   1.0    2.0   1.0   3.0    1.0   \n",
       "ABE_ABE              0.0   3.0  ...   0.0   0.0    0.0   0.0   1.0    0.0   \n",
       "ABG_ABG              1.0   5.0  ...   1.0   1.0    0.0   0.0   3.0    1.0   \n",
       "ABH_ABH              1.0   3.0  ...   2.0   2.0    2.0   0.0   5.0    4.0   \n",
       "ABI_ABI              0.0   8.0  ...   2.0   1.0    0.0   1.0   1.0    0.0   \n",
       "ABK_ABK              0.0   2.0  ...   1.0   1.0    4.0   1.0   2.0    2.0   \n",
       "ABL_ABL              1.0   6.0  ...   2.0   2.0    3.0   1.0   5.0    5.0   \n",
       "ABM_ABM              0.0   1.0  ...   2.0   0.0    0.0   1.0   1.0    0.0   \n",
       "ABP_ABP              0.0   1.0  ...   0.0   0.0    0.0   0.0   0.0    2.0   \n",
       "ABQ_ABQ              0.0   3.0  ...   2.0   0.0    0.0   0.0   1.0    0.0   \n",
       "...                  ...   ...  ...   ...   ...    ...   ...   ...    ...   \n",
       "SACE_YBQ_SACE_YBQ    0.0   6.0  ...   0.0   0.0    0.0   0.0   3.0    0.0   \n",
       "SACE_YBR_SACE_YBR    0.0   1.0  ...   2.0   0.0    0.0   1.0   2.0    0.0   \n",
       "SACE_YBS_SACE_YBS    2.0   7.0  ...   2.0   2.0    4.0   1.0   5.0    5.0   \n",
       "SACE_YBT_SACE_YBT    1.0   5.0  ...   0.0   2.0    2.0   1.0   1.0    2.0   \n",
       "SACE_YBU_SACE_YBU    0.0   5.0  ...   2.0   2.0    4.0   0.0   5.0    5.0   \n",
       "SACE_YBV_SACE_YBV    1.0   3.0  ...   2.0   0.0    2.0   0.0   5.0    1.0   \n",
       "SACE_YBW_SACE_YBW    0.0   1.0  ...   2.0   2.0    2.0   1.0   2.0    3.0   \n",
       "SACE_YBX_SACE_YBX    0.0   5.0  ...   2.0   0.0    0.0   0.0   0.0    0.0   \n",
       "SACE_YBY_SACE_YBY    0.0   4.0  ...   2.0   0.0    0.0   0.0   1.0    0.0   \n",
       "SACE_YBZ_SACE_YBZ    0.0   3.0  ...   0.0   1.0    4.0   1.0   2.0    3.0   \n",
       "SACE_YCA_SACE_YCA    0.0   4.0  ...   0.0   0.0    0.0   0.0   2.0    0.0   \n",
       "SACE_YCB_SACE_YCB    1.0   6.0  ...   2.0   2.0    3.0   1.0   5.0    5.0   \n",
       "SACE_YCC_SACE_YCC    0.0   3.0  ...   3.0   0.0    0.0   0.0   1.0    1.0   \n",
       "SACE_YCD_SACE_YCD    0.0   3.0  ...   0.0   0.0    0.0   0.0   2.0    0.0   \n",
       "SACE_YCE_SACE_YCE    1.0   6.0  ...   1.0   0.0    0.0   0.0   3.0    3.0   \n",
       "SACE_YCF_SACE_YCF    1.0   4.0  ...   0.0   1.0    0.0   0.0   3.0    1.0   \n",
       "SACE_YCG_SACE_YCG    0.0   6.0  ...   2.0   1.0    0.0   0.0   1.0    2.0   \n",
       "SACE_YCH_SACE_YCH    0.0   4.0  ...   4.0   0.0    2.0   0.0   5.0    1.0   \n",
       "SACE_YCI_SACE_YCI    1.0   6.0  ...   3.0   0.0    0.0   0.0   4.0    1.0   \n",
       "SACE_YCK_SACE_YCK    2.0   7.0  ...   2.0   2.0    4.0   1.0   5.0    5.0   \n",
       "SACE_YCL_SACE_YCL    0.0   1.0  ...   1.0   2.0    2.0   1.0   1.0    4.0   \n",
       "SACE_YCM_SACE_YCM    1.0   6.0  ...   2.0   2.0    4.0   1.0   5.0    5.0   \n",
       "SACE_YCN_SACE_YCN    1.0   5.0  ...   1.0   2.0    4.0   1.0   4.0    5.0   \n",
       "SACE_YCO_SACE_YCO    1.0   4.0  ...   2.0   1.0    3.0   1.0   4.0    4.0   \n",
       "SACE_YCP_SACE_YCP    0.0   3.0  ...   0.0   0.0    0.0   0.0   2.0    0.0   \n",
       "SACE_YCQ_SACE_YCQ    0.0   3.0  ...   0.0   0.0    0.0   0.0   1.0    0.0   \n",
       "SACE_YCR_SACE_YCR    1.0   7.0  ...   2.0   2.0    3.0   1.0   3.0    5.0   \n",
       "SACE_YDM_SACE_YDM    0.0   1.0  ...   2.0   0.0    0.0   0.0   0.0    1.0   \n",
       "SACE_YDN_SACE_YDN    1.0   5.0  ...   4.0   1.0    2.0   0.0   5.0    5.0   \n",
       "SACE_YDO_SACE_YDO    0.0   5.0  ...   2.0   0.0    0.0   0.0   1.0    0.0   \n",
       "\n",
       "                   PZF1  SKI3  RPC82  QCR2  \n",
       "AAA_AAA             0.0   0.0    0.0   0.0  \n",
       "AAB_AAB             1.0   0.0    0.0   0.0  \n",
       "AAC_AAC             0.0   1.0    3.0   1.0  \n",
       "AAD_AAD             0.0   3.0    4.0   1.0  \n",
       "AAE_AAE             0.0   0.0    0.0   0.0  \n",
       "AAG_AAG             0.0   2.0    0.0   1.0  \n",
       "AAH_AAH             0.0   2.0    2.0   1.0  \n",
       "AAI_AAI             1.0   0.0    0.0   0.0  \n",
       "AAK_AAK             0.0   0.0    0.0   0.0  \n",
       "AAL_AAL             1.0   2.0    2.0   1.0  \n",
       "AAM_AAM             1.0   2.0    3.0   1.0  \n",
       "AAN_AAN             0.0   2.0    2.0   1.0  \n",
       "AAP_AAP             0.0   0.0    0.0   1.0  \n",
       "AAQ_AAQ             1.0   2.0    2.0   0.0  \n",
       "AAR_AAR             1.0   0.0    0.0   0.0  \n",
       "AAS_AAS             1.0   0.0    1.0   1.0  \n",
       "AAT_AAT             0.0   1.0    2.0   1.0  \n",
       "AAV_AAV             1.0   3.0    2.0   1.0  \n",
       "ABA_ABA             1.0   1.0    1.0   0.0  \n",
       "ABB_ABB             1.0   3.0    4.0   1.0  \n",
       "ABD_ABD             0.0   1.0    2.0   1.0  \n",
       "ABE_ABE             0.0   1.0    0.0   0.0  \n",
       "ABG_ABG             0.0   1.0    1.0   1.0  \n",
       "ABH_ABH             1.0   2.0    3.0   0.0  \n",
       "ABI_ABI             2.0   0.0    0.0   0.0  \n",
       "ABK_ABK             1.0   1.0    1.0   0.0  \n",
       "ABL_ABL             1.0   2.0    4.0   1.0  \n",
       "ABM_ABM             1.0   0.0    0.0   0.0  \n",
       "ABP_ABP             0.0   0.0    0.0   0.0  \n",
       "ABQ_ABQ             1.0   0.0    0.0   0.0  \n",
       "...                 ...   ...    ...   ...  \n",
       "SACE_YBQ_SACE_YBQ   1.0   1.0    0.0   0.0  \n",
       "SACE_YBR_SACE_YBR   1.0   0.0    0.0   0.0  \n",
       "SACE_YBS_SACE_YBS   0.0   2.0    3.0   1.0  \n",
       "SACE_YBT_SACE_YBT   1.0   1.0    2.0   1.0  \n",
       "SACE_YBU_SACE_YBU   0.0   2.0    1.0   1.0  \n",
       "SACE_YBV_SACE_YBV   1.0   1.0    2.0   0.0  \n",
       "SACE_YBW_SACE_YBW   0.0   3.0    4.0   0.0  \n",
       "SACE_YBX_SACE_YBX   1.0   0.0    0.0   0.0  \n",
       "SACE_YBY_SACE_YBY   1.0   1.0    0.0   0.0  \n",
       "SACE_YBZ_SACE_YBZ   0.0   1.0    0.0   1.0  \n",
       "SACE_YCA_SACE_YCA   1.0   0.0    0.0   0.0  \n",
       "SACE_YCB_SACE_YCB   1.0   2.0    4.0   1.0  \n",
       "SACE_YCC_SACE_YCC   1.0   2.0    2.0   0.0  \n",
       "SACE_YCD_SACE_YCD   1.0   0.0    0.0   0.0  \n",
       "SACE_YCE_SACE_YCE   0.0   1.0    1.0   1.0  \n",
       "SACE_YCF_SACE_YCF   0.0   0.0    0.0   1.0  \n",
       "SACE_YCG_SACE_YCG   0.0   0.0    1.0   1.0  \n",
       "SACE_YCH_SACE_YCH   0.0   0.0    2.0   1.0  \n",
       "SACE_YCI_SACE_YCI   0.0   1.0    1.0   1.0  \n",
       "SACE_YCK_SACE_YCK   1.0   3.0    4.0   1.0  \n",
       "SACE_YCL_SACE_YCL   0.0   2.0    1.0   1.0  \n",
       "SACE_YCM_SACE_YCM   0.0   2.0    3.0   1.0  \n",
       "SACE_YCN_SACE_YCN   0.0   3.0    3.0   1.0  \n",
       "SACE_YCO_SACE_YCO   0.0   3.0    4.0   1.0  \n",
       "SACE_YCP_SACE_YCP   1.0   0.0    0.0   0.0  \n",
       "SACE_YCQ_SACE_YCQ   1.0   0.0    0.0   0.0  \n",
       "SACE_YCR_SACE_YCR   1.0   2.0    3.0   1.0  \n",
       "SACE_YDM_SACE_YDM   1.0   0.0    0.0   0.0  \n",
       "SACE_YDN_SACE_YDN   0.0   3.0    4.0   1.0  \n",
       "SACE_YDO_SACE_YDO   1.0   0.0    0.0   0.0  \n",
       "\n",
       "[971 rows x 376 columns]"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yeast_gene_matrix_list[0]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLO9</th>\n",
       "      <th>BDH2</th>\n",
       "      <th>BDH1</th>\n",
       "      <th>ECM1</th>\n",
       "      <th>CNE1</th>\n",
       "      <th>GPB2</th>\n",
       "      <th>ACS1</th>\n",
       "      <th>FLC2</th>\n",
       "      <th>OAF1</th>\n",
       "      <th>AIM2</th>\n",
       "      <th>...</th>\n",
       "      <th>TIF3</th>\n",
       "      <th>MMS1</th>\n",
       "      <th>BSP1</th>\n",
       "      <th>VPS4</th>\n",
       "      <th>DPB2</th>\n",
       "      <th>BET2</th>\n",
       "      <th>AOS1</th>\n",
       "      <th>GDB1</th>\n",
       "      <th>ATG13</th>\n",
       "      <th>PZF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAA_AAA</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAB_AAB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAC_AAC</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAD_AAD</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAE_AAE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 242 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         FLO9  BDH2  BDH1  ECM1  CNE1  GPB2  ACS1  FLC2  OAF1  AIM2  ...  \\\n",
       "AAA_AAA   4.0   1.0   1.0   2.0   1.0   0.0   3.0   1.0   2.0   2.0  ...   \n",
       "AAB_AAB   0.0   1.0   0.0   1.0   3.0   1.0   3.0   0.0   1.0   1.0  ...   \n",
       "AAC_AAC   6.0   0.0   0.0   2.0   2.0   0.0   2.0   2.0   1.0   0.0  ...   \n",
       "AAD_AAD  11.0   2.0   1.0   1.0   5.0   6.0   5.0   4.0   6.0   1.0  ...   \n",
       "AAE_AAE   0.0   0.0   0.0   1.0   2.0   0.0   2.0   0.0   2.0   1.0  ...   \n",
       "\n",
       "         TIF3  MMS1  BSP1  VPS4  DPB2  BET2  AOS1  GDB1  ATG13  PZF1  \n",
       "AAA_AAA   1.0   0.0   0.0   0.0   3.0   1.0   0.0   0.0    0.0   0.0  \n",
       "AAB_AAB   1.0   0.0   0.0   0.0   1.0   0.0   1.0   0.0    1.0   0.0  \n",
       "AAC_AAC   1.0   2.0   1.0   1.0   1.0   0.0   2.0   2.0    0.0   0.0  \n",
       "AAD_AAD   1.0   3.0   3.0   2.0   0.0   0.0   4.0   8.0    2.0   3.0  \n",
       "AAE_AAE   0.0   1.0   0.0   0.0   1.0   1.0   0.0   0.0    0.0   0.0  \n",
       "\n",
       "[5 rows x 242 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yeast_gene_matrix_list[1].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as dt\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, h1, h2, h3, output_size, lr):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, h1)\n",
    "        self.l2 = nn.Linear(h1, h2)\n",
    "        self.l3 = nn.Linear(h2, h3)\n",
    "        self.l4 = nn.Linear(h3, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l4(out)\n",
    "        return out\n",
    "     \n",
    "    def train_net(self, num_epochs, train_loader, test_loader):\n",
    "        max_rsq = 0\n",
    "        max_epoch = 0\n",
    "        for epoch in range (num_epochs):\n",
    "            total_loss = 0\n",
    "            counter = 0\n",
    "            for i, (data, target) in enumerate(train_loader):\n",
    "                self.zero_grad()\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = self.forward(data).to(device)\n",
    "                loss = self.loss_fn(output, target)\n",
    "                total_loss += loss\n",
    "                counter += 1\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            # empty arrays for empty tensor \n",
    "            empty1 = np.empty((0,1))\n",
    "            empty2 = np.empty((0,1))\n",
    "            output_tensor = torch.tensor(empty1, dtype = torch.float)\n",
    "            target_tensor = torch.tensor(empty2, dtype = torch.float)\n",
    "            # concatenate output and target to the empty tensors \n",
    "            for i, (data, target) in enumerate(test_loader): \n",
    "                data = data.to(device) \n",
    "                target = target.to(device) \n",
    "                output = self.forward(data).to(device)\n",
    "                target = torch.reshape(target, (len(target),1))\n",
    "                output_tensor = torch.cat((output_tensor, output.cpu()))\n",
    "                target_tensor = torch.cat((target_tensor, target.cpu()))\n",
    "            \n",
    "            #convert the tensor into numpy arrays and calculate correlation \n",
    "            output_array = output_tensor.detach().numpy()\n",
    "            target_array = target_tensor.detach().numpy()\n",
    "            if ((output_array.shape[0]!=target_array.shape[0]) or (output_array.shape[1]!=target_array.shape[1])):\n",
    "                raise Exception('Dimension mismatch')\n",
    "            result = stat.pearsonr(target_array, output_array)\n",
    "            print(\"Correlation: %f \\n P-Value: %f\" %(result[0], result[1])) \n",
    "            if(math.fabs(result[0])>math.fabs(max_rsq)):\n",
    "                max_rsq = result[0]\n",
    "                max_epoch = epoch\n",
    "            epoch += 1\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                .format(epoch, num_epochs, total_loss/len(train_loader)))\n",
    "    \n",
    "        print(\"Max correlation is: {} at {}th epoch\".format(max_rsq, max_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_url = \"./phenoMatrix_35ConditionsNormalizedByYPD.csv\"\n",
    "pheno_yeast_matrix = pd.read_csv(pheno_url, delimiter='\\t', error_bad_lines=False)\n",
    "YPD6AU = pheno_yeast_matrix[\"YPD6AU\"]\n",
    "YPDBENOMYL200 = pheno_yeast_matrix[\"YPDBENOMYL200\"]\n",
    "YPDNACL1M = pheno_yeast_matrix[\"YPDNACL1M\"]\n",
    "YPDSDS = pheno_yeast_matrix[\"YPDSDS\"]\n",
    "YPGALACTOSE = pheno_yeast_matrix[\"YPGALACTOSE\"]\n",
    "YPGLYCEROL = pheno_yeast_matrix[\"YPGLYCEROL\"]\n",
    "YPRIBOSE = pheno_yeast_matrix[\"YPRIBOSE\"]\n",
    "YPSORBITOL = pheno_yeast_matrix[\"YPSORBITOL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "h1 = 100\n",
    "h2 = 50\n",
    "h3 = 20\n",
    "output_size = 1\n",
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 200\n",
    "split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_0 = torch.tensor(yeast_gene_matrix_list[0].values, dtype = torch.float)\n",
    "answer_0 = torch.tensor(YPD6AU, dtype = torch.float)\n",
    "train_set_0 = dt.TensorDataset(training_data_0, answer_0)\n",
    "train_size = int(split * len(train_set_0))\n",
    "test_size = len(train_set_0) - train_size \n",
    "train_dataset_0, test_dataset_0 = dt.random_split(train_set_0, [train_size, test_size])\n",
    "train_loader_0 = dt.DataLoader(dataset=train_dataset_0, batch_size=batch_size)\n",
    "test_loader_0 = dt.DataLoader(dataset=test_dataset_0, batch_size=batch_size)\n",
    "\n",
    "\n",
    "training_data_1 = torch.tensor(yeast_gene_matrix_list[1].values, dtype = torch.float)\n",
    "answer_1 = torch.tensor(YPDBENOMYL200, dtype = torch.float)\n",
    "train_set_1 = dt.TensorDataset(training_data_1, answer_1)\n",
    "train_dataset_1, test_dataset_1 = dt.random_split(train_set_1, [train_size, test_size])\n",
    "train_loader_1 = dt.DataLoader(dataset=train_dataset_1, batch_size=batch_size)\n",
    "test_loader_1 = dt.DataLoader(dataset=test_dataset_1, batch_size=batch_size)\n",
    "\n",
    "\n",
    "training_data_2 = torch.tensor(yeast_gene_matrix_list[2].values, dtype = torch.float)\n",
    "answer_2 = torch.tensor(YPDNACL1M, dtype = torch.float)\n",
    "train_set_2 = dt.TensorDataset(training_data_2, answer_2)\n",
    "train_dataset_2, test_dataset_2 = dt.random_split(train_set_2, [train_size, test_size])\n",
    "train_loader_2 = dt.DataLoader(dataset=train_dataset_2, batch_size=batch_size)\n",
    "test_loader_2 = dt.DataLoader(dataset=test_dataset_2, batch_size=batch_size)\n",
    "\n",
    "\n",
    "training_data_3 = torch.tensor(yeast_gene_matrix_list[3].values, dtype = torch.float)\n",
    "answer_3 = torch.tensor(YPDSDS, dtype = torch.float)\n",
    "train_set_3 = dt.TensorDataset(training_data_3, answer_3)\n",
    "train_dataset_3, test_dataset_3 = dt.random_split(train_set_3, [train_size, test_size])\n",
    "train_loader_3 = dt.DataLoader(dataset=train_dataset_3, batch_size=batch_size)\n",
    "test_loader_3 = dt.DataLoader(dataset=test_dataset_3, batch_size=batch_size)\n",
    "\n",
    "training_data_5 = torch.tensor(yeast_gene_matrix_list[4].values, dtype = torch.float)\n",
    "answer_5 = torch.tensor(YPGALACTOSE, dtype = torch.float)\n",
    "train_set_5 = dt.TensorDataset(training_data_5, answer_5)\n",
    "train_dataset_5, test_dataset_5 = dt.random_split(train_set_5, [train_size, test_size])\n",
    "train_loader_5 = dt.DataLoader(dataset=train_dataset_5, batch_size=batch_size)\n",
    "test_loader_5 = dt.DataLoader(dataset=test_dataset_5, batch_size=batch_size)\n",
    "\n",
    "training_data_6 = torch.tensor(yeast_gene_matrix_list[5].values, dtype = torch.float)\n",
    "answer_6 = torch.tensor(YPGLYCEROL, dtype = torch.float)\n",
    "train_set_6 = dt.TensorDataset(training_data_6, answer_6)\n",
    "train_dataset_6, test_dataset_6 = dt.random_split(train_set_6, [train_size, test_size])\n",
    "train_loader_6 = dt.DataLoader(dataset=train_dataset_6, batch_size=batch_size)\n",
    "test_loader_6 = dt.DataLoader(dataset=test_dataset_6, batch_size=batch_size)\n",
    "\n",
    "training_data_7 = torch.tensor(yeast_gene_matrix_list[6].values, dtype = torch.float)\n",
    "answer_7 = torch.tensor(YPRIBOSE, dtype = torch.float)\n",
    "train_set_7 = dt.TensorDataset(training_data_7, answer_7)\n",
    "train_dataset_7, test_dataset_7 = dt.random_split(train_set_7, [train_size, test_size])\n",
    "train_loader_7 = dt.DataLoader(dataset=train_dataset_7, batch_size=batch_size)\n",
    "test_loader_7 = dt.DataLoader(dataset=test_dataset_7, batch_size=batch_size)\n",
    "\n",
    "training_data_8 = torch.tensor(yeast_gene_matrix_list[7].values, dtype = torch.float)\n",
    "answer_8 = torch.tensor(YPSORBITOL, dtype = torch.float)\n",
    "train_set_8 = dt.TensorDataset(training_data_8, answer_8)\n",
    "train_dataset_8, test_dataset_8 = dt.random_split(train_set_8, [train_size, test_size])\n",
    "train_loader_8 = dt.DataLoader(dataset=train_dataset_8, batch_size=batch_size)\n",
    "test_loader_8 = dt.DataLoader(dataset=test_dataset_8, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_list = [train_loader_0, train_loader_1, train_loader_2, train_loader_3, train_loader_5, train_loader_6, train_loader_7, train_loader_8]\n",
    "test_loader_list = [test_loader_0, test_loader_1, test_loader_2, test_loader_3, test_loader_5, test_loader_6, test_loader_7, test_loader_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n"
     ]
    }
   ],
   "source": [
    "print(int(split * len(train_set_0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: -0.103957 \n",
      " P-Value: 0.148103\n",
      "Epoch [1/100], Loss: 0.1223\n",
      "Correlation: 0.177342 \n",
      " P-Value: 0.013130\n",
      "Epoch [2/100], Loss: 0.0489\n",
      "Correlation: 0.216044 \n",
      " P-Value: 0.002418\n",
      "Epoch [3/100], Loss: 0.0358\n",
      "Correlation: 0.230327 \n",
      " P-Value: 0.001198\n",
      "Epoch [4/100], Loss: 0.0356\n",
      "Correlation: 0.067649 \n",
      " P-Value: 0.347387\n",
      "Epoch [5/100], Loss: 0.0297\n",
      "Correlation: 0.037864 \n",
      " P-Value: 0.599216\n",
      "Epoch [6/100], Loss: 0.0268\n",
      "Correlation: 0.030737 \n",
      " P-Value: 0.669705\n",
      "Epoch [7/100], Loss: 0.0239\n",
      "Correlation: -0.003450 \n",
      " P-Value: 0.961793\n",
      "Epoch [8/100], Loss: 0.0219\n",
      "Correlation: -0.081135 \n",
      " P-Value: 0.259505\n",
      "Epoch [9/100], Loss: 0.0212\n",
      "Correlation: -0.117717 \n",
      " P-Value: 0.101221\n",
      "Epoch [10/100], Loss: 0.0208\n",
      "Correlation: -0.087992 \n",
      " P-Value: 0.221245\n",
      "Epoch [11/100], Loss: 0.0204\n",
      "Correlation: -0.071595 \n",
      " P-Value: 0.319923\n",
      "Epoch [12/100], Loss: 0.0200\n",
      "Correlation: -0.074027 \n",
      " P-Value: 0.303717\n",
      "Epoch [13/100], Loss: 0.0197\n",
      "Correlation: -0.086717 \n",
      " P-Value: 0.228040\n",
      "Epoch [14/100], Loss: 0.0194\n",
      "Correlation: -0.087033 \n",
      " P-Value: 0.226341\n",
      "Epoch [15/100], Loss: 0.0191\n",
      "Correlation: -0.074270 \n",
      " P-Value: 0.302129\n",
      "Epoch [16/100], Loss: 0.0189\n",
      "Correlation: -0.061307 \n",
      " P-Value: 0.394546\n",
      "Epoch [17/100], Loss: 0.0188\n",
      "Correlation: -0.058267 \n",
      " P-Value: 0.418449\n",
      "Epoch [18/100], Loss: 0.0186\n",
      "Correlation: -0.073860 \n",
      " P-Value: 0.304813\n",
      "Epoch [19/100], Loss: 0.0185\n",
      "Correlation: -0.086687 \n",
      " P-Value: 0.228199\n",
      "Epoch [20/100], Loss: 0.0183\n",
      "Correlation: -0.075650 \n",
      " P-Value: 0.293205\n",
      "Epoch [21/100], Loss: 0.0182\n",
      "Correlation: -0.060859 \n",
      " P-Value: 0.398019\n",
      "Epoch [22/100], Loss: 0.0182\n",
      "Correlation: -0.069883 \n",
      " P-Value: 0.331659\n",
      "Epoch [23/100], Loss: 0.0180\n",
      "Correlation: -0.085046 \n",
      " P-Value: 0.237163\n",
      "Epoch [24/100], Loss: 0.0179\n",
      "Correlation: -0.078644 \n",
      " P-Value: 0.274468\n",
      "Epoch [25/100], Loss: 0.0179\n",
      "Correlation: -0.070067 \n",
      " P-Value: 0.330381\n",
      "Epoch [26/100], Loss: 0.0178\n",
      "Correlation: -0.083020 \n",
      " P-Value: 0.248561\n",
      "Epoch [27/100], Loss: 0.0177\n",
      "Correlation: -0.095542 \n",
      " P-Value: 0.183970\n",
      "Epoch [28/100], Loss: 0.0177\n",
      "Correlation: -0.088383 \n",
      " P-Value: 0.219197\n",
      "Epoch [29/100], Loss: 0.0176\n",
      "Correlation: -0.080237 \n",
      " P-Value: 0.264832\n",
      "Epoch [30/100], Loss: 0.0176\n",
      "Correlation: -0.087930 \n",
      " P-Value: 0.221575\n",
      "Epoch [31/100], Loss: 0.0176\n",
      "Correlation: -0.097923 \n",
      " P-Value: 0.173227\n",
      "Epoch [32/100], Loss: 0.0175\n",
      "Correlation: -0.101935 \n",
      " P-Value: 0.156195\n",
      "Epoch [33/100], Loss: 0.0174\n",
      "Correlation: -0.101425 \n",
      " P-Value: 0.158285\n",
      "Epoch [34/100], Loss: 0.0174\n",
      "Correlation: -0.095867 \n",
      " P-Value: 0.182475\n",
      "Epoch [35/100], Loss: 0.0174\n",
      "Correlation: -0.093223 \n",
      " P-Value: 0.194890\n",
      "Epoch [36/100], Loss: 0.0174\n",
      "Correlation: -0.095144 \n",
      " P-Value: 0.185810\n",
      "Epoch [37/100], Loss: 0.0173\n",
      "Correlation: -0.103552 \n",
      " P-Value: 0.149697\n",
      "Epoch [38/100], Loss: 0.0173\n",
      "Correlation: -0.114393 \n",
      " P-Value: 0.111300\n",
      "Epoch [39/100], Loss: 0.0173\n",
      "Correlation: -0.114997 \n",
      " P-Value: 0.109412\n",
      "Epoch [40/100], Loss: 0.0172\n",
      "Correlation: -0.108917 \n",
      " P-Value: 0.129602\n",
      "Epoch [41/100], Loss: 0.0172\n",
      "Correlation: -0.104591 \n",
      " P-Value: 0.145630\n",
      "Epoch [42/100], Loss: 0.0172\n",
      "Correlation: -0.101685 \n",
      " P-Value: 0.157219\n",
      "Epoch [43/100], Loss: 0.0172\n",
      "Correlation: -0.103164 \n",
      " P-Value: 0.151236\n",
      "Epoch [44/100], Loss: 0.0172\n",
      "Correlation: -0.108021 \n",
      " P-Value: 0.132806\n",
      "Epoch [45/100], Loss: 0.0172\n",
      "Correlation: -0.114126 \n",
      " P-Value: 0.112144\n",
      "Epoch [46/100], Loss: 0.0172\n",
      "Correlation: -0.120536 \n",
      " P-Value: 0.093251\n",
      "Epoch [47/100], Loss: 0.0171\n",
      "Correlation: -0.122544 \n",
      " P-Value: 0.087884\n",
      "Epoch [48/100], Loss: 0.0171\n",
      "Correlation: -0.121962 \n",
      " P-Value: 0.089413\n",
      "Epoch [49/100], Loss: 0.0171\n",
      "Correlation: -0.121178 \n",
      " P-Value: 0.091505\n",
      "Epoch [50/100], Loss: 0.0171\n",
      "Correlation: -0.117199 \n",
      " P-Value: 0.102742\n",
      "Epoch [51/100], Loss: 0.0171\n",
      "Correlation: -0.113051 \n",
      " P-Value: 0.115586\n",
      "Epoch [52/100], Loss: 0.0171\n",
      "Correlation: -0.113298 \n",
      " P-Value: 0.114789\n",
      "Epoch [53/100], Loss: 0.0171\n",
      "Correlation: -0.116186 \n",
      " P-Value: 0.105770\n",
      "Epoch [54/100], Loss: 0.0171\n",
      "Correlation: -0.122822 \n",
      " P-Value: 0.087161\n",
      "Epoch [55/100], Loss: 0.0171\n",
      "Correlation: -0.131911 \n",
      " P-Value: 0.066028\n",
      "Epoch [56/100], Loss: 0.0171\n",
      "Correlation: -0.137497 \n",
      " P-Value: 0.055263\n",
      "Epoch [57/100], Loss: 0.0171\n",
      "Correlation: -0.142105 \n",
      " P-Value: 0.047514\n",
      "Epoch [58/100], Loss: 0.0170\n",
      "Correlation: -0.145254 \n",
      " P-Value: 0.042757\n",
      "Epoch [59/100], Loss: 0.0170\n",
      "Correlation: -0.140114 \n",
      " P-Value: 0.050742\n",
      "Epoch [60/100], Loss: 0.0170\n",
      "Correlation: -0.133226 \n",
      " P-Value: 0.063350\n",
      "Epoch [61/100], Loss: 0.0170\n",
      "Correlation: -0.124641 \n",
      " P-Value: 0.082545\n",
      "Epoch [62/100], Loss: 0.0170\n",
      "Correlation: -0.115835 \n",
      " P-Value: 0.106835\n",
      "Epoch [63/100], Loss: 0.0171\n",
      "Correlation: -0.111163 \n",
      " P-Value: 0.121831\n",
      "Epoch [64/100], Loss: 0.0171\n",
      "Correlation: -0.111175 \n",
      " P-Value: 0.121794\n",
      "Epoch [65/100], Loss: 0.0171\n",
      "Correlation: -0.120609 \n",
      " P-Value: 0.093049\n",
      "Epoch [66/100], Loss: 0.0171\n",
      "Correlation: -0.140548 \n",
      " P-Value: 0.050023\n",
      "Epoch [67/100], Loss: 0.0171\n",
      "Correlation: -0.157865 \n",
      " P-Value: 0.027515\n",
      "Epoch [68/100], Loss: 0.0170\n",
      "Correlation: -0.169229 \n",
      " P-Value: 0.018028\n",
      "Epoch [69/100], Loss: 0.0170\n",
      "Correlation: -0.171059 \n",
      " P-Value: 0.016802\n",
      "Epoch [70/100], Loss: 0.0170\n",
      "Correlation: -0.162436 \n",
      " P-Value: 0.023281\n",
      "Epoch [71/100], Loss: 0.0170\n",
      "Correlation: -0.145812 \n",
      " P-Value: 0.041957\n",
      "Epoch [72/100], Loss: 0.0170\n",
      "Correlation: -0.117026 \n",
      " P-Value: 0.103256\n",
      "Epoch [73/100], Loss: 0.0171\n",
      "Correlation: -0.101594 \n",
      " P-Value: 0.157590\n",
      "Epoch [74/100], Loss: 0.0171\n",
      "Correlation: -0.107252 \n",
      " P-Value: 0.135601\n",
      "Epoch [75/100], Loss: 0.0171\n",
      "Correlation: -0.127620 \n",
      " P-Value: 0.075415\n",
      "Epoch [76/100], Loss: 0.0171\n",
      "Correlation: -0.161999 \n",
      " P-Value: 0.023660\n",
      "Epoch [77/100], Loss: 0.0171\n",
      "Correlation: -0.168223 \n",
      " P-Value: 0.018734\n",
      "Epoch [78/100], Loss: 0.0170\n",
      "Correlation: -0.158314 \n",
      " P-Value: 0.027072\n",
      "Epoch [79/100], Loss: 0.0170\n",
      "Correlation: -0.148730 \n",
      " P-Value: 0.037976\n",
      "Epoch [80/100], Loss: 0.0170\n",
      "Correlation: -0.127083 \n",
      " P-Value: 0.076662\n",
      "Epoch [81/100], Loss: 0.0170\n",
      "Correlation: -0.120490 \n",
      " P-Value: 0.093376\n",
      "Epoch [82/100], Loss: 0.0170\n",
      "Correlation: -0.126117 \n",
      " P-Value: 0.078948\n",
      "Epoch [83/100], Loss: 0.0170\n",
      "Correlation: -0.142256 \n",
      " P-Value: 0.047275\n",
      "Epoch [84/100], Loss: 0.0170\n",
      "Correlation: -0.159548 \n",
      " P-Value: 0.025885\n",
      "Epoch [85/100], Loss: 0.0170\n",
      "Correlation: -0.166061 \n",
      " P-Value: 0.020334\n",
      "Epoch [86/100], Loss: 0.0170\n",
      "Correlation: -0.156331 \n",
      " P-Value: 0.029077\n",
      "Epoch [87/100], Loss: 0.0170\n",
      "Correlation: -0.142363 \n",
      " P-Value: 0.047108\n",
      "Epoch [88/100], Loss: 0.0170\n",
      "Correlation: -0.133921 \n",
      " P-Value: 0.061972\n",
      "Epoch [89/100], Loss: 0.0170\n",
      "Correlation: -0.126332 \n",
      " P-Value: 0.078434\n",
      "Epoch [90/100], Loss: 0.0170\n",
      "Correlation: -0.141352 \n",
      " P-Value: 0.048713\n",
      "Epoch [91/100], Loss: 0.0170\n",
      "Correlation: -0.153970 \n",
      " P-Value: 0.031627\n",
      "Epoch [92/100], Loss: 0.0170\n",
      "Correlation: -0.160019 \n",
      " P-Value: 0.025444\n",
      "Epoch [93/100], Loss: 0.0170\n",
      "Correlation: -0.166353 \n",
      " P-Value: 0.020111\n",
      "Epoch [94/100], Loss: 0.0170\n",
      "Correlation: -0.158267 \n",
      " P-Value: 0.027118\n",
      "Epoch [95/100], Loss: 0.0170\n",
      "Correlation: -0.151721 \n",
      " P-Value: 0.034233\n",
      "Epoch [96/100], Loss: 0.0170\n",
      "Correlation: -0.142500 \n",
      " P-Value: 0.046892\n",
      "Epoch [97/100], Loss: 0.0170\n",
      "Correlation: -0.133576 \n",
      " P-Value: 0.062653\n",
      "Epoch [98/100], Loss: 0.0170\n",
      "Correlation: -0.134531 \n",
      " P-Value: 0.060782\n",
      "Epoch [99/100], Loss: 0.0170\n",
      "Correlation: -0.143090 \n",
      " P-Value: 0.045980\n",
      "Epoch [100/100], Loss: 0.0170\n",
      "Max correlation is: [0.2303267] at 3th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 376\n",
    "net_0 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_0.train_net(num_epochs, train_loader_list[0], test_loader_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.000605 \n",
      " P-Value: 0.992849\n",
      "Epoch [1/100], Loss: 0.0158\n",
      "Correlation: 0.022077 \n",
      " P-Value: 0.759345\n",
      "Epoch [2/100], Loss: 0.0111\n",
      "Correlation: -0.001336 \n",
      " P-Value: 0.984953\n",
      "Epoch [3/100], Loss: 0.0105\n",
      "Correlation: -0.134255 \n",
      " P-Value: 0.061318\n",
      "Epoch [4/100], Loss: 0.0100\n",
      "Correlation: -0.078696 \n",
      " P-Value: 0.274143\n",
      "Epoch [5/100], Loss: 0.0098\n",
      "Correlation: 0.040840 \n",
      " P-Value: 0.570801\n",
      "Epoch [6/100], Loss: 0.0097\n",
      "Correlation: 0.009288 \n",
      " P-Value: 0.897473\n",
      "Epoch [7/100], Loss: 0.0097\n",
      "Correlation: 0.025219 \n",
      " P-Value: 0.726363\n",
      "Epoch [8/100], Loss: 0.0096\n",
      "Correlation: 0.043958 \n",
      " P-Value: 0.541735\n",
      "Epoch [9/100], Loss: 0.0096\n",
      "Correlation: 0.035506 \n",
      " P-Value: 0.622164\n",
      "Epoch [10/100], Loss: 0.0096\n",
      "Correlation: 0.001756 \n",
      " P-Value: 0.980513\n",
      "Epoch [11/100], Loss: 0.0096\n",
      "Correlation: -0.011915 \n",
      " P-Value: 0.868687\n",
      "Epoch [12/100], Loss: 0.0096\n",
      "Correlation: -0.002773 \n",
      " P-Value: 0.969311\n",
      "Epoch [13/100], Loss: 0.0095\n",
      "Correlation: 0.002834 \n",
      " P-Value: 0.968606\n",
      "Epoch [14/100], Loss: 0.0095\n",
      "Correlation: 0.015905 \n",
      " P-Value: 0.825318\n",
      "Epoch [15/100], Loss: 0.0095\n",
      "Correlation: 0.013041 \n",
      " P-Value: 0.856393\n",
      "Epoch [16/100], Loss: 0.0095\n",
      "Correlation: 0.015891 \n",
      " P-Value: 0.825480\n",
      "Epoch [17/100], Loss: 0.0095\n",
      "Correlation: 0.015547 \n",
      " P-Value: 0.829211\n",
      "Epoch [18/100], Loss: 0.0095\n",
      "Correlation: 0.007496 \n",
      " P-Value: 0.917153\n",
      "Epoch [19/100], Loss: 0.0095\n",
      "Correlation: 0.005468 \n",
      " P-Value: 0.939563\n",
      "Epoch [20/100], Loss: 0.0095\n",
      "Correlation: 0.003329 \n",
      " P-Value: 0.963153\n",
      "Epoch [21/100], Loss: 0.0095\n",
      "Correlation: 0.010333 \n",
      " P-Value: 0.885978\n",
      "Epoch [22/100], Loss: 0.0095\n",
      "Correlation: 0.011879 \n",
      " P-Value: 0.869097\n",
      "Epoch [23/100], Loss: 0.0095\n",
      "Correlation: 0.009195 \n",
      " P-Value: 0.898464\n",
      "Epoch [24/100], Loss: 0.0095\n",
      "Correlation: 0.006747 \n",
      " P-Value: 0.925404\n",
      "Epoch [25/100], Loss: 0.0095\n",
      "Correlation: 0.005416 \n",
      " P-Value: 0.940107\n",
      "Epoch [26/100], Loss: 0.0095\n",
      "Correlation: 0.006619 \n",
      " P-Value: 0.926830\n",
      "Epoch [27/100], Loss: 0.0095\n",
      "Correlation: 0.004729 \n",
      " P-Value: 0.947700\n",
      "Epoch [28/100], Loss: 0.0095\n",
      "Correlation: 0.006685 \n",
      " P-Value: 0.926138\n",
      "Epoch [29/100], Loss: 0.0095\n",
      "Correlation: 0.008045 \n",
      " P-Value: 0.911117\n",
      "Epoch [30/100], Loss: 0.0095\n",
      "Correlation: 0.006405 \n",
      " P-Value: 0.929202\n",
      "Epoch [31/100], Loss: 0.0095\n",
      "Correlation: 0.002551 \n",
      " P-Value: 0.971789\n",
      "Epoch [32/100], Loss: 0.0095\n",
      "Correlation: 0.007288 \n",
      " P-Value: 0.919462\n",
      "Epoch [33/100], Loss: 0.0095\n",
      "Correlation: 0.005700 \n",
      " P-Value: 0.936970\n",
      "Epoch [34/100], Loss: 0.0095\n",
      "Correlation: 0.002942 \n",
      " P-Value: 0.967464\n",
      "Epoch [35/100], Loss: 0.0095\n",
      "Correlation: 0.004884 \n",
      " P-Value: 0.945920\n",
      "Epoch [36/100], Loss: 0.0095\n",
      "Correlation: 0.005675 \n",
      " P-Value: 0.937259\n",
      "Epoch [37/100], Loss: 0.0095\n",
      "Correlation: 0.007050 \n",
      " P-Value: 0.922072\n",
      "Epoch [38/100], Loss: 0.0095\n",
      "Correlation: 0.007201 \n",
      " P-Value: 0.920413\n",
      "Epoch [39/100], Loss: 0.0095\n",
      "Correlation: 0.008030 \n",
      " P-Value: 0.911281\n",
      "Epoch [40/100], Loss: 0.0095\n",
      "Correlation: 0.010036 \n",
      " P-Value: 0.889249\n",
      "Epoch [41/100], Loss: 0.0095\n",
      "Correlation: 0.010136 \n",
      " P-Value: 0.888147\n",
      "Epoch [42/100], Loss: 0.0095\n",
      "Correlation: 0.011349 \n",
      " P-Value: 0.874903\n",
      "Epoch [43/100], Loss: 0.0095\n",
      "Correlation: 0.012956 \n",
      " P-Value: 0.857342\n",
      "Epoch [44/100], Loss: 0.0095\n",
      "Correlation: 0.012265 \n",
      " P-Value: 0.864866\n",
      "Epoch [45/100], Loss: 0.0095\n",
      "Correlation: 0.013242 \n",
      " P-Value: 0.854244\n",
      "Epoch [46/100], Loss: 0.0095\n",
      "Correlation: 0.013127 \n",
      " P-Value: 0.855475\n",
      "Epoch [47/100], Loss: 0.0095\n",
      "Correlation: 0.016347 \n",
      " P-Value: 0.820569\n",
      "Epoch [48/100], Loss: 0.0095\n",
      "Correlation: 0.013114 \n",
      " P-Value: 0.855599\n",
      "Epoch [49/100], Loss: 0.0095\n",
      "Correlation: 0.015319 \n",
      " P-Value: 0.831677\n",
      "Epoch [50/100], Loss: 0.0095\n",
      "Correlation: 0.014715 \n",
      " P-Value: 0.838235\n",
      "Epoch [51/100], Loss: 0.0095\n",
      "Correlation: 0.016247 \n",
      " P-Value: 0.821634\n",
      "Epoch [52/100], Loss: 0.0095\n",
      "Correlation: 0.015975 \n",
      " P-Value: 0.824571\n",
      "Epoch [53/100], Loss: 0.0095\n",
      "Correlation: 0.016317 \n",
      " P-Value: 0.820903\n",
      "Epoch [54/100], Loss: 0.0095\n",
      "Correlation: 0.017894 \n",
      " P-Value: 0.803910\n",
      "Epoch [55/100], Loss: 0.0095\n",
      "Correlation: 0.017720 \n",
      " P-Value: 0.805780\n",
      "Epoch [56/100], Loss: 0.0095\n",
      "Correlation: 0.019108 \n",
      " P-Value: 0.790897\n",
      "Epoch [57/100], Loss: 0.0095\n",
      "Correlation: 0.018861 \n",
      " P-Value: 0.793550\n",
      "Epoch [58/100], Loss: 0.0095\n",
      "Correlation: 0.018650 \n",
      " P-Value: 0.795795\n",
      "Epoch [59/100], Loss: 0.0095\n",
      "Correlation: 0.021714 \n",
      " P-Value: 0.763188\n",
      "Epoch [60/100], Loss: 0.0095\n",
      "Correlation: 0.019331 \n",
      " P-Value: 0.788511\n",
      "Epoch [61/100], Loss: 0.0095\n",
      "Correlation: 0.021813 \n",
      " P-Value: 0.762130\n",
      "Epoch [62/100], Loss: 0.0095\n",
      "Correlation: 0.016202 \n",
      " P-Value: 0.822130\n",
      "Epoch [63/100], Loss: 0.0095\n",
      "Correlation: 0.022268 \n",
      " P-Value: 0.757329\n",
      "Epoch [64/100], Loss: 0.0095\n",
      "Correlation: 0.022681 \n",
      " P-Value: 0.752964\n",
      "Epoch [65/100], Loss: 0.0095\n",
      "Correlation: 0.021456 \n",
      " P-Value: 0.765921\n",
      "Epoch [66/100], Loss: 0.0095\n",
      "Correlation: 0.023174 \n",
      " P-Value: 0.747778\n",
      "Epoch [67/100], Loss: 0.0095\n",
      "Correlation: 0.022808 \n",
      " P-Value: 0.751639\n",
      "Epoch [68/100], Loss: 0.0095\n",
      "Correlation: 0.018629 \n",
      " P-Value: 0.796035\n",
      "Epoch [69/100], Loss: 0.0095\n",
      "Correlation: 0.019690 \n",
      " P-Value: 0.784696\n",
      "Epoch [70/100], Loss: 0.0095\n",
      "Correlation: 0.027141 \n",
      " P-Value: 0.706452\n",
      "Epoch [71/100], Loss: 0.0095\n",
      "Correlation: 0.021151 \n",
      " P-Value: 0.769140\n",
      "Epoch [72/100], Loss: 0.0095\n",
      "Correlation: 0.024765 \n",
      " P-Value: 0.731114\n",
      "Epoch [73/100], Loss: 0.0095\n",
      "Correlation: 0.019889 \n",
      " P-Value: 0.782576\n",
      "Epoch [74/100], Loss: 0.0095\n",
      "Correlation: 0.030305 \n",
      " P-Value: 0.674076\n",
      "Epoch [75/100], Loss: 0.0095\n",
      "Correlation: 0.023065 \n",
      " P-Value: 0.748917\n",
      "Epoch [76/100], Loss: 0.0095\n",
      "Correlation: 0.024192 \n",
      " P-Value: 0.737096\n",
      "Epoch [77/100], Loss: 0.0095\n",
      "Correlation: 0.021798 \n",
      " P-Value: 0.762289\n",
      "Epoch [78/100], Loss: 0.0095\n",
      "Correlation: 0.028795 \n",
      " P-Value: 0.689451\n",
      "Epoch [79/100], Loss: 0.0095\n",
      "Correlation: 0.027723 \n",
      " P-Value: 0.700452\n",
      "Epoch [80/100], Loss: 0.0095\n",
      "Correlation: 0.020938 \n",
      " P-Value: 0.771424\n",
      "Epoch [81/100], Loss: 0.0095\n",
      "Correlation: 0.026015 \n",
      " P-Value: 0.718101\n",
      "Epoch [82/100], Loss: 0.0095\n",
      "Correlation: 0.032471 \n",
      " P-Value: 0.652255\n",
      "Epoch [83/100], Loss: 0.0095\n",
      "Correlation: 0.023741 \n",
      " P-Value: 0.741826\n",
      "Epoch [84/100], Loss: 0.0095\n",
      "Correlation: 0.026633 \n",
      " P-Value: 0.711697\n",
      "Epoch [85/100], Loss: 0.0095\n",
      "Correlation: 0.031027 \n",
      " P-Value: 0.666769\n",
      "Epoch [86/100], Loss: 0.0095\n",
      "Correlation: 0.026657 \n",
      " P-Value: 0.711430\n",
      "Epoch [87/100], Loss: 0.0095\n",
      "Correlation: 0.022691 \n",
      " P-Value: 0.752853\n",
      "Epoch [88/100], Loss: 0.0095\n",
      "Correlation: 0.024293 \n",
      " P-Value: 0.736041\n",
      "Epoch [89/100], Loss: 0.0095\n",
      "Correlation: 0.025513 \n",
      " P-Value: 0.723305\n",
      "Epoch [90/100], Loss: 0.0095\n",
      "Correlation: 0.022563 \n",
      " P-Value: 0.754228\n",
      "Epoch [91/100], Loss: 0.0095\n",
      "Correlation: 0.025444 \n",
      " P-Value: 0.724023\n",
      "Epoch [92/100], Loss: 0.0095\n",
      "Correlation: 0.031699 \n",
      " P-Value: 0.659999\n",
      "Epoch [93/100], Loss: 0.0095\n",
      "Correlation: 0.029790 \n",
      " P-Value: 0.679313\n",
      "Epoch [94/100], Loss: 0.0095\n",
      "Correlation: 0.040209 \n",
      " P-Value: 0.576772\n",
      "Epoch [95/100], Loss: 0.0095\n",
      "Correlation: 0.039279 \n",
      " P-Value: 0.585623\n",
      "Epoch [96/100], Loss: 0.0095\n",
      "Correlation: 0.039246 \n",
      " P-Value: 0.585942\n",
      "Epoch [97/100], Loss: 0.0095\n",
      "Correlation: 0.034662 \n",
      " P-Value: 0.630474\n",
      "Epoch [98/100], Loss: 0.0095\n",
      "Correlation: 0.030369 \n",
      " P-Value: 0.673428\n",
      "Epoch [99/100], Loss: 0.0095\n",
      "Correlation: 0.020546 \n",
      " P-Value: 0.775561\n",
      "Epoch [100/100], Loss: 0.0095\n",
      "Max correlation is: [-0.13425493] at 3th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 371\n",
    "net_1 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_1.train_net(num_epochs, train_loader_list[1], test_loader_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: -0.029119 \n",
      " P-Value: 0.686139\n",
      "Epoch [1/100], Loss: 0.0341\n",
      "Correlation: 0.059690 \n",
      " P-Value: 0.407160\n",
      "Epoch [2/100], Loss: 0.0134\n",
      "Correlation: 0.142289 \n",
      " P-Value: 0.047223\n",
      "Epoch [3/100], Loss: 0.0118\n",
      "Correlation: 0.023395 \n",
      " P-Value: 0.745450\n",
      "Epoch [4/100], Loss: 0.0116\n",
      "Correlation: 0.091103 \n",
      " P-Value: 0.205285\n",
      "Epoch [5/100], Loss: 0.0115\n",
      "Correlation: 0.054921 \n",
      " P-Value: 0.445710\n",
      "Epoch [6/100], Loss: 0.0110\n",
      "Correlation: 0.094024 \n",
      " P-Value: 0.191067\n",
      "Epoch [7/100], Loss: 0.0110\n",
      "Correlation: 0.049966 \n",
      " P-Value: 0.487880\n",
      "Epoch [8/100], Loss: 0.0108\n",
      "Correlation: 0.053566 \n",
      " P-Value: 0.457038\n",
      "Epoch [9/100], Loss: 0.0108\n",
      "Correlation: 0.036718 \n",
      " P-Value: 0.610324\n",
      "Epoch [10/100], Loss: 0.0107\n",
      "Correlation: 0.036258 \n",
      " P-Value: 0.614806\n",
      "Epoch [11/100], Loss: 0.0107\n",
      "Correlation: 0.048865 \n",
      " P-Value: 0.497533\n",
      "Epoch [12/100], Loss: 0.0107\n",
      "Correlation: 0.045162 \n",
      " P-Value: 0.530718\n",
      "Epoch [13/100], Loss: 0.0107\n",
      "Correlation: 0.041046 \n",
      " P-Value: 0.568859\n",
      "Epoch [14/100], Loss: 0.0106\n",
      "Correlation: 0.037826 \n",
      " P-Value: 0.599573\n",
      "Epoch [15/100], Loss: 0.0106\n",
      "Correlation: 0.047493 \n",
      " P-Value: 0.509698\n",
      "Epoch [16/100], Loss: 0.0106\n",
      "Correlation: 0.056940 \n",
      " P-Value: 0.429148\n",
      "Epoch [17/100], Loss: 0.0106\n",
      "Correlation: 0.053845 \n",
      " P-Value: 0.454693\n",
      "Epoch [18/100], Loss: 0.0106\n",
      "Correlation: 0.057419 \n",
      " P-Value: 0.425270\n",
      "Epoch [19/100], Loss: 0.0106\n",
      "Correlation: 0.079222 \n",
      " P-Value: 0.270943\n",
      "Epoch [20/100], Loss: 0.0106\n",
      "Correlation: 0.070655 \n",
      " P-Value: 0.326336\n",
      "Epoch [21/100], Loss: 0.0106\n",
      "Correlation: 0.067520 \n",
      " P-Value: 0.348307\n",
      "Epoch [22/100], Loss: 0.0106\n",
      "Correlation: 0.062977 \n",
      " P-Value: 0.381774\n",
      "Epoch [23/100], Loss: 0.0105\n",
      "Correlation: 0.068631 \n",
      " P-Value: 0.340417\n",
      "Epoch [24/100], Loss: 0.0105\n",
      "Correlation: 0.070715 \n",
      " P-Value: 0.325919\n",
      "Epoch [25/100], Loss: 0.0105\n",
      "Correlation: 0.070921 \n",
      " P-Value: 0.324510\n",
      "Epoch [26/100], Loss: 0.0105\n",
      "Correlation: 0.076419 \n",
      " P-Value: 0.288314\n",
      "Epoch [27/100], Loss: 0.0105\n",
      "Correlation: 0.080239 \n",
      " P-Value: 0.264817\n",
      "Epoch [28/100], Loss: 0.0105\n",
      "Correlation: 0.080627 \n",
      " P-Value: 0.262508\n",
      "Epoch [29/100], Loss: 0.0105\n",
      "Correlation: 0.085067 \n",
      " P-Value: 0.237045\n",
      "Epoch [30/100], Loss: 0.0105\n",
      "Correlation: 0.083491 \n",
      " P-Value: 0.245877\n",
      "Epoch [31/100], Loss: 0.0105\n",
      "Correlation: 0.082020 \n",
      " P-Value: 0.254326\n",
      "Epoch [32/100], Loss: 0.0105\n",
      "Correlation: 0.082178 \n",
      " P-Value: 0.253408\n",
      "Epoch [33/100], Loss: 0.0105\n",
      "Correlation: 0.062842 \n",
      " P-Value: 0.382794\n",
      "Epoch [34/100], Loss: 0.0105\n",
      "Correlation: 0.064415 \n",
      " P-Value: 0.370971\n",
      "Epoch [35/100], Loss: 0.0105\n",
      "Correlation: 0.068182 \n",
      " P-Value: 0.343591\n",
      "Epoch [36/100], Loss: 0.0105\n",
      "Correlation: 0.079044 \n",
      " P-Value: 0.272023\n",
      "Epoch [37/100], Loss: 0.0105\n",
      "Correlation: 0.080920 \n",
      " P-Value: 0.260771\n",
      "Epoch [38/100], Loss: 0.0105\n",
      "Correlation: 0.067985 \n",
      " P-Value: 0.344991\n",
      "Epoch [39/100], Loss: 0.0105\n",
      "Correlation: 0.068371 \n",
      " P-Value: 0.342249\n",
      "Epoch [40/100], Loss: 0.0105\n",
      "Correlation: 0.067808 \n",
      " P-Value: 0.346250\n",
      "Epoch [41/100], Loss: 0.0105\n",
      "Correlation: 0.078549 \n",
      " P-Value: 0.275046\n",
      "Epoch [42/100], Loss: 0.0105\n",
      "Correlation: 0.069804 \n",
      " P-Value: 0.332207\n",
      "Epoch [43/100], Loss: 0.0105\n",
      "Correlation: 0.068596 \n",
      " P-Value: 0.340662\n",
      "Epoch [44/100], Loss: 0.0105\n",
      "Correlation: 0.073040 \n",
      " P-Value: 0.310229\n",
      "Epoch [45/100], Loss: 0.0105\n",
      "Correlation: 0.071839 \n",
      " P-Value: 0.318267\n",
      "Epoch [46/100], Loss: 0.0105\n",
      "Correlation: 0.067390 \n",
      " P-Value: 0.349244\n",
      "Epoch [47/100], Loss: 0.0105\n",
      "Correlation: 0.069110 \n",
      " P-Value: 0.337050\n",
      "Epoch [48/100], Loss: 0.0105\n",
      "Correlation: 0.071699 \n",
      " P-Value: 0.319218\n",
      "Epoch [49/100], Loss: 0.0105\n",
      "Correlation: 0.063669 \n",
      " P-Value: 0.376545\n",
      "Epoch [50/100], Loss: 0.0104\n",
      "Correlation: 0.068227 \n",
      " P-Value: 0.343272\n",
      "Epoch [51/100], Loss: 0.0104\n",
      "Correlation: 0.068170 \n",
      " P-Value: 0.343681\n",
      "Epoch [52/100], Loss: 0.0104\n",
      "Correlation: 0.062925 \n",
      " P-Value: 0.382165\n",
      "Epoch [53/100], Loss: 0.0104\n",
      "Correlation: 0.066510 \n",
      " P-Value: 0.355588\n",
      "Epoch [54/100], Loss: 0.0104\n",
      "Correlation: 0.062639 \n",
      " P-Value: 0.384338\n",
      "Epoch [55/100], Loss: 0.0104\n",
      "Correlation: 0.062098 \n",
      " P-Value: 0.388458\n",
      "Epoch [56/100], Loss: 0.0104\n",
      "Correlation: 0.060236 \n",
      " P-Value: 0.402877\n",
      "Epoch [57/100], Loss: 0.0104\n",
      "Correlation: 0.058560 \n",
      " P-Value: 0.416106\n",
      "Epoch [58/100], Loss: 0.0104\n",
      "Correlation: 0.061555 \n",
      " P-Value: 0.392629\n",
      "Epoch [59/100], Loss: 0.0104\n",
      "Correlation: 0.057310 \n",
      " P-Value: 0.426147\n",
      "Epoch [60/100], Loss: 0.0104\n",
      "Correlation: 0.059300 \n",
      " P-Value: 0.410234\n",
      "Epoch [61/100], Loss: 0.0104\n",
      "Correlation: 0.058662 \n",
      " P-Value: 0.415295\n",
      "Epoch [62/100], Loss: 0.0104\n",
      "Correlation: 0.056820 \n",
      " P-Value: 0.430119\n",
      "Epoch [63/100], Loss: 0.0104\n",
      "Correlation: 0.054770 \n",
      " P-Value: 0.446973\n",
      "Epoch [64/100], Loss: 0.0104\n",
      "Correlation: 0.055265 \n",
      " P-Value: 0.442870\n",
      "Epoch [65/100], Loss: 0.0104\n",
      "Correlation: 0.056941 \n",
      " P-Value: 0.429136\n",
      "Epoch [66/100], Loss: 0.0104\n",
      "Correlation: 0.053123 \n",
      " P-Value: 0.460770\n",
      "Epoch [67/100], Loss: 0.0104\n",
      "Correlation: 0.054922 \n",
      " P-Value: 0.445710\n",
      "Epoch [68/100], Loss: 0.0104\n",
      "Correlation: 0.053840 \n",
      " P-Value: 0.454735\n",
      "Epoch [69/100], Loss: 0.0104\n",
      "Correlation: 0.052059 \n",
      " P-Value: 0.469817\n",
      "Epoch [70/100], Loss: 0.0104\n",
      "Correlation: 0.055595 \n",
      " P-Value: 0.440146\n",
      "Epoch [71/100], Loss: 0.0104\n",
      "Correlation: 0.051529 \n",
      " P-Value: 0.474357\n",
      "Epoch [72/100], Loss: 0.0104\n",
      "Correlation: 0.056570 \n",
      " P-Value: 0.432164\n",
      "Epoch [73/100], Loss: 0.0104\n",
      "Correlation: 0.060067 \n",
      " P-Value: 0.404200\n",
      "Epoch [74/100], Loss: 0.0104\n",
      "Correlation: 0.051715 \n",
      " P-Value: 0.472752\n",
      "Epoch [75/100], Loss: 0.0104\n",
      "Correlation: 0.057591 \n",
      " P-Value: 0.423882\n",
      "Epoch [76/100], Loss: 0.0104\n",
      "Correlation: 0.052177 \n",
      " P-Value: 0.468812\n",
      "Epoch [77/100], Loss: 0.0104\n",
      "Correlation: 0.055947 \n",
      " P-Value: 0.437250\n",
      "Epoch [78/100], Loss: 0.0104\n",
      "Correlation: 0.053218 \n",
      " P-Value: 0.459965\n",
      "Epoch [79/100], Loss: 0.0104\n",
      "Correlation: 0.052390 \n",
      " P-Value: 0.466985\n",
      "Epoch [80/100], Loss: 0.0104\n",
      "Correlation: 0.055303 \n",
      " P-Value: 0.442549\n",
      "Epoch [81/100], Loss: 0.0104\n",
      "Correlation: 0.051465 \n",
      " P-Value: 0.474899\n",
      "Epoch [82/100], Loss: 0.0104\n",
      "Correlation: 0.050682 \n",
      " P-Value: 0.481654\n",
      "Epoch [83/100], Loss: 0.0104\n",
      "Correlation: 0.053927 \n",
      " P-Value: 0.454007\n",
      "Epoch [84/100], Loss: 0.0104\n",
      "Correlation: 0.050133 \n",
      " P-Value: 0.486427\n",
      "Epoch [85/100], Loss: 0.0104\n",
      "Correlation: 0.051986 \n",
      " P-Value: 0.470444\n",
      "Epoch [86/100], Loss: 0.0104\n",
      "Correlation: 0.052718 \n",
      " P-Value: 0.464199\n",
      "Epoch [87/100], Loss: 0.0104\n",
      "Correlation: 0.051585 \n",
      " P-Value: 0.473870\n",
      "Epoch [88/100], Loss: 0.0104\n",
      "Correlation: 0.048503 \n",
      " P-Value: 0.500720\n",
      "Epoch [89/100], Loss: 0.0104\n",
      "Correlation: 0.050625 \n",
      " P-Value: 0.482153\n",
      "Epoch [90/100], Loss: 0.0104\n",
      "Correlation: 0.050387 \n",
      " P-Value: 0.484216\n",
      "Epoch [91/100], Loss: 0.0104\n",
      "Correlation: 0.047655 \n",
      " P-Value: 0.508251\n",
      "Epoch [92/100], Loss: 0.0104\n",
      "Correlation: 0.049906 \n",
      " P-Value: 0.488406\n",
      "Epoch [93/100], Loss: 0.0104\n",
      "Correlation: 0.045356 \n",
      " P-Value: 0.528950\n",
      "Epoch [94/100], Loss: 0.0104\n",
      "Correlation: 0.048128 \n",
      " P-Value: 0.504052\n",
      "Epoch [95/100], Loss: 0.0104\n",
      "Correlation: 0.047101 \n",
      " P-Value: 0.513201\n",
      "Epoch [96/100], Loss: 0.0104\n",
      "Correlation: 0.050507 \n",
      " P-Value: 0.483175\n",
      "Epoch [97/100], Loss: 0.0104\n",
      "Correlation: 0.044865 \n",
      " P-Value: 0.533424\n",
      "Epoch [98/100], Loss: 0.0104\n",
      "Correlation: 0.050008 \n",
      " P-Value: 0.487511\n",
      "Epoch [99/100], Loss: 0.0104\n",
      "Correlation: 0.048420 \n",
      " P-Value: 0.501460\n",
      "Epoch [100/100], Loss: 0.0104\n",
      "Max correlation is: [0.14228949] at 2th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 377\n",
    "net_2 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_2.train_net(num_epochs, train_loader_list[2], test_loader_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.170849 \n",
      " P-Value: 0.016939\n",
      "Epoch [1/100], Loss: 0.0693\n",
      "Correlation: 0.119016 \n",
      " P-Value: 0.097482\n",
      "Epoch [2/100], Loss: 0.0404\n",
      "Correlation: -0.086658 \n",
      " P-Value: 0.228356\n",
      "Epoch [3/100], Loss: 0.0327\n",
      "Correlation: -0.068944 \n",
      " P-Value: 0.338213\n",
      "Epoch [4/100], Loss: 0.0315\n",
      "Correlation: -0.036250 \n",
      " P-Value: 0.614894\n",
      "Epoch [5/100], Loss: 0.0306\n",
      "Correlation: 0.071234 \n",
      " P-Value: 0.322375\n",
      "Epoch [6/100], Loss: 0.0292\n",
      "Correlation: 0.040348 \n",
      " P-Value: 0.575460\n",
      "Epoch [7/100], Loss: 0.0285\n",
      "Correlation: -0.059568 \n",
      " P-Value: 0.408116\n",
      "Epoch [8/100], Loss: 0.0277\n",
      "Correlation: -0.070849 \n",
      " P-Value: 0.324999\n",
      "Epoch [9/100], Loss: 0.0272\n",
      "Correlation: 0.011284 \n",
      " P-Value: 0.875566\n",
      "Epoch [10/100], Loss: 0.0268\n",
      "Correlation: -0.005060 \n",
      " P-Value: 0.944001\n",
      "Epoch [11/100], Loss: 0.0266\n",
      "Correlation: -0.103612 \n",
      " P-Value: 0.149459\n",
      "Epoch [12/100], Loss: 0.0265\n",
      "Correlation: -0.099167 \n",
      " P-Value: 0.167803\n",
      "Epoch [13/100], Loss: 0.0264\n",
      "Correlation: -0.060355 \n",
      " P-Value: 0.401945\n",
      "Epoch [14/100], Loss: 0.0264\n",
      "Correlation: -0.083258 \n",
      " P-Value: 0.247203\n",
      "Epoch [15/100], Loss: 0.0263\n",
      "Correlation: -0.078727 \n",
      " P-Value: 0.273956\n",
      "Epoch [16/100], Loss: 0.0263\n",
      "Correlation: -0.054595 \n",
      " P-Value: 0.448423\n",
      "Epoch [17/100], Loss: 0.0262\n",
      "Correlation: -0.054846 \n",
      " P-Value: 0.446336\n",
      "Epoch [18/100], Loss: 0.0262\n",
      "Correlation: -0.065449 \n",
      " P-Value: 0.363325\n",
      "Epoch [19/100], Loss: 0.0262\n",
      "Correlation: -0.061025 \n",
      " P-Value: 0.396725\n",
      "Epoch [20/100], Loss: 0.0261\n",
      "Correlation: -0.052550 \n",
      " P-Value: 0.465627\n",
      "Epoch [21/100], Loss: 0.0261\n",
      "Correlation: -0.049719 \n",
      " P-Value: 0.490036\n",
      "Epoch [22/100], Loss: 0.0261\n",
      "Correlation: -0.056635 \n",
      " P-Value: 0.431623\n",
      "Epoch [23/100], Loss: 0.0261\n",
      "Correlation: -0.055099 \n",
      " P-Value: 0.444246\n",
      "Epoch [24/100], Loss: 0.0261\n",
      "Correlation: -0.054289 \n",
      " P-Value: 0.450975\n",
      "Epoch [25/100], Loss: 0.0261\n",
      "Correlation: -0.051640 \n",
      " P-Value: 0.473405\n",
      "Epoch [26/100], Loss: 0.0260\n",
      "Correlation: -0.053814 \n",
      " P-Value: 0.454953\n",
      "Epoch [27/100], Loss: 0.0260\n",
      "Correlation: -0.056332 \n",
      " P-Value: 0.434099\n",
      "Epoch [28/100], Loss: 0.0260\n",
      "Correlation: -0.058103 \n",
      " P-Value: 0.419765\n",
      "Epoch [29/100], Loss: 0.0260\n",
      "Correlation: -0.056379 \n",
      " P-Value: 0.433719\n",
      "Epoch [30/100], Loss: 0.0260\n",
      "Correlation: -0.056364 \n",
      " P-Value: 0.433835\n",
      "Epoch [31/100], Loss: 0.0260\n",
      "Correlation: -0.057437 \n",
      " P-Value: 0.425119\n",
      "Epoch [32/100], Loss: 0.0260\n",
      "Correlation: -0.054398 \n",
      " P-Value: 0.450064\n",
      "Epoch [33/100], Loss: 0.0260\n",
      "Correlation: -0.054199 \n",
      " P-Value: 0.451727\n",
      "Epoch [34/100], Loss: 0.0260\n",
      "Correlation: -0.054383 \n",
      " P-Value: 0.450196\n",
      "Epoch [35/100], Loss: 0.0260\n",
      "Correlation: -0.056288 \n",
      " P-Value: 0.434458\n",
      "Epoch [36/100], Loss: 0.0260\n",
      "Correlation: -0.055046 \n",
      " P-Value: 0.444681\n",
      "Epoch [37/100], Loss: 0.0260\n",
      "Correlation: -0.056806 \n",
      " P-Value: 0.430234\n",
      "Epoch [38/100], Loss: 0.0259\n",
      "Correlation: -0.058053 \n",
      " P-Value: 0.420169\n",
      "Epoch [39/100], Loss: 0.0259\n",
      "Correlation: -0.061726 \n",
      " P-Value: 0.391316\n",
      "Epoch [40/100], Loss: 0.0259\n",
      "Correlation: -0.060130 \n",
      " P-Value: 0.403702\n",
      "Epoch [41/100], Loss: 0.0259\n",
      "Correlation: -0.059876 \n",
      " P-Value: 0.405697\n",
      "Epoch [42/100], Loss: 0.0259\n",
      "Correlation: -0.062849 \n",
      " P-Value: 0.382736\n",
      "Epoch [43/100], Loss: 0.0259\n",
      "Correlation: -0.062180 \n",
      " P-Value: 0.387830\n",
      "Epoch [44/100], Loss: 0.0259\n",
      "Correlation: -0.060327 \n",
      " P-Value: 0.402161\n",
      "Epoch [45/100], Loss: 0.0259\n",
      "Correlation: -0.057835 \n",
      " P-Value: 0.421920\n",
      "Epoch [46/100], Loss: 0.0259\n",
      "Correlation: -0.058535 \n",
      " P-Value: 0.416317\n",
      "Epoch [47/100], Loss: 0.0259\n",
      "Correlation: -0.055217 \n",
      " P-Value: 0.443267\n",
      "Epoch [48/100], Loss: 0.0259\n",
      "Correlation: -0.058173 \n",
      " P-Value: 0.419203\n",
      "Epoch [49/100], Loss: 0.0259\n",
      "Correlation: -0.057581 \n",
      " P-Value: 0.423961\n",
      "Epoch [50/100], Loss: 0.0259\n",
      "Correlation: -0.056664 \n",
      " P-Value: 0.431388\n",
      "Epoch [51/100], Loss: 0.0259\n",
      "Correlation: -0.053629 \n",
      " P-Value: 0.456515\n",
      "Epoch [52/100], Loss: 0.0259\n",
      "Correlation: -0.055559 \n",
      " P-Value: 0.440442\n",
      "Epoch [53/100], Loss: 0.0259\n",
      "Correlation: -0.052999 \n",
      " P-Value: 0.461824\n",
      "Epoch [54/100], Loss: 0.0259\n",
      "Correlation: -0.053945 \n",
      " P-Value: 0.453849\n",
      "Epoch [55/100], Loss: 0.0259\n",
      "Correlation: -0.049335 \n",
      " P-Value: 0.493397\n",
      "Epoch [56/100], Loss: 0.0259\n",
      "Correlation: -0.050483 \n",
      " P-Value: 0.483385\n",
      "Epoch [57/100], Loss: 0.0259\n",
      "Correlation: -0.049873 \n",
      " P-Value: 0.488692\n",
      "Epoch [58/100], Loss: 0.0259\n",
      "Correlation: -0.049516 \n",
      " P-Value: 0.491812\n",
      "Epoch [59/100], Loss: 0.0259\n",
      "Correlation: -0.045297 \n",
      " P-Value: 0.529489\n",
      "Epoch [60/100], Loss: 0.0259\n",
      "Correlation: -0.048301 \n",
      " P-Value: 0.502518\n",
      "Epoch [61/100], Loss: 0.0259\n",
      "Correlation: -0.046731 \n",
      " P-Value: 0.516523\n",
      "Epoch [62/100], Loss: 0.0259\n",
      "Correlation: -0.044649 \n",
      " P-Value: 0.535402\n",
      "Epoch [63/100], Loss: 0.0259\n",
      "Correlation: -0.044997 \n",
      " P-Value: 0.532213\n",
      "Epoch [64/100], Loss: 0.0259\n",
      "Correlation: -0.047652 \n",
      " P-Value: 0.508279\n",
      "Epoch [65/100], Loss: 0.0259\n",
      "Correlation: -0.043173 \n",
      " P-Value: 0.548993\n",
      "Epoch [66/100], Loss: 0.0259\n",
      "Correlation: -0.046331 \n",
      " P-Value: 0.520120\n",
      "Epoch [67/100], Loss: 0.0259\n",
      "Correlation: -0.044786 \n",
      " P-Value: 0.534146\n",
      "Epoch [68/100], Loss: 0.0259\n",
      "Correlation: -0.042799 \n",
      " P-Value: 0.552447\n",
      "Epoch [69/100], Loss: 0.0259\n",
      "Correlation: -0.044159 \n",
      " P-Value: 0.539888\n",
      "Epoch [70/100], Loss: 0.0259\n",
      "Correlation: -0.043819 \n",
      " P-Value: 0.543017\n",
      "Epoch [71/100], Loss: 0.0259\n",
      "Correlation: -0.042962 \n",
      " P-Value: 0.550943\n",
      "Epoch [72/100], Loss: 0.0259\n",
      "Correlation: -0.044883 \n",
      " P-Value: 0.533254\n",
      "Epoch [73/100], Loss: 0.0259\n",
      "Correlation: -0.041442 \n",
      " P-Value: 0.565140\n",
      "Epoch [74/100], Loss: 0.0259\n",
      "Correlation: -0.040585 \n",
      " P-Value: 0.573215\n",
      "Epoch [75/100], Loss: 0.0259\n",
      "Correlation: -0.043829 \n",
      " P-Value: 0.542917\n",
      "Epoch [76/100], Loss: 0.0259\n",
      "Correlation: -0.043395 \n",
      " P-Value: 0.546934\n",
      "Epoch [77/100], Loss: 0.0259\n",
      "Correlation: -0.041133 \n",
      " P-Value: 0.568039\n",
      "Epoch [78/100], Loss: 0.0259\n",
      "Correlation: -0.039708 \n",
      " P-Value: 0.581528\n",
      "Epoch [79/100], Loss: 0.0259\n",
      "Correlation: -0.041049 \n",
      " P-Value: 0.568831\n",
      "Epoch [80/100], Loss: 0.0259\n",
      "Correlation: -0.040684 \n",
      " P-Value: 0.572279\n",
      "Epoch [81/100], Loss: 0.0259\n",
      "Correlation: -0.038978 \n",
      " P-Value: 0.588500\n",
      "Epoch [82/100], Loss: 0.0259\n",
      "Correlation: -0.039073 \n",
      " P-Value: 0.587594\n",
      "Epoch [83/100], Loss: 0.0259\n",
      "Correlation: -0.043307 \n",
      " P-Value: 0.547742\n",
      "Epoch [84/100], Loss: 0.0259\n",
      "Correlation: -0.035746 \n",
      " P-Value: 0.619818\n",
      "Epoch [85/100], Loss: 0.0259\n",
      "Correlation: -0.034072 \n",
      " P-Value: 0.636305\n",
      "Epoch [86/100], Loss: 0.0259\n",
      "Correlation: -0.038308 \n",
      " P-Value: 0.594932\n",
      "Epoch [87/100], Loss: 0.0258\n",
      "Correlation: -0.039520 \n",
      " P-Value: 0.583334\n",
      "Epoch [88/100], Loss: 0.0258\n",
      "Correlation: -0.036360 \n",
      " P-Value: 0.613813\n",
      "Epoch [89/100], Loss: 0.0258\n",
      "Correlation: -0.037158 \n",
      " P-Value: 0.606051\n",
      "Epoch [90/100], Loss: 0.0258\n",
      "Correlation: -0.037990 \n",
      " P-Value: 0.598003\n",
      "Epoch [91/100], Loss: 0.0258\n",
      "Correlation: -0.037240 \n",
      " P-Value: 0.605258\n",
      "Epoch [92/100], Loss: 0.0258\n",
      "Correlation: -0.034561 \n",
      " P-Value: 0.631468\n",
      "Epoch [93/100], Loss: 0.0258\n",
      "Correlation: -0.035143 \n",
      " P-Value: 0.625737\n",
      "Epoch [94/100], Loss: 0.0258\n",
      "Correlation: -0.038236 \n",
      " P-Value: 0.595629\n",
      "Epoch [95/100], Loss: 0.0258\n",
      "Correlation: -0.034044 \n",
      " P-Value: 0.636591\n",
      "Epoch [96/100], Loss: 0.0258\n",
      "Correlation: -0.033528 \n",
      " P-Value: 0.641704\n",
      "Epoch [97/100], Loss: 0.0258\n",
      "Correlation: -0.036399 \n",
      " P-Value: 0.613429\n",
      "Epoch [98/100], Loss: 0.0258\n",
      "Correlation: -0.033765 \n",
      " P-Value: 0.639346\n",
      "Epoch [99/100], Loss: 0.0258\n",
      "Correlation: -0.033331 \n",
      " P-Value: 0.643673\n",
      "Epoch [100/100], Loss: 0.0258\n",
      "Max correlation is: [0.17084913] at 0th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 369\n",
    "net_3 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_3.train_net(num_epochs, train_loader_list[3], test_loader_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.150088 \n",
      " P-Value: 0.036236\n",
      "Epoch [1/100], Loss: 0.6821\n",
      "Correlation: 0.135224 \n",
      " P-Value: 0.059453\n",
      "Epoch [2/100], Loss: 0.3375\n",
      "Correlation: 0.108547 \n",
      " P-Value: 0.130914\n",
      "Epoch [3/100], Loss: 0.2552\n",
      "Correlation: 0.030267 \n",
      " P-Value: 0.674455\n",
      "Epoch [4/100], Loss: 0.1765\n",
      "Correlation: -0.021643 \n",
      " P-Value: 0.763930\n",
      "Epoch [5/100], Loss: 0.1545\n",
      "Correlation: -0.049026 \n",
      " P-Value: 0.496117\n",
      "Epoch [6/100], Loss: 0.1521\n",
      "Correlation: -0.000249 \n",
      " P-Value: 0.997297\n",
      "Epoch [7/100], Loss: 0.1380\n",
      "Correlation: 0.008499 \n",
      " P-Value: 0.906125\n",
      "Epoch [8/100], Loss: 0.1278\n",
      "Correlation: 0.016180 \n",
      " P-Value: 0.822369\n",
      "Epoch [9/100], Loss: 0.1218\n",
      "Correlation: -0.031869 \n",
      " P-Value: 0.658284\n",
      "Epoch [10/100], Loss: 0.1142\n",
      "Correlation: -0.056199 \n",
      " P-Value: 0.435190\n",
      "Epoch [11/100], Loss: 0.1091\n",
      "Correlation: -0.062883 \n",
      " P-Value: 0.382488\n",
      "Epoch [12/100], Loss: 0.1053\n",
      "Correlation: -0.059144 \n",
      " P-Value: 0.411473\n",
      "Epoch [13/100], Loss: 0.1029\n",
      "Correlation: -0.052161 \n",
      " P-Value: 0.468948\n",
      "Epoch [14/100], Loss: 0.1017\n",
      "Correlation: -0.060362 \n",
      " P-Value: 0.401887\n",
      "Epoch [15/100], Loss: 0.1006\n",
      "Correlation: -0.057965 \n",
      " P-Value: 0.420873\n",
      "Epoch [16/100], Loss: 0.0997\n",
      "Correlation: -0.050303 \n",
      " P-Value: 0.484946\n",
      "Epoch [17/100], Loss: 0.0990\n",
      "Correlation: -0.044599 \n",
      " P-Value: 0.535860\n",
      "Epoch [18/100], Loss: 0.0982\n",
      "Correlation: -0.035399 \n",
      " P-Value: 0.623212\n",
      "Epoch [19/100], Loss: 0.0975\n",
      "Correlation: -0.029222 \n",
      " P-Value: 0.685096\n",
      "Epoch [20/100], Loss: 0.0969\n",
      "Correlation: -0.032856 \n",
      " P-Value: 0.648405\n",
      "Epoch [21/100], Loss: 0.0963\n",
      "Correlation: -0.035138 \n",
      " P-Value: 0.625787\n",
      "Epoch [22/100], Loss: 0.0958\n",
      "Correlation: -0.038542 \n",
      " P-Value: 0.592688\n",
      "Epoch [23/100], Loss: 0.0953\n",
      "Correlation: -0.045719 \n",
      " P-Value: 0.525652\n",
      "Epoch [24/100], Loss: 0.0949\n",
      "Correlation: -0.049894 \n",
      " P-Value: 0.488515\n",
      "Epoch [25/100], Loss: 0.0945\n",
      "Correlation: -0.051281 \n",
      " P-Value: 0.476485\n",
      "Epoch [26/100], Loss: 0.0942\n",
      "Correlation: -0.050676 \n",
      " P-Value: 0.481715\n",
      "Epoch [27/100], Loss: 0.0939\n",
      "Correlation: -0.047424 \n",
      " P-Value: 0.510309\n",
      "Epoch [28/100], Loss: 0.0937\n",
      "Correlation: -0.043947 \n",
      " P-Value: 0.541835\n",
      "Epoch [29/100], Loss: 0.0935\n",
      "Correlation: -0.040855 \n",
      " P-Value: 0.570657\n",
      "Epoch [30/100], Loss: 0.0933\n",
      "Correlation: -0.037753 \n",
      " P-Value: 0.600281\n",
      "Epoch [31/100], Loss: 0.0931\n",
      "Correlation: -0.037891 \n",
      " P-Value: 0.598950\n",
      "Epoch [32/100], Loss: 0.0929\n",
      "Correlation: -0.036480 \n",
      " P-Value: 0.612639\n",
      "Epoch [33/100], Loss: 0.0928\n",
      "Correlation: -0.036718 \n",
      " P-Value: 0.610324\n",
      "Epoch [34/100], Loss: 0.0926\n",
      "Correlation: -0.037723 \n",
      " P-Value: 0.600571\n",
      "Epoch [35/100], Loss: 0.0925\n",
      "Correlation: -0.036881 \n",
      " P-Value: 0.608732\n",
      "Epoch [36/100], Loss: 0.0923\n",
      "Correlation: -0.037242 \n",
      " P-Value: 0.605235\n",
      "Epoch [37/100], Loss: 0.0922\n",
      "Correlation: -0.036366 \n",
      " P-Value: 0.613749\n",
      "Epoch [38/100], Loss: 0.0921\n",
      "Correlation: -0.036024 \n",
      " P-Value: 0.617086\n",
      "Epoch [39/100], Loss: 0.0920\n",
      "Correlation: -0.035377 \n",
      " P-Value: 0.623427\n",
      "Epoch [40/100], Loss: 0.0919\n",
      "Correlation: -0.036957 \n",
      " P-Value: 0.608003\n",
      "Epoch [41/100], Loss: 0.0918\n",
      "Correlation: -0.035769 \n",
      " P-Value: 0.619589\n",
      "Epoch [42/100], Loss: 0.0918\n",
      "Correlation: -0.033236 \n",
      " P-Value: 0.644601\n",
      "Epoch [43/100], Loss: 0.0917\n",
      "Correlation: -0.033149 \n",
      " P-Value: 0.645477\n",
      "Epoch [44/100], Loss: 0.0916\n",
      "Correlation: -0.029370 \n",
      " P-Value: 0.683581\n",
      "Epoch [45/100], Loss: 0.0916\n",
      "Correlation: -0.029610 \n",
      " P-Value: 0.681141\n",
      "Epoch [46/100], Loss: 0.0915\n",
      "Correlation: -0.028351 \n",
      " P-Value: 0.694012\n",
      "Epoch [47/100], Loss: 0.0914\n",
      "Correlation: -0.027274 \n",
      " P-Value: 0.705073\n",
      "Epoch [48/100], Loss: 0.0914\n",
      "Correlation: -0.026785 \n",
      " P-Value: 0.710116\n",
      "Epoch [49/100], Loss: 0.0913\n",
      "Correlation: -0.025972 \n",
      " P-Value: 0.718554\n",
      "Epoch [50/100], Loss: 0.0913\n",
      "Correlation: -0.024782 \n",
      " P-Value: 0.730925\n",
      "Epoch [51/100], Loss: 0.0913\n",
      "Correlation: -0.024132 \n",
      " P-Value: 0.737729\n",
      "Epoch [52/100], Loss: 0.0912\n",
      "Correlation: -0.022108 \n",
      " P-Value: 0.759017\n",
      "Epoch [53/100], Loss: 0.0912\n",
      "Correlation: -0.022138 \n",
      " P-Value: 0.758704\n",
      "Epoch [54/100], Loss: 0.0911\n",
      "Correlation: -0.022033 \n",
      " P-Value: 0.759816\n",
      "Epoch [55/100], Loss: 0.0911\n",
      "Correlation: -0.019886 \n",
      " P-Value: 0.782592\n",
      "Epoch [56/100], Loss: 0.0911\n",
      "Correlation: -0.022483 \n",
      " P-Value: 0.755051\n",
      "Epoch [57/100], Loss: 0.0910\n",
      "Correlation: -0.017014 \n",
      " P-Value: 0.813384\n",
      "Epoch [58/100], Loss: 0.0910\n",
      "Correlation: -0.020033 \n",
      " P-Value: 0.781032\n",
      "Epoch [59/100], Loss: 0.0910\n",
      "Correlation: -0.016569 \n",
      " P-Value: 0.818185\n",
      "Epoch [60/100], Loss: 0.0910\n",
      "Correlation: -0.016442 \n",
      " P-Value: 0.819549\n",
      "Epoch [61/100], Loss: 0.0910\n",
      "Correlation: -0.016183 \n",
      " P-Value: 0.822309\n",
      "Epoch [62/100], Loss: 0.0909\n",
      "Correlation: -0.016535 \n",
      " P-Value: 0.818535\n",
      "Epoch [63/100], Loss: 0.0909\n",
      "Correlation: -0.015385 \n",
      " P-Value: 0.830983\n",
      "Epoch [64/100], Loss: 0.0909\n",
      "Correlation: -0.015395 \n",
      " P-Value: 0.830858\n",
      "Epoch [65/100], Loss: 0.0909\n",
      "Correlation: -0.016392 \n",
      " P-Value: 0.820097\n",
      "Epoch [66/100], Loss: 0.0909\n",
      "Correlation: -0.014995 \n",
      " P-Value: 0.835190\n",
      "Epoch [67/100], Loss: 0.0909\n",
      "Correlation: -0.014909 \n",
      " P-Value: 0.836119\n",
      "Epoch [68/100], Loss: 0.0908\n",
      "Correlation: -0.014739 \n",
      " P-Value: 0.837972\n",
      "Epoch [69/100], Loss: 0.0908\n",
      "Correlation: -0.013618 \n",
      " P-Value: 0.850139\n",
      "Epoch [70/100], Loss: 0.0908\n",
      "Correlation: -0.012393 \n",
      " P-Value: 0.863468\n",
      "Epoch [71/100], Loss: 0.0908\n",
      "Correlation: -0.012507 \n",
      " P-Value: 0.862215\n",
      "Epoch [72/100], Loss: 0.0908\n",
      "Correlation: -0.011225 \n",
      " P-Value: 0.876232\n",
      "Epoch [73/100], Loss: 0.0908\n",
      "Correlation: -0.010445 \n",
      " P-Value: 0.884783\n",
      "Epoch [74/100], Loss: 0.0908\n",
      "Correlation: -0.013144 \n",
      " P-Value: 0.855277\n",
      "Epoch [75/100], Loss: 0.0908\n",
      "Correlation: -0.012209 \n",
      " P-Value: 0.865477\n",
      "Epoch [76/100], Loss: 0.0907\n",
      "Correlation: -0.014308 \n",
      " P-Value: 0.842625\n",
      "Epoch [77/100], Loss: 0.0907\n",
      "Correlation: -0.016250 \n",
      " P-Value: 0.821615\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Correlation: -0.017650 \n",
      " P-Value: 0.806540\n",
      "Epoch [79/100], Loss: 0.0907\n",
      "Correlation: -0.019418 \n",
      " P-Value: 0.787591\n",
      "Epoch [80/100], Loss: 0.0907\n",
      "Correlation: -0.018588 \n",
      " P-Value: 0.796464\n",
      "Epoch [81/100], Loss: 0.0908\n",
      "Correlation: -0.017641 \n",
      " P-Value: 0.806613\n",
      "Epoch [82/100], Loss: 0.0908\n",
      "Correlation: -0.013611 \n",
      " P-Value: 0.850187\n",
      "Epoch [83/100], Loss: 0.0908\n",
      "Correlation: -0.008343 \n",
      " P-Value: 0.907876\n",
      "Epoch [84/100], Loss: 0.0909\n",
      "Correlation: -0.004258 \n",
      " P-Value: 0.952904\n",
      "Epoch [85/100], Loss: 0.0909\n",
      "Correlation: 0.003179 \n",
      " P-Value: 0.964773\n",
      "Epoch [86/100], Loss: 0.0909\n",
      "Correlation: 0.004256 \n",
      " P-Value: 0.952904\n",
      "Epoch [87/100], Loss: 0.0909\n",
      "Correlation: 0.002820 \n",
      " P-Value: 0.968722\n",
      "Epoch [88/100], Loss: 0.0909\n",
      "Correlation: -0.011219 \n",
      " P-Value: 0.876319\n",
      "Epoch [89/100], Loss: 0.0909\n",
      "Correlation: -0.022173 \n",
      " P-Value: 0.758334\n",
      "Epoch [90/100], Loss: 0.0909\n",
      "Correlation: -0.041018 \n",
      " P-Value: 0.569119\n",
      "Epoch [91/100], Loss: 0.0908\n",
      "Correlation: -0.050838 \n",
      " P-Value: 0.480312\n",
      "Epoch [92/100], Loss: 0.0909\n",
      "Correlation: -0.041431 \n",
      " P-Value: 0.565241\n",
      "Epoch [93/100], Loss: 0.0912\n",
      "Correlation: -0.017495 \n",
      " P-Value: 0.808181\n",
      "Epoch [94/100], Loss: 0.0913\n",
      "Correlation: -0.002659 \n",
      " P-Value: 0.970648\n",
      "Epoch [95/100], Loss: 0.0910\n",
      "Correlation: -0.005768 \n",
      " P-Value: 0.936225\n",
      "Epoch [96/100], Loss: 0.0909\n",
      "Correlation: -0.016727 \n",
      " P-Value: 0.816467\n",
      "Epoch [97/100], Loss: 0.0908\n",
      "Correlation: -0.029560 \n",
      " P-Value: 0.681644\n",
      "Epoch [98/100], Loss: 0.0907\n",
      "Correlation: -0.036841 \n",
      " P-Value: 0.609118\n",
      "Epoch [99/100], Loss: 0.0906\n",
      "Correlation: -0.032856 \n",
      " P-Value: 0.648405\n",
      "Epoch [100/100], Loss: 0.0907\n",
      "Max correlation is: [0.1500877] at 0th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 370\n",
    "net_4 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_4.train_net(num_epochs, train_loader_list[4], test_loader_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.144569 \n",
      " P-Value: 0.043756\n",
      "Epoch [1/100], Loss: 0.5155\n",
      "Correlation: 0.138178 \n",
      " P-Value: 0.054055\n",
      "Epoch [2/100], Loss: 0.2062\n",
      "Correlation: 0.131143 \n",
      " P-Value: 0.067634\n",
      "Epoch [3/100], Loss: 0.1535\n",
      "Correlation: 0.079983 \n",
      " P-Value: 0.266353\n",
      "Epoch [4/100], Loss: 0.0930\n",
      "Correlation: 0.061468 \n",
      " P-Value: 0.393301\n",
      "Epoch [5/100], Loss: 0.0769\n",
      "Correlation: 0.025833 \n",
      " P-Value: 0.719987\n",
      "Epoch [6/100], Loss: 0.0712\n",
      "Correlation: 0.018111 \n",
      " P-Value: 0.801583\n",
      "Epoch [7/100], Loss: 0.0688\n",
      "Correlation: 0.076749 \n",
      " P-Value: 0.286231\n",
      "Epoch [8/100], Loss: 0.0621\n",
      "Correlation: 0.079397 \n",
      " P-Value: 0.269879\n",
      "Epoch [9/100], Loss: 0.0571\n",
      "Correlation: 0.088932 \n",
      " P-Value: 0.216336\n",
      "Epoch [10/100], Loss: 0.0535\n",
      "Correlation: 0.060723 \n",
      " P-Value: 0.399075\n",
      "Epoch [11/100], Loss: 0.0492\n",
      "Correlation: 0.048236 \n",
      " P-Value: 0.503087\n",
      "Epoch [12/100], Loss: 0.0463\n",
      "Correlation: 0.037398 \n",
      " P-Value: 0.603718\n",
      "Epoch [13/100], Loss: 0.0441\n",
      "Correlation: 0.024118 \n",
      " P-Value: 0.737884\n",
      "Epoch [14/100], Loss: 0.0422\n",
      "Correlation: -0.000609 \n",
      " P-Value: 0.992849\n",
      "Epoch [15/100], Loss: 0.0412\n",
      "Correlation: -0.043361 \n",
      " P-Value: 0.547239\n",
      "Epoch [16/100], Loss: 0.0405\n",
      "Correlation: -0.069821 \n",
      " P-Value: 0.332088\n",
      "Epoch [17/100], Loss: 0.0399\n",
      "Correlation: -0.086235 \n",
      " P-Value: 0.230644\n",
      "Epoch [18/100], Loss: 0.0394\n",
      "Correlation: -0.082362 \n",
      " P-Value: 0.252340\n",
      "Epoch [19/100], Loss: 0.0389\n",
      "Correlation: -0.071872 \n",
      " P-Value: 0.318048\n",
      "Epoch [20/100], Loss: 0.0385\n",
      "Correlation: -0.069004 \n",
      " P-Value: 0.337794\n",
      "Epoch [21/100], Loss: 0.0380\n",
      "Correlation: -0.064648 \n",
      " P-Value: 0.369238\n",
      "Epoch [22/100], Loss: 0.0377\n",
      "Correlation: -0.061809 \n",
      " P-Value: 0.390678\n",
      "Epoch [23/100], Loss: 0.0374\n",
      "Correlation: -0.067801 \n",
      " P-Value: 0.346303\n",
      "Epoch [24/100], Loss: 0.0371\n",
      "Correlation: -0.075109 \n",
      " P-Value: 0.296687\n",
      "Epoch [25/100], Loss: 0.0369\n",
      "Correlation: -0.080049 \n",
      " P-Value: 0.265960\n",
      "Epoch [26/100], Loss: 0.0366\n",
      "Correlation: -0.082730 \n",
      " P-Value: 0.250223\n",
      "Epoch [27/100], Loss: 0.0364\n",
      "Correlation: -0.083174 \n",
      " P-Value: 0.247682\n",
      "Epoch [28/100], Loss: 0.0362\n",
      "Correlation: -0.081298 \n",
      " P-Value: 0.258546\n",
      "Epoch [29/100], Loss: 0.0361\n",
      "Correlation: -0.079272 \n",
      " P-Value: 0.270639\n",
      "Epoch [30/100], Loss: 0.0359\n",
      "Correlation: -0.077818 \n",
      " P-Value: 0.279553\n",
      "Epoch [31/100], Loss: 0.0357\n",
      "Correlation: -0.078662 \n",
      " P-Value: 0.274357\n",
      "Epoch [32/100], Loss: 0.0356\n",
      "Correlation: -0.078294 \n",
      " P-Value: 0.276615\n",
      "Epoch [33/100], Loss: 0.0355\n",
      "Correlation: -0.076949 \n",
      " P-Value: 0.284973\n",
      "Epoch [34/100], Loss: 0.0353\n",
      "Correlation: -0.076230 \n",
      " P-Value: 0.289516\n",
      "Epoch [35/100], Loss: 0.0352\n",
      "Correlation: -0.076315 \n",
      " P-Value: 0.288976\n",
      "Epoch [36/100], Loss: 0.0351\n",
      "Correlation: -0.077668 \n",
      " P-Value: 0.280485\n",
      "Epoch [37/100], Loss: 0.0350\n",
      "Correlation: -0.079144 \n",
      " P-Value: 0.271416\n",
      "Epoch [38/100], Loss: 0.0349\n",
      "Correlation: -0.079295 \n",
      " P-Value: 0.270500\n",
      "Epoch [39/100], Loss: 0.0348\n",
      "Correlation: -0.078089 \n",
      " P-Value: 0.277876\n",
      "Epoch [40/100], Loss: 0.0347\n",
      "Correlation: -0.077594 \n",
      " P-Value: 0.280946\n",
      "Epoch [41/100], Loss: 0.0346\n",
      "Correlation: -0.077259 \n",
      " P-Value: 0.283031\n",
      "Epoch [42/100], Loss: 0.0345\n",
      "Correlation: -0.075462 \n",
      " P-Value: 0.294410\n",
      "Epoch [43/100], Loss: 0.0345\n",
      "Correlation: -0.074012 \n",
      " P-Value: 0.303814\n",
      "Epoch [44/100], Loss: 0.0344\n",
      "Correlation: -0.074299 \n",
      " P-Value: 0.301935\n",
      "Epoch [45/100], Loss: 0.0343\n",
      "Correlation: -0.073912 \n",
      " P-Value: 0.304469\n",
      "Epoch [46/100], Loss: 0.0343\n",
      "Correlation: -0.072708 \n",
      " P-Value: 0.312437\n",
      "Epoch [47/100], Loss: 0.0342\n",
      "Correlation: -0.071083 \n",
      " P-Value: 0.323403\n",
      "Epoch [48/100], Loss: 0.0342\n",
      "Correlation: -0.070129 \n",
      " P-Value: 0.329958\n",
      "Epoch [49/100], Loss: 0.0341\n",
      "Correlation: -0.068060 \n",
      " P-Value: 0.344461\n",
      "Epoch [50/100], Loss: 0.0341\n",
      "Correlation: -0.065473 \n",
      " P-Value: 0.363147\n",
      "Epoch [51/100], Loss: 0.0340\n",
      "Correlation: -0.064226 \n",
      " P-Value: 0.372383\n",
      "Epoch [52/100], Loss: 0.0340\n",
      "Correlation: -0.063186 \n",
      " P-Value: 0.380183\n",
      "Epoch [53/100], Loss: 0.0339\n",
      "Correlation: -0.061077 \n",
      " P-Value: 0.396328\n",
      "Epoch [54/100], Loss: 0.0339\n",
      "Correlation: -0.059841 \n",
      " P-Value: 0.405971\n",
      "Epoch [55/100], Loss: 0.0338\n",
      "Correlation: -0.059890 \n",
      " P-Value: 0.405584\n",
      "Epoch [56/100], Loss: 0.0338\n",
      "Correlation: -0.059237 \n",
      " P-Value: 0.410728\n",
      "Epoch [57/100], Loss: 0.0338\n",
      "Correlation: -0.058412 \n",
      " P-Value: 0.417289\n",
      "Epoch [58/100], Loss: 0.0337\n",
      "Correlation: -0.058713 \n",
      " P-Value: 0.414887\n",
      "Epoch [59/100], Loss: 0.0337\n",
      "Correlation: -0.058328 \n",
      " P-Value: 0.417958\n",
      "Epoch [60/100], Loss: 0.0337\n",
      "Correlation: -0.057460 \n",
      " P-Value: 0.424939\n",
      "Epoch [61/100], Loss: 0.0336\n",
      "Correlation: -0.056503 \n",
      " P-Value: 0.432701\n",
      "Epoch [62/100], Loss: 0.0336\n",
      "Correlation: -0.056012 \n",
      " P-Value: 0.436722\n",
      "Epoch [63/100], Loss: 0.0336\n",
      "Correlation: -0.056173 \n",
      " P-Value: 0.435394\n",
      "Epoch [64/100], Loss: 0.0336\n",
      "Correlation: -0.055641 \n",
      " P-Value: 0.439766\n",
      "Epoch [65/100], Loss: 0.0335\n",
      "Correlation: -0.055769 \n",
      " P-Value: 0.438710\n",
      "Epoch [66/100], Loss: 0.0335\n",
      "Correlation: -0.054224 \n",
      " P-Value: 0.451521\n",
      "Epoch [67/100], Loss: 0.0335\n",
      "Correlation: -0.052446 \n",
      " P-Value: 0.466511\n",
      "Epoch [68/100], Loss: 0.0335\n",
      "Correlation: -0.053593 \n",
      " P-Value: 0.456809\n",
      "Epoch [69/100], Loss: 0.0335\n",
      "Correlation: -0.053087 \n",
      " P-Value: 0.461073\n",
      "Epoch [70/100], Loss: 0.0334\n",
      "Correlation: -0.050229 \n",
      " P-Value: 0.485585\n",
      "Epoch [71/100], Loss: 0.0334\n",
      "Correlation: -0.051392 \n",
      " P-Value: 0.475531\n",
      "Epoch [72/100], Loss: 0.0334\n",
      "Correlation: -0.052609 \n",
      " P-Value: 0.465131\n",
      "Epoch [73/100], Loss: 0.0334\n",
      "Correlation: -0.049459 \n",
      " P-Value: 0.492313\n",
      "Epoch [74/100], Loss: 0.0334\n",
      "Correlation: -0.049030 \n",
      " P-Value: 0.496080\n",
      "Epoch [75/100], Loss: 0.0334\n",
      "Correlation: -0.051080 \n",
      " P-Value: 0.478211\n",
      "Epoch [76/100], Loss: 0.0333\n",
      "Correlation: -0.050586 \n",
      " P-Value: 0.482490\n",
      "Epoch [77/100], Loss: 0.0333\n",
      "Correlation: -0.050524 \n",
      " P-Value: 0.483026\n",
      "Epoch [78/100], Loss: 0.0333\n",
      "Correlation: -0.049827 \n",
      " P-Value: 0.489099\n",
      "Epoch [79/100], Loss: 0.0333\n",
      "Correlation: -0.050040 \n",
      " P-Value: 0.487236\n",
      "Epoch [80/100], Loss: 0.0333\n",
      "Correlation: -0.050718 \n",
      " P-Value: 0.481343\n",
      "Epoch [81/100], Loss: 0.0333\n",
      "Correlation: -0.050169 \n",
      " P-Value: 0.486107\n",
      "Epoch [82/100], Loss: 0.0333\n",
      "Correlation: -0.048173 \n",
      " P-Value: 0.503646\n",
      "Epoch [83/100], Loss: 0.0333\n",
      "Correlation: -0.049012 \n",
      " P-Value: 0.496235\n",
      "Epoch [84/100], Loss: 0.0333\n",
      "Correlation: -0.049641 \n",
      " P-Value: 0.490718\n",
      "Epoch [85/100], Loss: 0.0332\n",
      "Correlation: -0.048218 \n",
      " P-Value: 0.503246\n",
      "Epoch [86/100], Loss: 0.0332\n",
      "Correlation: -0.047719 \n",
      " P-Value: 0.507678\n",
      "Epoch [87/100], Loss: 0.0332\n",
      "Correlation: -0.048171 \n",
      " P-Value: 0.503668\n",
      "Epoch [88/100], Loss: 0.0332\n",
      "Correlation: -0.048161 \n",
      " P-Value: 0.503756\n",
      "Epoch [89/100], Loss: 0.0332\n",
      "Correlation: -0.047964 \n",
      " P-Value: 0.505506\n",
      "Epoch [90/100], Loss: 0.0332\n",
      "Correlation: -0.046844 \n",
      " P-Value: 0.515499\n",
      "Epoch [91/100], Loss: 0.0332\n",
      "Correlation: -0.045724 \n",
      " P-Value: 0.525611\n",
      "Epoch [92/100], Loss: 0.0332\n",
      "Correlation: -0.048099 \n",
      " P-Value: 0.504305\n",
      "Epoch [93/100], Loss: 0.0332\n",
      "Correlation: -0.047301 \n",
      " P-Value: 0.511411\n",
      "Epoch [94/100], Loss: 0.0332\n",
      "Correlation: -0.045731 \n",
      " P-Value: 0.525540\n",
      "Epoch [95/100], Loss: 0.0332\n",
      "Correlation: -0.045696 \n",
      " P-Value: 0.525859\n",
      "Epoch [96/100], Loss: 0.0332\n",
      "Correlation: -0.045188 \n",
      " P-Value: 0.530478\n",
      "Epoch [97/100], Loss: 0.0331\n",
      "Correlation: -0.045750 \n",
      " P-Value: 0.525375\n",
      "Epoch [98/100], Loss: 0.0331\n",
      "Correlation: -0.045458 \n",
      " P-Value: 0.528020\n",
      "Epoch [99/100], Loss: 0.0331\n",
      "Correlation: -0.043871 \n",
      " P-Value: 0.542535\n",
      "Epoch [100/100], Loss: 0.0331\n",
      "Max correlation is: [0.14456916] at 0th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 361\n",
    "net_5 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_5.train_net(num_epochs, train_loader_list[5], test_loader_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.163928 \n",
      " P-Value: 0.022025\n",
      "Epoch [1/100], Loss: 0.3208\n",
      "Correlation: 0.171523 \n",
      " P-Value: 0.016504\n",
      "Epoch [2/100], Loss: 0.1148\n",
      "Correlation: 0.166829 \n",
      " P-Value: 0.019752\n",
      "Epoch [3/100], Loss: 0.0830\n",
      "Correlation: 0.131358 \n",
      " P-Value: 0.067182\n",
      "Epoch [4/100], Loss: 0.0579\n",
      "Correlation: 0.107329 \n",
      " P-Value: 0.135321\n",
      "Epoch [5/100], Loss: 0.0473\n",
      "Correlation: 0.056469 \n",
      " P-Value: 0.432977\n",
      "Epoch [6/100], Loss: 0.0459\n",
      "Correlation: 0.048758 \n",
      " P-Value: 0.498469\n",
      "Epoch [7/100], Loss: 0.0423\n",
      "Correlation: 0.083011 \n",
      " P-Value: 0.248613\n",
      "Epoch [8/100], Loss: 0.0383\n",
      "Correlation: 0.065814 \n",
      " P-Value: 0.360652\n",
      "Epoch [9/100], Loss: 0.0346\n",
      "Correlation: 0.057067 \n",
      " P-Value: 0.428118\n",
      "Epoch [10/100], Loss: 0.0319\n",
      "Correlation: 0.013368 \n",
      " P-Value: 0.852830\n",
      "Epoch [11/100], Loss: 0.0289\n",
      "Correlation: -0.032389 \n",
      " P-Value: 0.653073\n",
      "Epoch [12/100], Loss: 0.0265\n",
      "Correlation: -0.080106 \n",
      " P-Value: 0.265617\n",
      "Epoch [13/100], Loss: 0.0248\n",
      "Correlation: -0.089804 \n",
      " P-Value: 0.211846\n",
      "Epoch [14/100], Loss: 0.0237\n",
      "Correlation: -0.094036 \n",
      " P-Value: 0.191011\n",
      "Epoch [15/100], Loss: 0.0232\n",
      "Correlation: -0.086136 \n",
      " P-Value: 0.231185\n",
      "Epoch [16/100], Loss: 0.0228\n",
      "Correlation: -0.072488 \n",
      " P-Value: 0.313910\n",
      "Epoch [17/100], Loss: 0.0224\n",
      "Correlation: -0.046733 \n",
      " P-Value: 0.516500\n",
      "Epoch [18/100], Loss: 0.0221\n",
      "Correlation: -0.028149 \n",
      " P-Value: 0.696079\n",
      "Epoch [19/100], Loss: 0.0217\n",
      "Correlation: -0.016431 \n",
      " P-Value: 0.819646\n",
      "Epoch [20/100], Loss: 0.0214\n",
      "Correlation: -0.012769 \n",
      " P-Value: 0.859361\n",
      "Epoch [21/100], Loss: 0.0212\n",
      "Correlation: -0.005422 \n",
      " P-Value: 0.940047\n",
      "Epoch [22/100], Loss: 0.0209\n",
      "Correlation: 0.004664 \n",
      " P-Value: 0.948401\n",
      "Epoch [23/100], Loss: 0.0208\n",
      "Correlation: 0.007869 \n",
      " P-Value: 0.913054\n",
      "Epoch [24/100], Loss: 0.0206\n",
      "Correlation: 0.009298 \n",
      " P-Value: 0.897332\n",
      "Epoch [25/100], Loss: 0.0204\n",
      "Correlation: 0.010389 \n",
      " P-Value: 0.885410\n",
      "Epoch [26/100], Loss: 0.0203\n",
      "Correlation: 0.015139 \n",
      " P-Value: 0.833647\n",
      "Epoch [27/100], Loss: 0.0201\n",
      "Correlation: 0.020366 \n",
      " P-Value: 0.777467\n",
      "Epoch [28/100], Loss: 0.0200\n",
      "Correlation: 0.024554 \n",
      " P-Value: 0.733310\n",
      "Epoch [29/100], Loss: 0.0199\n",
      "Correlation: 0.027714 \n",
      " P-Value: 0.700530\n",
      "Epoch [30/100], Loss: 0.0198\n",
      "Correlation: 0.030242 \n",
      " P-Value: 0.674705\n",
      "Epoch [31/100], Loss: 0.0197\n",
      "Correlation: 0.032026 \n",
      " P-Value: 0.656719\n",
      "Epoch [32/100], Loss: 0.0196\n",
      "Correlation: 0.032951 \n",
      " P-Value: 0.647456\n",
      "Epoch [33/100], Loss: 0.0196\n",
      "Correlation: 0.036536 \n",
      " P-Value: 0.612091\n",
      "Epoch [34/100], Loss: 0.0195\n",
      "Correlation: 0.038470 \n",
      " P-Value: 0.593372\n",
      "Epoch [35/100], Loss: 0.0194\n",
      "Correlation: 0.039109 \n",
      " P-Value: 0.587251\n",
      "Epoch [36/100], Loss: 0.0194\n",
      "Correlation: 0.041669 \n",
      " P-Value: 0.562999\n",
      "Epoch [37/100], Loss: 0.0193\n",
      "Correlation: 0.043886 \n",
      " P-Value: 0.542397\n",
      "Epoch [38/100], Loss: 0.0192\n",
      "Correlation: 0.043855 \n",
      " P-Value: 0.542685\n",
      "Epoch [39/100], Loss: 0.0192\n",
      "Correlation: 0.046573 \n",
      " P-Value: 0.517936\n",
      "Epoch [40/100], Loss: 0.0191\n",
      "Correlation: 0.049698 \n",
      " P-Value: 0.490224\n",
      "Epoch [41/100], Loss: 0.0191\n",
      "Correlation: 0.050818 \n",
      " P-Value: 0.480480\n",
      "Epoch [42/100], Loss: 0.0190\n",
      "Correlation: 0.052143 \n",
      " P-Value: 0.469095\n",
      "Epoch [43/100], Loss: 0.0190\n",
      "Correlation: 0.050390 \n",
      " P-Value: 0.484185\n",
      "Epoch [44/100], Loss: 0.0190\n",
      "Correlation: 0.051773 \n",
      " P-Value: 0.472263\n",
      "Epoch [45/100], Loss: 0.0189\n",
      "Correlation: 0.050464 \n",
      " P-Value: 0.483549\n",
      "Epoch [46/100], Loss: 0.0189\n",
      "Correlation: 0.048755 \n",
      " P-Value: 0.498496\n",
      "Epoch [47/100], Loss: 0.0189\n",
      "Correlation: 0.051178 \n",
      " P-Value: 0.477372\n",
      "Epoch [48/100], Loss: 0.0188\n",
      "Correlation: 0.052247 \n",
      " P-Value: 0.468208\n",
      "Epoch [49/100], Loss: 0.0188\n",
      "Correlation: 0.051453 \n",
      " P-Value: 0.475013\n",
      "Epoch [50/100], Loss: 0.0188\n",
      "Correlation: 0.054771 \n",
      " P-Value: 0.446968\n",
      "Epoch [51/100], Loss: 0.0188\n",
      "Correlation: 0.053000 \n",
      " P-Value: 0.461810\n",
      "Epoch [52/100], Loss: 0.0187\n",
      "Correlation: 0.053526 \n",
      " P-Value: 0.457376\n",
      "Epoch [53/100], Loss: 0.0187\n",
      "Correlation: 0.055913 \n",
      " P-Value: 0.437535\n",
      "Epoch [54/100], Loss: 0.0187\n",
      "Correlation: 0.055017 \n",
      " P-Value: 0.444918\n",
      "Epoch [55/100], Loss: 0.0187\n",
      "Correlation: 0.057228 \n",
      " P-Value: 0.426812\n",
      "Epoch [56/100], Loss: 0.0186\n",
      "Correlation: 0.059304 \n",
      " P-Value: 0.410199\n",
      "Epoch [57/100], Loss: 0.0186\n",
      "Correlation: 0.058514 \n",
      " P-Value: 0.416483\n",
      "Epoch [58/100], Loss: 0.0186\n",
      "Correlation: 0.060828 \n",
      " P-Value: 0.398259\n",
      "Epoch [59/100], Loss: 0.0186\n",
      "Correlation: 0.058543 \n",
      " P-Value: 0.416252\n",
      "Epoch [60/100], Loss: 0.0186\n",
      "Correlation: 0.061735 \n",
      " P-Value: 0.391249\n",
      "Epoch [61/100], Loss: 0.0186\n",
      "Correlation: 0.060024 \n",
      " P-Value: 0.404530\n",
      "Epoch [62/100], Loss: 0.0186\n",
      "Correlation: 0.061131 \n",
      " P-Value: 0.395905\n",
      "Epoch [63/100], Loss: 0.0185\n",
      "Correlation: 0.063013 \n",
      " P-Value: 0.381494\n",
      "Epoch [64/100], Loss: 0.0185\n",
      "Correlation: 0.061043 \n",
      " P-Value: 0.396585\n",
      "Epoch [65/100], Loss: 0.0185\n",
      "Correlation: 0.064297 \n",
      " P-Value: 0.371851\n",
      "Epoch [66/100], Loss: 0.0185\n",
      "Correlation: 0.061960 \n",
      " P-Value: 0.389521\n",
      "Epoch [67/100], Loss: 0.0185\n",
      "Correlation: 0.064086 \n",
      " P-Value: 0.373424\n",
      "Epoch [68/100], Loss: 0.0185\n",
      "Correlation: 0.064220 \n",
      " P-Value: 0.372425\n",
      "Epoch [69/100], Loss: 0.0185\n",
      "Correlation: 0.063628 \n",
      " P-Value: 0.376861\n",
      "Epoch [70/100], Loss: 0.0185\n",
      "Correlation: 0.065363 \n",
      " P-Value: 0.363958\n",
      "Epoch [71/100], Loss: 0.0185\n",
      "Correlation: 0.064823 \n",
      " P-Value: 0.367941\n",
      "Epoch [72/100], Loss: 0.0184\n",
      "Correlation: 0.066193 \n",
      " P-Value: 0.357888\n",
      "Epoch [73/100], Loss: 0.0184\n",
      "Correlation: 0.065749 \n",
      " P-Value: 0.361129\n",
      "Epoch [74/100], Loss: 0.0184\n",
      "Correlation: 0.066008 \n",
      " P-Value: 0.359232\n",
      "Epoch [75/100], Loss: 0.0184\n",
      "Correlation: 0.066949 \n",
      " P-Value: 0.352414\n",
      "Epoch [76/100], Loss: 0.0184\n",
      "Correlation: 0.067362 \n",
      " P-Value: 0.349441\n",
      "Epoch [77/100], Loss: 0.0184\n",
      "Correlation: 0.066273 \n",
      " P-Value: 0.357305\n",
      "Epoch [78/100], Loss: 0.0184\n",
      "Correlation: 0.068377 \n",
      " P-Value: 0.342212\n",
      "Epoch [79/100], Loss: 0.0184\n",
      "Correlation: 0.067562 \n",
      " P-Value: 0.348011\n",
      "Epoch [80/100], Loss: 0.0184\n",
      "Correlation: 0.067958 \n",
      " P-Value: 0.345184\n",
      "Epoch [81/100], Loss: 0.0184\n",
      "Correlation: 0.067540 \n",
      " P-Value: 0.348165\n",
      "Epoch [82/100], Loss: 0.0184\n",
      "Correlation: 0.069366 \n",
      " P-Value: 0.335261\n",
      "Epoch [83/100], Loss: 0.0184\n",
      "Correlation: 0.067537 \n",
      " P-Value: 0.348187\n",
      "Epoch [84/100], Loss: 0.0184\n",
      "Correlation: 0.070059 \n",
      " P-Value: 0.330443\n",
      "Epoch [85/100], Loss: 0.0184\n",
      "Correlation: 0.068901 \n",
      " P-Value: 0.338510\n",
      "Epoch [86/100], Loss: 0.0183\n",
      "Correlation: 0.070373 \n",
      " P-Value: 0.328275\n",
      "Epoch [87/100], Loss: 0.0183\n",
      "Correlation: 0.070130 \n",
      " P-Value: 0.329949\n",
      "Epoch [88/100], Loss: 0.0183\n",
      "Correlation: 0.071089 \n",
      " P-Value: 0.323363\n",
      "Epoch [89/100], Loss: 0.0183\n",
      "Correlation: 0.070573 \n",
      " P-Value: 0.326895\n",
      "Epoch [90/100], Loss: 0.0183\n",
      "Correlation: 0.072580 \n",
      " P-Value: 0.313289\n",
      "Epoch [91/100], Loss: 0.0183\n",
      "Correlation: 0.070457 \n",
      " P-Value: 0.327692\n",
      "Epoch [92/100], Loss: 0.0183\n",
      "Correlation: 0.072020 \n",
      " P-Value: 0.317051\n",
      "Epoch [93/100], Loss: 0.0183\n",
      "Correlation: 0.070915 \n",
      " P-Value: 0.324550\n",
      "Epoch [94/100], Loss: 0.0183\n",
      "Correlation: 0.070536 \n",
      " P-Value: 0.327151\n",
      "Epoch [95/100], Loss: 0.0183\n",
      "Correlation: 0.069175 \n",
      " P-Value: 0.336594\n",
      "Epoch [96/100], Loss: 0.0183\n",
      "Correlation: 0.069374 \n",
      " P-Value: 0.335204\n",
      "Epoch [97/100], Loss: 0.0183\n",
      "Correlation: 0.069220 \n",
      " P-Value: 0.336280\n",
      "Epoch [98/100], Loss: 0.0183\n",
      "Correlation: 0.070683 \n",
      " P-Value: 0.326142\n",
      "Epoch [99/100], Loss: 0.0183\n",
      "Correlation: 0.072590 \n",
      " P-Value: 0.313226\n",
      "Epoch [100/100], Loss: 0.0183\n",
      "Max correlation is: [0.17152254] at 1th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 369\n",
    "net_6 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_6.train_net(num_epochs, train_loader_list[6], test_loader_list[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.209169 \n",
      " P-Value: 0.003339\n",
      "Epoch [1/100], Loss: 0.2187\n",
      "Correlation: 0.174654 \n",
      " P-Value: 0.014605\n",
      "Epoch [2/100], Loss: 0.1061\n",
      "Correlation: 0.128870 \n",
      " P-Value: 0.072575\n",
      "Epoch [3/100], Loss: 0.0588\n",
      "Correlation: -0.078347 \n",
      " P-Value: 0.276287\n",
      "Epoch [4/100], Loss: 0.0421\n",
      "Correlation: -0.057024 \n",
      " P-Value: 0.428461\n",
      "Epoch [5/100], Loss: 0.0419\n",
      "Correlation: -0.055446 \n",
      " P-Value: 0.441376\n",
      "Epoch [6/100], Loss: 0.0365\n",
      "Correlation: 0.001800 \n",
      " P-Value: 0.980141\n",
      "Epoch [7/100], Loss: 0.0312\n",
      "Correlation: -0.031794 \n",
      " P-Value: 0.659036\n",
      "Epoch [8/100], Loss: 0.0296\n",
      "Correlation: -0.154756 \n",
      " P-Value: 0.030758\n",
      "Epoch [9/100], Loss: 0.0280\n",
      "Correlation: -0.193399 \n",
      " P-Value: 0.006750\n",
      "Epoch [10/100], Loss: 0.0273\n",
      "Correlation: -0.169231 \n",
      " P-Value: 0.018026\n",
      "Epoch [11/100], Loss: 0.0265\n",
      "Correlation: -0.172395 \n",
      " P-Value: 0.015954\n",
      "Epoch [12/100], Loss: 0.0262\n",
      "Correlation: -0.193717 \n",
      " P-Value: 0.006658\n",
      "Epoch [13/100], Loss: 0.0258\n",
      "Correlation: -0.120795 \n",
      " P-Value: 0.092543\n",
      "Epoch [14/100], Loss: 0.0254\n",
      "Correlation: -0.074220 \n",
      " P-Value: 0.302452\n",
      "Epoch [15/100], Loss: 0.0247\n",
      "Correlation: -0.115562 \n",
      " P-Value: 0.107671\n",
      "Epoch [16/100], Loss: 0.0245\n",
      "Correlation: -0.126102 \n",
      " P-Value: 0.078982\n",
      "Epoch [17/100], Loss: 0.0243\n",
      "Correlation: -0.083757 \n",
      " P-Value: 0.244372\n",
      "Epoch [18/100], Loss: 0.0241\n",
      "Correlation: -0.064348 \n",
      " P-Value: 0.371474\n",
      "Epoch [19/100], Loss: 0.0238\n",
      "Correlation: -0.089359 \n",
      " P-Value: 0.214129\n",
      "Epoch [20/100], Loss: 0.0236\n",
      "Correlation: -0.104697 \n",
      " P-Value: 0.145223\n",
      "Epoch [21/100], Loss: 0.0235\n",
      "Correlation: -0.113507 \n",
      " P-Value: 0.114115\n",
      "Epoch [22/100], Loss: 0.0235\n",
      "Correlation: -0.118410 \n",
      " P-Value: 0.099213\n",
      "Epoch [23/100], Loss: 0.0234\n",
      "Correlation: -0.096705 \n",
      " P-Value: 0.178663\n",
      "Epoch [24/100], Loss: 0.0233\n",
      "Correlation: -0.084630 \n",
      " P-Value: 0.239475\n",
      "Epoch [25/100], Loss: 0.0231\n",
      "Correlation: -0.090410 \n",
      " P-Value: 0.208767\n",
      "Epoch [26/100], Loss: 0.0230\n",
      "Correlation: -0.098190 \n",
      " P-Value: 0.172052\n",
      "Epoch [27/100], Loss: 0.0229\n",
      "Correlation: -0.111667 \n",
      " P-Value: 0.120141\n",
      "Epoch [28/100], Loss: 0.0229\n",
      "Correlation: -0.112593 \n",
      " P-Value: 0.117078\n",
      "Epoch [29/100], Loss: 0.0229\n",
      "Correlation: -0.104466 \n",
      " P-Value: 0.146118\n",
      "Epoch [30/100], Loss: 0.0229\n",
      "Correlation: -0.107283 \n",
      " P-Value: 0.135488\n",
      "Epoch [31/100], Loss: 0.0228\n",
      "Correlation: -0.105289 \n",
      " P-Value: 0.142946\n",
      "Epoch [32/100], Loss: 0.0227\n",
      "Correlation: -0.097753 \n",
      " P-Value: 0.173978\n",
      "Epoch [33/100], Loss: 0.0227\n",
      "Correlation: -0.092685 \n",
      " P-Value: 0.197495\n",
      "Epoch [34/100], Loss: 0.0226\n",
      "Correlation: -0.092756 \n",
      " P-Value: 0.197148\n",
      "Epoch [35/100], Loss: 0.0226\n",
      "Correlation: -0.092443 \n",
      " P-Value: 0.198672\n",
      "Epoch [36/100], Loss: 0.0226\n",
      "Correlation: -0.095366 \n",
      " P-Value: 0.184782\n",
      "Epoch [37/100], Loss: 0.0225\n",
      "Correlation: -0.097844 \n",
      " P-Value: 0.173578\n",
      "Epoch [38/100], Loss: 0.0225\n",
      "Correlation: -0.098666 \n",
      " P-Value: 0.169974\n",
      "Epoch [39/100], Loss: 0.0225\n",
      "Correlation: -0.100621 \n",
      " P-Value: 0.161629\n",
      "Epoch [40/100], Loss: 0.0225\n",
      "Correlation: -0.099717 \n",
      " P-Value: 0.165448\n",
      "Epoch [41/100], Loss: 0.0225\n",
      "Correlation: -0.105042 \n",
      " P-Value: 0.143893\n",
      "Epoch [42/100], Loss: 0.0225\n",
      "Correlation: -0.102379 \n",
      " P-Value: 0.154389\n",
      "Epoch [43/100], Loss: 0.0225\n",
      "Correlation: -0.102688 \n",
      " P-Value: 0.153141\n",
      "Epoch [44/100], Loss: 0.0225\n",
      "Correlation: -0.100318 \n",
      " P-Value: 0.162898\n",
      "Epoch [45/100], Loss: 0.0225\n",
      "Correlation: -0.092778 \n",
      " P-Value: 0.197039\n",
      "Epoch [46/100], Loss: 0.0225\n",
      "Correlation: -0.087869 \n",
      " P-Value: 0.221899\n",
      "Epoch [47/100], Loss: 0.0224\n",
      "Correlation: -0.077982 \n",
      " P-Value: 0.278539\n",
      "Epoch [48/100], Loss: 0.0224\n",
      "Correlation: -0.066050 \n",
      " P-Value: 0.358922\n",
      "Epoch [49/100], Loss: 0.0224\n",
      "Correlation: -0.053715 \n",
      " P-Value: 0.455782\n",
      "Epoch [50/100], Loss: 0.0224\n",
      "Correlation: -0.037360 \n",
      " P-Value: 0.604089\n",
      "Epoch [51/100], Loss: 0.0223\n",
      "Correlation: -0.022862 \n",
      " P-Value: 0.751075\n",
      "Epoch [52/100], Loss: 0.0223\n",
      "Correlation: -0.016477 \n",
      " P-Value: 0.819158\n",
      "Epoch [53/100], Loss: 0.0223\n",
      "Correlation: -0.012266 \n",
      " P-Value: 0.864866\n",
      "Epoch [54/100], Loss: 0.0224\n",
      "Correlation: -0.018043 \n",
      " P-Value: 0.802308\n",
      "Epoch [55/100], Loss: 0.0225\n",
      "Correlation: -0.034110 \n",
      " P-Value: 0.635941\n",
      "Epoch [56/100], Loss: 0.0226\n",
      "Correlation: -0.068834 \n",
      " P-Value: 0.338988\n",
      "Epoch [57/100], Loss: 0.0227\n",
      "Correlation: -0.115371 \n",
      " P-Value: 0.108256\n",
      "Epoch [58/100], Loss: 0.0228\n",
      "Correlation: -0.150341 \n",
      " P-Value: 0.035919\n",
      "Epoch [59/100], Loss: 0.0228\n",
      "Correlation: -0.154371 \n",
      " P-Value: 0.031181\n",
      "Epoch [60/100], Loss: 0.0230\n",
      "Correlation: -0.106510 \n",
      " P-Value: 0.138346\n",
      "Epoch [61/100], Loss: 0.0232\n",
      "Correlation: -0.016643 \n",
      " P-Value: 0.817372\n",
      "Epoch [62/100], Loss: 0.0230\n",
      "Correlation: 0.028989 \n",
      " P-Value: 0.687469\n",
      "Epoch [63/100], Loss: 0.0225\n",
      "Correlation: -0.020387 \n",
      " P-Value: 0.777264\n",
      "Epoch [64/100], Loss: 0.0225\n",
      "Correlation: -0.069654 \n",
      " P-Value: 0.333252\n",
      "Epoch [65/100], Loss: 0.0224\n",
      "Correlation: -0.086430 \n",
      " P-Value: 0.229589\n",
      "Epoch [66/100], Loss: 0.0224\n",
      "Correlation: -0.066985 \n",
      " P-Value: 0.352154\n",
      "Epoch [67/100], Loss: 0.0225\n",
      "Correlation: -0.032975 \n",
      " P-Value: 0.647203\n",
      "Epoch [68/100], Loss: 0.0224\n",
      "Correlation: -0.025799 \n",
      " P-Value: 0.720347\n",
      "Epoch [69/100], Loss: 0.0223\n",
      "Correlation: -0.040866 \n",
      " P-Value: 0.570561\n",
      "Epoch [70/100], Loss: 0.0223\n",
      "Correlation: -0.068869 \n",
      " P-Value: 0.338739\n",
      "Epoch [71/100], Loss: 0.0223\n",
      "Correlation: -0.069361 \n",
      " P-Value: 0.335291\n",
      "Epoch [72/100], Loss: 0.0223\n",
      "Correlation: -0.058951 \n",
      " P-Value: 0.413002\n",
      "Epoch [73/100], Loss: 0.0223\n",
      "Correlation: -0.046948 \n",
      " P-Value: 0.514570\n",
      "Epoch [74/100], Loss: 0.0223\n",
      "Correlation: -0.035573 \n",
      " P-Value: 0.621514\n",
      "Epoch [75/100], Loss: 0.0222\n",
      "Correlation: -0.036961 \n",
      " P-Value: 0.607956\n",
      "Epoch [76/100], Loss: 0.0222\n",
      "Correlation: -0.043903 \n",
      " P-Value: 0.542247\n",
      "Epoch [77/100], Loss: 0.0222\n",
      "Correlation: -0.052087 \n",
      " P-Value: 0.469578\n",
      "Epoch [78/100], Loss: 0.0222\n",
      "Correlation: -0.053582 \n",
      " P-Value: 0.456903\n",
      "Epoch [79/100], Loss: 0.0222\n",
      "Correlation: -0.048729 \n",
      " P-Value: 0.498722\n",
      "Epoch [80/100], Loss: 0.0222\n",
      "Correlation: -0.041308 \n",
      " P-Value: 0.566392\n",
      "Epoch [81/100], Loss: 0.0222\n",
      "Correlation: -0.033581 \n",
      " P-Value: 0.641174\n",
      "Epoch [82/100], Loss: 0.0222\n",
      "Correlation: -0.032472 \n",
      " P-Value: 0.652245\n",
      "Epoch [83/100], Loss: 0.0222\n",
      "Correlation: -0.038128 \n",
      " P-Value: 0.596674\n",
      "Epoch [84/100], Loss: 0.0222\n",
      "Correlation: -0.043641 \n",
      " P-Value: 0.544663\n",
      "Epoch [85/100], Loss: 0.0222\n",
      "Correlation: -0.048999 \n",
      " P-Value: 0.496347\n",
      "Epoch [86/100], Loss: 0.0222\n",
      "Correlation: -0.051036 \n",
      " P-Value: 0.478599\n",
      "Epoch [87/100], Loss: 0.0222\n",
      "Correlation: -0.047333 \n",
      " P-Value: 0.511118\n",
      "Epoch [88/100], Loss: 0.0222\n",
      "Correlation: -0.042077 \n",
      " P-Value: 0.559181\n",
      "Epoch [89/100], Loss: 0.0222\n",
      "Correlation: -0.036912 \n",
      " P-Value: 0.608434\n",
      "Epoch [90/100], Loss: 0.0222\n",
      "Correlation: -0.032409 \n",
      " P-Value: 0.652871\n",
      "Epoch [91/100], Loss: 0.0222\n",
      "Correlation: -0.031246 \n",
      " P-Value: 0.664557\n",
      "Epoch [92/100], Loss: 0.0222\n",
      "Correlation: -0.032087 \n",
      " P-Value: 0.656104\n",
      "Epoch [93/100], Loss: 0.0222\n",
      "Correlation: -0.034707 \n",
      " P-Value: 0.630024\n",
      "Epoch [94/100], Loss: 0.0222\n",
      "Correlation: -0.039532 \n",
      " P-Value: 0.583205\n",
      "Epoch [95/100], Loss: 0.0222\n",
      "Correlation: -0.042415 \n",
      " P-Value: 0.556027\n",
      "Epoch [96/100], Loss: 0.0222\n",
      "Correlation: -0.045487 \n",
      " P-Value: 0.527763\n",
      "Epoch [97/100], Loss: 0.0222\n",
      "Correlation: -0.046537 \n",
      " P-Value: 0.518264\n",
      "Epoch [98/100], Loss: 0.0222\n",
      "Correlation: -0.046992 \n",
      " P-Value: 0.514172\n",
      "Epoch [99/100], Loss: 0.0222\n",
      "Correlation: -0.043186 \n",
      " P-Value: 0.548865\n",
      "Epoch [100/100], Loss: 0.0222\n",
      "Max correlation is: [0.20916899] at 0th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 368\n",
    "net_7 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_7.train_net(num_epochs, train_loader_list[7], test_loader_list[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
