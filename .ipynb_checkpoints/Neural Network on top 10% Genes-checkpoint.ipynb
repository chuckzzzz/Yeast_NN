{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions ##\n",
    "1. Map top 10% snps to genes\n",
    "2. Use additive model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import bisect as bs\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import scipy.stats as stat\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18921</th>\n",
       "      <th>18922</th>\n",
       "      <th>18923</th>\n",
       "      <th>18924</th>\n",
       "      <th>18925</th>\n",
       "      <th>18926</th>\n",
       "      <th>18927</th>\n",
       "      <th>18928</th>\n",
       "      <th>18929</th>\n",
       "      <th>18930</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAA_AAA</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAB_AAB</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAC_AAC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAD_AAD</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAE_AAE</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 18931 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2      3      4      5      6      7      8      9      \\\n",
       "AAA_AAA      0      0      0      1      0      0      0      0      0      0   \n",
       "AAB_AAB      0      0      0      0      1      0      0      0      0      0   \n",
       "AAC_AAC      0      0      0      1      0      0      0      0      0      0   \n",
       "AAD_AAD      0      0      0      0      1      1      1      0      0      0   \n",
       "AAE_AAE      0      0      0      1      0      0      0      0      0      0   \n",
       "\n",
       "         ...  18921  18922  18923  18924  18925  18926  18927  18928  18929  \\\n",
       "AAA_AAA  ...      0      0      0      0      0      0      1      0      0   \n",
       "AAB_AAB  ...      0      0      0      0      0      0      1      0      0   \n",
       "AAC_AAC  ...      0      0      0      0      0      0      0      0      0   \n",
       "AAD_AAD  ...      0      0      1      1      1      1      0      0      1   \n",
       "AAE_AAE  ...      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "         18930  \n",
       "AAA_AAA      0  \n",
       "AAB_AAB      0  \n",
       "AAC_AAC      0  \n",
       "AAD_AAD      1  \n",
       "AAE_AAE      0  \n",
       "\n",
       "[5 rows x 18931 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SNP_url = \"./1011geneSNP.csv\"\n",
    "Gene_url = \"./GCF_000146045.2_R64_genomic.gff\"\n",
    "\n",
    "snp_yeast_matrix = pd.read_csv(SNP_url, sep = ',' , error_bad_lines=False)\n",
    "del snp_yeast_matrix['#CHROM']\n",
    "pos_list = list(snp_yeast_matrix['POS'])\n",
    "del snp_yeast_matrix['POS']\n",
    "del snp_yeast_matrix['REF']\n",
    "del snp_yeast_matrix['ALT']\n",
    "del snp_yeast_matrix['ANN[*].GENE']\n",
    "del snp_yeast_matrix['ANN[*].GENEID']\n",
    "index_list = snp_yeast_matrix[\"ID\"].tolist()\n",
    "del snp_yeast_matrix['ID']\n",
    "snp_yeast_matrix = snp_yeast_matrix.transpose()\n",
    "column_names = list(snp_yeast_matrix.index)\n",
    "to_delete = [\"ABC\", \"ABF\", \"ASP\", \"BGS\",\"BGF\", \"BHL\",\"BHQ\",\"BID\",\"BIR\",\"BKG\",\"BKN\",\"CFH\",\"SACE_YAL\",\"SACE_YBA\",\n",
    "             \"SACE_YBB\",\"SACE_YBM\",\"SACE_YBN\",\"SACE_YBO\",\"SACE_YBP\",\"SACE_YCJ\",\"SACE_YCS\",\"SACE_YCT\",\"SACE_YCU\",\"SACE_YCV\",\"SACE_YCW\",\n",
    "             \"SACE_YCX\",\"SACE_YCY\",\"SACE_YCZ\",\"SACE_YDA\",\"SACE_YDB\",\"SACE_YDC\",\"SACE_YDD\", \"SACE_YDE\",\"SACE_YDF\",\"SACE_YDG\",\"SACE_YDH\",\"SACE_YDI\",\n",
    "             \"SACE_YDJ\", \"SACE_YDK\", \"SACE_YDL\"]\n",
    "print(len(to_delete))\n",
    "index = 0\n",
    "for index in range(len(to_delete)):\n",
    "    curr = to_delete[index]\n",
    "    curr = curr+\"_\"+curr\n",
    "    to_delete[index] = curr\n",
    "    index+=1\n",
    "to_delete_index = []\n",
    "for item in to_delete:\n",
    "    curr_index = column_names.index(item)\n",
    "    to_delete_index.append(curr_index)\n",
    "snp_yeast_matrix = snp_yeast_matrix.drop(to_delete)\n",
    "snp_yeast_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_url = \"./phenoMatrix_35ConditionsNormalizedByYPD.csv\"\n",
    "pheno_yeast_matrix = pd.read_csv(pheno_url, delimiter='\\t', error_bad_lines=False)\n",
    "YPD6AU = pheno_yeast_matrix[\"YPD6AU\"]\n",
    "YPDBENOMYL200 = pheno_yeast_matrix[\"YPDBENOMYL200\"]\n",
    "YPDNACL1M = pheno_yeast_matrix[\"YPDNACL1M\"]\n",
    "YPDSDS = pheno_yeast_matrix[\"YPDSDS\"]\n",
    "YPGALACTOSE = pheno_yeast_matrix[\"YPGALACTOSE\"]\n",
    "YPGLYCEROL = pheno_yeast_matrix[\"YPGLYCEROL\"]\n",
    "YPRIBOSE = pheno_yeast_matrix[\"YPRIBOSE\"]\n",
    "YPSORBITOL = pheno_yeast_matrix[\"YPSORBITOL\"]\n",
    "total_run = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate index of top associated snps in 100 runs ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012257347815486095\n",
      "[-4.70382797e+07 -6.89749386e+07 -1.83195975e+07 ...  2.27410609e+05\n",
      "  1.05730682e+04 -2.23611876e+06]\n",
      "[    3    21    10 ... 14519 14741  1267]\n",
      "[63489555.34764801 37168502.67938235 25187533.21800183 ...\n",
      "   966291.92206568   966192.06802127   966080.69411545]\n",
      "0.02928149642082002\n",
      "[11362998.05945773 28713208.33432954 13627430.90108617 ...\n",
      "   -40442.2075441    165859.89389064   -39600.91799881]\n",
      "[   1    3    7 ... 5896 1129 4482]\n",
      "[28713208.33432954 23918107.46240122 22478385.36228473 ...\n",
      "   322581.45664118   322579.76917957   322544.18034099]\n",
      "0.03055584375431484\n",
      "[-1.30255456e+07  1.16265475e+07  7.70332847e+07 ...  6.63462302e+04\n",
      "  2.67797084e+05  1.03873146e+06]\n",
      "[    2     5    11 ... 13456  9317 17605]\n",
      "[77033284.7110372  34113975.27897751 28712000.58812311 ...\n",
      "   985321.52233595   985242.63674778   984991.83188687]\n",
      "0.02986436825179075\n",
      "[-31589016.07814947 -32537335.13906072 -32012055.36644748 ...\n",
      "    -50114.80156448   -819854.95537944   -350007.71854033]\n",
      "[    4    37    12 ... 16097  6437  2089]\n",
      "[21165602.61480784 20843123.0516498  20276192.70727721 ...\n",
      "   637939.48879862   637814.05869804   637282.26414056]\n",
      "0.026770273423110634\n",
      "[-71481987.88642606  17174529.77613132 -59147369.21048155 ...\n",
      "    371590.27775718  -1964216.62093946  -4752374.63636292]\n",
      "[    4    19    13 ... 17851  6138 16731]\n",
      "[90739831.80296534 70743129.31193127 51387931.88249398 ...\n",
      "  1741249.1324289   1740814.05549015  1740175.89062396]\n",
      "0.011781339932532088\n",
      "[-4194653.11724212 48673137.07161298 16891541.77723872 ...\n",
      "  -277769.64717834    76209.49268603  2498787.84096578]\n",
      "[    1     3     9 ... 13950 12643  9140]\n",
      "[48673137.07161298 48659819.5903003  42163759.72037011 ...\n",
      "  1092693.61702056  1091647.38100573  1091398.26219651]\n",
      "0.020967674289920638\n",
      "[ 1.92152663e+08  2.02541566e+07 -5.64396107e+06 ... -2.67101077e+05\n",
      "  1.09653873e+06 -2.77311977e+06]\n",
      "[    0     7     6 ... 18857  4819 16847]\n",
      "[1.92152663e+08 6.31532385e+07 6.00849243e+07 ... 3.04051239e+06\n",
      " 3.04037542e+06 3.04021103e+06]\n",
      "-0.0008324804522335593\n",
      "[ 1.28849337e+09  6.27748969e+07  2.77675781e+08 ...  3.05645157e+05\n",
      "  2.67787235e+07 -1.56707847e+07]\n",
      "[   0    2    6 ... 4134 6782 3716]\n",
      "[1.28849337e+09 2.77675781e+08 1.75685079e+08 ... 1.25043884e+07\n",
      " 1.25037916e+07 1.25017774e+07]\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "weight_vector_1 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPD6AU, test_size=0.2)\n",
    "    lr1 = linear_model.LinearRegression()\n",
    "    lr1.fit(x_train, y_train)\n",
    "    temp = np.array(lr1.coef_)\n",
    "    weight_vector_1 = [x + y for x, y in zip(weight_vector_1, temp)]\n",
    "    predictions_1 = lr1.predict(x_test)\n",
    "    pcc_1 = stat.pearsonr(predictions_1, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_1[0]\n",
    "\n",
    "avg_assoc_1 = sum/total_run\n",
    "avg_weight_vector_1 = np.divide(weight_vector_1, total_run)\n",
    "print(avg_assoc_1)\n",
    "print(avg_weight_vector_1)\n",
    "top2000_index = (-avg_weight_vector_1).argsort()[:2000]\n",
    "print (top2000_index)\n",
    "print (avg_weight_vector_1[top2000_index])\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "sum = 0\n",
    "weight_vector_2 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDBENOMYL200, test_size=0.2)\n",
    "    lr2 = linear_model.LinearRegression()\n",
    "    lr2.fit(x_train, y_train)\n",
    "    temp = np.array(lr2.coef_)\n",
    "    weight_vector_2 = [x + y for x, y in zip(weight_vector_2, temp)]\n",
    "    predictions_2 = lr2.predict(x_test)\n",
    "    pcc_2 = stat.pearsonr(predictions_2, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_2[0]\n",
    "\n",
    "avg_assoc_2 = sum/total_run\n",
    "avg_weight_vector_2 = np.divide(weight_vector_2, total_run)\n",
    "print(avg_assoc_2)\n",
    "print(avg_weight_vector_2)\n",
    "top2000_index_2 = (-avg_weight_vector_2).argsort()[:2000]\n",
    "print (top2000_index_2)\n",
    "print (avg_weight_vector_2[top2000_index_2])\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "sum = 0\n",
    "weight_vector_3 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDNACL1M, test_size=0.2)\n",
    "    lr3 = linear_model.LinearRegression()\n",
    "    lr3.fit(x_train, y_train)\n",
    "    temp = np.array(lr3.coef_)\n",
    "    weight_vector_3 = [x + y for x, y in zip(weight_vector_3, temp)]\n",
    "    predictions_3 = lr3.predict(x_test)\n",
    "    pcc_3 = stat.pearsonr(predictions_3, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_3[0]\n",
    "\n",
    "avg_assoc_3 = sum/total_run\n",
    "avg_weight_vector_3 = np.divide(weight_vector_3, total_run)\n",
    "print(avg_assoc_3)\n",
    "print(avg_weight_vector_3)\n",
    "top2000_index_3 = (-avg_weight_vector_3).argsort()[:2000]\n",
    "print (top2000_index_3)\n",
    "print (avg_weight_vector_3[top2000_index_3])\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "sum = 0\n",
    "weight_vector_4 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPDSDS, test_size=0.2)\n",
    "    lr4 = linear_model.LinearRegression()\n",
    "    lr4.fit(x_train, y_train)\n",
    "    temp = np.array(lr4.coef_)\n",
    "    weight_vector_4 = [x + y for x, y in zip(weight_vector_4, temp)]\n",
    "    predictions_4 = lr4.predict(x_test)\n",
    "    pcc_4 = stat.pearsonr(predictions_4, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_4[0]\n",
    "\n",
    "avg_assoc_4 = sum/total_run\n",
    "avg_weight_vector_4 = np.divide(weight_vector_4, total_run)\n",
    "print(avg_assoc_4)\n",
    "print(avg_weight_vector_4)\n",
    "top2000_index_4 = (-avg_weight_vector_4).argsort()[:2000]\n",
    "print (top2000_index_4)\n",
    "print (avg_weight_vector_4[top2000_index_4])\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "sum = 0\n",
    "weight_vector_5 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPGALACTOSE, test_size=0.2)\n",
    "    lr5 = linear_model.LinearRegression()\n",
    "    lr5.fit(x_train, y_train)\n",
    "    temp = np.array(lr5.coef_)\n",
    "    weight_vector_5 = [x + y for x, y in zip(weight_vector_5, temp)]\n",
    "    predictions_5 = lr5.predict(x_test)\n",
    "    pcc_5 = stat.pearsonr(predictions_5, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_5[0]\n",
    "\n",
    "avg_assoc_5 = sum/total_run\n",
    "avg_weight_vector_5 = np.divide(weight_vector_5, total_run)\n",
    "print(avg_assoc_5)\n",
    "print(avg_weight_vector_5)\n",
    "top2000_index_5 = (-avg_weight_vector_5).argsort()[:2000]\n",
    "print (top2000_index_5)\n",
    "print (avg_weight_vector_5[top2000_index_5])\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "sum = 0\n",
    "weight_vector_6 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPGLYCEROL, test_size=0.2)\n",
    "    lr6 = linear_model.LinearRegression()\n",
    "    lr6.fit(x_train, y_train)\n",
    "    temp = np.array(lr6.coef_)\n",
    "    weight_vector_6 = [x + y for x, y in zip(weight_vector_6, temp)]\n",
    "    predictions_6 = lr6.predict(x_test)\n",
    "    pcc_6 = stat.pearsonr(predictions_6, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_6[0]\n",
    "\n",
    "avg_assoc_6 = sum/total_run\n",
    "avg_weight_vector_6 = np.divide(weight_vector_6, total_run)\n",
    "print(avg_assoc_6)\n",
    "print(avg_weight_vector_6)\n",
    "top2000_index_6 = (-avg_weight_vector_6).argsort()[:2000]\n",
    "print (top2000_index_6)\n",
    "print (avg_weight_vector_6[top2000_index_6])\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "sum = 0\n",
    "weight_vector_7 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPRIBOSE, test_size=0.2)\n",
    "    lr7 = linear_model.LinearRegression()\n",
    "    lr7.fit(x_train, y_train)\n",
    "    temp = np.array(lr7.coef_)\n",
    "    weight_vector_7 = [x + y for x, y in zip(weight_vector_7, temp)]\n",
    "    predictions_7 = lr7.predict(x_test)\n",
    "    pcc_7 = stat.pearsonr(predictions_7, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_7[0]\n",
    "\n",
    "avg_assoc_7 = sum/total_run\n",
    "avg_weight_vector_7 = np.divide(weight_vector_7, total_run)\n",
    "print(avg_assoc_7)\n",
    "print(avg_weight_vector_7)\n",
    "top2000_index_7 = (-avg_weight_vector_7).argsort()[:2000]\n",
    "print (top2000_index_7)\n",
    "print (avg_weight_vector_7[top2000_index_7])\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "sum = 0\n",
    "weight_vector_8 =np.zeros(18931)\n",
    "for counter in range (total_run):\n",
    "    x_train,x_test,y_train,y_test = train_test_split(snp_yeast_matrix, YPSORBITOL, test_size=0.2)\n",
    "    lr8 = linear_model.LinearRegression()\n",
    "    lr8.fit(x_train, y_train)\n",
    "    temp = np.array(lr8.coef_)\n",
    "    weight_vector_8 = [x + y for x, y in zip(weight_vector_8, temp)]\n",
    "    predictions_8 = lr8.predict(x_test)\n",
    "    pcc_8 = stat.pearsonr(predictions_8, y_test)\n",
    "    counter+=1\n",
    "    sum += pcc_8[0]\n",
    "\n",
    "avg_assoc_8 = sum/total_run\n",
    "avg_weight_vector_8 = np.divide(weight_vector_8, total_run)\n",
    "print(avg_assoc_8)\n",
    "print(avg_weight_vector_8)\n",
    "top2000_index_8 = (-avg_weight_vector_8).argsort()[:2000]\n",
    "print (top2000_index_8)\n",
    "print (avg_weight_vector_8[top2000_index_8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_snp_list=[top2000_index,top2000_index_2,top2000_index_3,top2000_index_4,top2000_index_5,top2000_index_6,top2000_index_7,top2000_index_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict finished. There are 6376 genes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAU8</th>\n",
       "      <th>YAL067W-A</th>\n",
       "      <th>SEO1</th>\n",
       "      <th>YAL065C</th>\n",
       "      <th>YAL064W-B</th>\n",
       "      <th>TDA8</th>\n",
       "      <th>YAL064W</th>\n",
       "      <th>YAL063C-A</th>\n",
       "      <th>FLO9</th>\n",
       "      <th>GDH3</th>\n",
       "      <th>...</th>\n",
       "      <th>tN(GUU)Q</th>\n",
       "      <th>tM(CAU)Q1</th>\n",
       "      <th>COX2</th>\n",
       "      <th>Q0255</th>\n",
       "      <th>tF(GAA)Q</th>\n",
       "      <th>tT(UAG)Q2</th>\n",
       "      <th>tV(UAC)Q</th>\n",
       "      <th>COX3</th>\n",
       "      <th>tM(CAU)Q2</th>\n",
       "      <th>RPM1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAA_AAA</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAB_AAB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAC_AAC</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAD_AAD</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAE_AAE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 6427 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PAU8  YAL067W-A  SEO1  YAL065C  YAL064W-B  TDA8  YAL064W  YAL063C-A  \\\n",
       "AAA_AAA   0.0        0.0   0.0      0.0        0.0   0.0      0.0        0.0   \n",
       "AAB_AAB   0.0        0.0   0.0      0.0        0.0   0.0      0.0        0.0   \n",
       "AAC_AAC   0.0        0.0   0.0      0.0        0.0   0.0      0.0        0.0   \n",
       "AAD_AAD   0.0        0.0   0.0      0.0        0.0   0.0      0.0        0.0   \n",
       "AAE_AAE   0.0        0.0   0.0      0.0        0.0   0.0      0.0        0.0   \n",
       "\n",
       "         FLO9  GDH3  ...  tN(GUU)Q  tM(CAU)Q1  COX2  Q0255  tF(GAA)Q  \\\n",
       "AAA_AAA   0.0   0.0  ...       0.0        0.0   0.0    0.0       0.0   \n",
       "AAB_AAB   0.0   0.0  ...       0.0        0.0   0.0    0.0       0.0   \n",
       "AAC_AAC   0.0   0.0  ...       0.0        0.0   0.0    0.0       0.0   \n",
       "AAD_AAD   0.0   0.0  ...       0.0        0.0   0.0    0.0       0.0   \n",
       "AAE_AAE   0.0   0.0  ...       0.0        0.0   0.0    0.0       0.0   \n",
       "\n",
       "         tT(UAG)Q2  tV(UAC)Q  COX3  tM(CAU)Q2  RPM1  \n",
       "AAA_AAA        0.0       0.0   0.0        0.0   0.0  \n",
       "AAB_AAB        0.0       0.0   0.0        0.0   0.0  \n",
       "AAC_AAC        0.0       0.0   0.0        0.0   0.0  \n",
       "AAD_AAD        0.0       0.0   0.0        0.0   0.0  \n",
       "AAE_AAE        0.0       0.0   0.0        0.0   0.0  \n",
       "\n",
       "[5 rows x 6427 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_gene(input):\n",
    "    words = input.split(';')\n",
    "    gene_tuple = words[2]\n",
    "    words = gene_tuple.split('=')\n",
    "    gene = words[1]\n",
    "    return gene\n",
    "\n",
    "def snp_pos_to_gene(pos):\n",
    "    index = bs.bisect(start_pos, pos)\n",
    "    curr_start = start_pos[index-1]\n",
    "    curr_tuple = dict[curr_start]\n",
    "    curr_end = curr_tuple[2]\n",
    "    if pos in range ((int)(curr_start), (int)(curr_end)):\n",
    "        return curr_tuple[0]\n",
    "\n",
    "names=[\"Contig\", \"Seq\",\"Type\", \"Start\", \"End\", \"N/A\", \"N/A1\", \"N/A2\", \"Info\"]\n",
    "gene_matrix = pd.read_csv(Gene_url, sep = '\\t', error_bad_lines=False, skiprows = 8, names = names)\n",
    "gene_matrix.head()\n",
    "\n",
    "dict = {}\n",
    "gene_list = []\n",
    "for row in gene_matrix.itertuples():\n",
    "    if(row.Type == \"gene\"):\n",
    "        curr_info = row.Info\n",
    "        curr_gene = get_gene(curr_info)\n",
    "        gene_list.append(curr_gene)\n",
    "        curr_start = row.Start\n",
    "        curr_end = row.End\n",
    "        tuple = (curr_gene, curr_start, curr_end)\n",
    "        dict[curr_start] = tuple\n",
    "print(\"Dict finished. There are %u genes\" %len(dict))\n",
    "start_pos = list(dict.keys())\n",
    "rows = snp_yeast_matrix.index.values\n",
    "rows = rows[0:]\n",
    "array = np.zeros([len(rows),len(gene_list)])\n",
    "\n",
    "yeast_gene_matrix_1 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "yeast_gene_matrix_2 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "yeast_gene_matrix_3 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "yeast_gene_matrix_4 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "yeast_gene_matrix_5 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "yeast_gene_matrix_6 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "yeast_gene_matrix_7 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "yeast_gene_matrix_8 = pd.DataFrame(array, columns = gene_list, index = rows, copy = True)\n",
    "\n",
    "yeast_gene_matrix_list = [yeast_gene_matrix_1,yeast_gene_matrix_2,yeast_gene_matrix_3,yeast_gene_matrix_4,yeast_gene_matrix_5,yeast_gene_matrix_6,yeast_gene_matrix_7,yeast_gene_matrix_8]\n",
    "yeast_gene_matrix_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snp_list_to_gene(valid_snp_list, snp_yeast_matrix, yeast_gene_matrix):\n",
    "    max_snp = 18931\n",
    "    max_yeast = 971\n",
    "    snp_counter = 0\n",
    "    for snp_counter in range(max_snp):\n",
    "        if(snp_counter %2000 == 0):\n",
    "            print(snp_counter)\n",
    "        curr_column = snp_yeast_matrix[snp_counter].tolist()\n",
    "        curr_pos = pos_list[snp_counter]\n",
    "        yeast_counter = 0\n",
    "        for yeast_counter in range (max_yeast):\n",
    "            curr_val = curr_column[yeast_counter]\n",
    "            if(curr_val)!=0:\n",
    "                if snp_counter in valid_snp_list:\n",
    "                    gene = snp_pos_to_gene(curr_pos)\n",
    "                    if(gene is None):\n",
    "                        continue\n",
    "                    yeast_name = rows[yeast_counter]\n",
    "                    yeast_gene_matrix[gene][yeast_name] += 1\n",
    "            yeast_counter += 1\n",
    "        snp_counter += 1\n",
    "    print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n",
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for valid_snp, yg_matrix in zip(valid_snp_list, yeast_gene_matrix_list):\n",
    "    snp_list_to_gene(valid_snp, snp_yeast_matrix, yg_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(971, 251)\n",
      "(971, 242)\n",
      "(971, 259)\n",
      "(971, 252)\n",
      "(971, 251)\n",
      "(971, 242)\n",
      "(971, 235)\n",
      "(971, 232)\n"
     ]
    }
   ],
   "source": [
    "min_yeast_with_gene = 400\n",
    "min_total_gene = 2000\n",
    "for matrix in yeast_gene_matrix_list:\n",
    "    for column in matrix:\n",
    "        counter = 0\n",
    "        value = 0\n",
    "        temp = matrix[column]\n",
    "        for element in temp:\n",
    "            if element != 0:\n",
    "                counter += 1\n",
    "                value += 4\n",
    "        if(counter < min_yeast_with_gene and value <min_total_gene ):\n",
    "            del matrix[column]\n",
    "            \n",
    "for matrix in yeast_gene_matrix_list:\n",
    "    print(matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLO9</th>\n",
       "      <th>BDH2</th>\n",
       "      <th>BDH1</th>\n",
       "      <th>CNE1</th>\n",
       "      <th>GPB2</th>\n",
       "      <th>ACS1</th>\n",
       "      <th>FLC2</th>\n",
       "      <th>OAF1</th>\n",
       "      <th>AIM2</th>\n",
       "      <th>GEM1</th>\n",
       "      <th>...</th>\n",
       "      <th>BET2</th>\n",
       "      <th>PRP4</th>\n",
       "      <th>HDA3</th>\n",
       "      <th>AOS1</th>\n",
       "      <th>GDB1</th>\n",
       "      <th>ATG13</th>\n",
       "      <th>PZF1</th>\n",
       "      <th>SKI3</th>\n",
       "      <th>RPC82</th>\n",
       "      <th>QCR2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAA_AAA</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAB_AAB</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAC_AAC</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAD_AAD</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAE_AAE</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAG_AAG</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAH_AAH</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAI_AAI</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAK_AAK</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAL_AAL</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAM_AAM</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAN_AAN</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAP_AAP</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAQ_AAQ</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAR_AAR</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAS_AAS</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAT_AAT</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAV_AAV</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABA_ABA</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABB_ABB</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABD_ABD</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABE_ABE</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABG_ABG</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABH_ABH</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABI_ABI</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABK_ABK</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABL_ABL</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABM_ABM</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABP_ABP</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABQ_ABQ</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBQ_SACE_YBQ</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBR_SACE_YBR</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBS_SACE_YBS</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBT_SACE_YBT</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBU_SACE_YBU</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBV_SACE_YBV</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBW_SACE_YBW</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBX_SACE_YBX</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBY_SACE_YBY</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YBZ_SACE_YBZ</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCA_SACE_YCA</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCB_SACE_YCB</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCC_SACE_YCC</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCD_SACE_YCD</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCE_SACE_YCE</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCF_SACE_YCF</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCG_SACE_YCG</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCH_SACE_YCH</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCI_SACE_YCI</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCK_SACE_YCK</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCL_SACE_YCL</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCM_SACE_YCM</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCN_SACE_YCN</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCO_SACE_YCO</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCP_SACE_YCP</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCQ_SACE_YCQ</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YCR_SACE_YCR</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YDM_SACE_YDM</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YDN_SACE_YDN</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SACE_YDO_SACE_YDO</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>971 rows Ã— 251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   FLO9  BDH2  BDH1  CNE1  GPB2  ACS1  FLC2  OAF1  AIM2  GEM1  \\\n",
       "AAA_AAA             1.0   0.0   1.0   0.0   1.0   3.0   2.0   1.0   1.0   1.0   \n",
       "AAB_AAB             1.0   0.0   1.0   1.0   2.0   3.0   1.0   0.0   0.0   0.0   \n",
       "AAC_AAC             4.0   1.0   1.0   1.0   1.0   2.0   2.0   4.0   1.0   2.0   \n",
       "AAD_AAD             8.0   1.0   4.0   6.0   6.0   8.0   5.0  12.0   3.0   2.0   \n",
       "AAE_AAE             1.0   1.0   1.0   1.0   2.0   2.0   0.0   1.0   0.0   1.0   \n",
       "AAG_AAG            10.0   0.0   3.0   3.0   2.0   6.0   4.0   6.0   3.0   0.0   \n",
       "AAH_AAH             8.0   1.0   2.0   4.0   5.0   6.0   3.0   6.0   3.0   0.0   \n",
       "AAI_AAI             1.0   1.0   0.0   0.0   1.0   4.0   2.0   1.0   1.0   0.0   \n",
       "AAK_AAK             1.0   1.0   0.0   1.0   2.0   1.0   0.0   1.0   0.0   1.0   \n",
       "AAL_AAL             1.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   2.0   \n",
       "AAM_AAM             2.0   1.0   3.0   2.0   4.0   9.0   3.0   5.0   3.0   2.0   \n",
       "AAN_AAN             6.0   0.0   1.0   3.0   2.0   0.0   2.0   3.0   3.0   0.0   \n",
       "AAP_AAP             2.0   0.0   3.0   3.0   0.0   0.0   2.0   3.0   3.0   0.0   \n",
       "AAQ_AAQ             2.0   0.0   1.0   1.0   0.0   4.0   0.0   2.0   0.0   1.0   \n",
       "AAR_AAR             3.0   0.0   1.0   2.0   1.0   5.0   1.0   1.0   1.0   0.0   \n",
       "AAS_AAS             6.0   1.0   1.0   2.0   1.0   5.0   1.0   4.0   1.0   2.0   \n",
       "AAT_AAT             5.0   1.0   4.0   6.0   6.0   6.0   5.0   6.0   0.0   0.0   \n",
       "AAV_AAV             2.0   1.0   5.0   2.0   3.0   5.0   4.0   6.0   4.0   0.0   \n",
       "ABA_ABA             2.0   1.0   1.0   4.0   6.0   2.0   6.0   5.0   0.0   0.0   \n",
       "ABB_ABB             9.0   1.0   4.0   6.0   6.0   8.0   5.0  12.0   3.0   2.0   \n",
       "ABD_ABD             4.0   1.0   3.0   3.0   4.0   6.0   5.0   3.0   1.0   2.0   \n",
       "ABE_ABE             1.0   1.0   1.0   0.0   1.0   3.0   1.0   2.0   1.0   0.0   \n",
       "ABG_ABG             7.0   1.0   2.0   4.0   4.0   5.0   2.0   8.0   3.0   0.0   \n",
       "ABH_ABH             2.0   2.0   4.0   7.0   6.0   3.0   5.0   6.0   3.0   2.0   \n",
       "ABI_ABI             6.0   0.0   1.0   5.0   6.0   8.0   4.0   6.0   1.0   2.0   \n",
       "ABK_ABK             2.0   2.0   1.0   4.0   2.0   2.0   4.0   3.0   2.0   2.0   \n",
       "ABL_ABL             8.0   1.0   4.0   5.0   5.0   6.0   3.0   9.0   3.0   2.0   \n",
       "ABM_ABM             1.0   0.0   0.0   0.0   2.0   1.0   1.0   1.0   1.0   0.0   \n",
       "ABP_ABP             1.0   1.0   0.0   3.0   1.0   1.0   2.0   1.0   1.0   0.0   \n",
       "ABQ_ABQ             1.0   1.0   1.0   0.0   1.0   3.0   2.0   1.0   1.0   0.0   \n",
       "...                 ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "SACE_YBQ_SACE_YBQ   3.0   0.0   1.0   0.0   2.0   6.0   2.0   1.0   0.0   0.0   \n",
       "SACE_YBR_SACE_YBR   1.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   0.0   \n",
       "SACE_YBS_SACE_YBS   8.0   1.0   4.0   6.0   6.0   7.0   4.0  11.0   4.0   2.0   \n",
       "SACE_YBT_SACE_YBT   4.0   1.0   3.0   3.0   3.0   5.0   1.0   7.0   3.0   0.0   \n",
       "SACE_YBU_SACE_YBU   6.0   1.0   2.0   5.0   4.0   5.0   3.0   8.0   4.0   2.0   \n",
       "SACE_YBV_SACE_YBV   9.0   0.0   2.0   1.0   0.0   3.0   1.0   6.0   3.0   2.0   \n",
       "SACE_YBW_SACE_YBW   3.0   0.0   4.0   1.0   6.0   1.0   2.0   5.0   0.0   1.0   \n",
       "SACE_YBX_SACE_YBX   2.0   0.0   0.0   0.0   2.0   5.0   1.0   1.0   1.0   0.0   \n",
       "SACE_YBY_SACE_YBY   1.0   1.0   1.0   0.0   1.0   4.0   2.0   1.0   0.0   0.0   \n",
       "SACE_YBZ_SACE_YBZ   2.0   1.0   2.0   3.0   3.0   3.0   2.0   2.0   1.0   0.0   \n",
       "SACE_YCA_SACE_YCA   1.0   1.0   1.0   0.0   1.0   4.0   2.0   1.0   1.0   0.0   \n",
       "SACE_YCB_SACE_YCB   8.0   1.0   4.0   5.0   5.0   6.0   3.0   9.0   3.0   2.0   \n",
       "SACE_YCC_SACE_YCC   8.0   0.0   3.0   3.0   0.0   3.0   3.0   4.0   3.0   0.0   \n",
       "SACE_YCD_SACE_YCD   1.0   1.0   1.0   0.0   1.0   3.0   2.0   1.0   1.0   0.0   \n",
       "SACE_YCE_SACE_YCE   9.0   0.0   4.0   2.0   4.0   6.0   1.0  10.0   4.0   1.0   \n",
       "SACE_YCF_SACE_YCF   4.0   0.0   2.0   1.0   3.0   4.0   2.0   3.0   3.0   0.0   \n",
       "SACE_YCG_SACE_YCG   7.0   0.0   5.0   3.0   4.0   6.0   2.0   4.0   1.0   2.0   \n",
       "SACE_YCH_SACE_YCH   3.0   0.0   4.0   4.0   5.0   4.0   4.0   8.0   3.0   2.0   \n",
       "SACE_YCI_SACE_YCI   2.0   1.0   0.0   1.0   0.0   6.0   2.0   3.0   1.0   2.0   \n",
       "SACE_YCK_SACE_YCK   9.0   1.0   4.0   6.0   6.0   7.0   5.0  12.0   3.0   2.0   \n",
       "SACE_YCL_SACE_YCL   3.0   0.0   2.0   3.0   5.0   1.0   3.0   4.0   3.0   3.0   \n",
       "SACE_YCM_SACE_YCM   8.0   1.0   4.0   5.0   6.0   6.0   4.0  11.0   3.0   2.0   \n",
       "SACE_YCN_SACE_YCN   8.0   1.0   4.0   6.0   6.0   5.0   4.0  12.0   3.0   2.0   \n",
       "SACE_YCO_SACE_YCO   5.0   1.0   2.0   4.0   6.0   4.0   2.0   9.0   3.0   1.0   \n",
       "SACE_YCP_SACE_YCP   1.0   1.0   1.0   0.0   1.0   3.0   2.0   1.0   1.0   0.0   \n",
       "SACE_YCQ_SACE_YCQ   2.0   1.0   0.0   0.0   1.0   3.0   1.0   1.0   1.0   0.0   \n",
       "SACE_YCR_SACE_YCR   6.0   0.0   4.0   6.0   6.0   7.0   3.0  10.0   4.0   3.0   \n",
       "SACE_YDM_SACE_YDM   1.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   0.0   \n",
       "SACE_YDN_SACE_YDN   7.0   0.0   5.0   2.0   1.0   5.0   4.0   5.0   4.0   2.0   \n",
       "SACE_YDO_SACE_YDO   1.0   0.0   1.0   1.0   1.0   5.0   2.0   1.0   1.0   0.0   \n",
       "\n",
       "                   ...  BET2  PRP4  HDA3  AOS1  GDB1  ATG13  PZF1  SKI3  \\\n",
       "AAA_AAA            ...   3.0   1.0   1.0   0.0   1.0    0.0   0.0   0.0   \n",
       "AAB_AAB            ...   0.0   1.0   2.0   0.0   1.0    0.0   1.0   0.0   \n",
       "AAC_AAC            ...   0.0   2.0   2.0   1.0   0.0    2.0   0.0   1.0   \n",
       "AAD_AAD            ...   0.0   2.0   2.0   2.0   5.0    5.0   0.0   3.0   \n",
       "AAE_AAE            ...   2.0   2.0   2.0   0.0   2.0    0.0   0.0   0.0   \n",
       "AAG_AAG            ...   0.0   3.0   1.0   2.0   2.0    4.0   0.0   2.0   \n",
       "AAH_AAH            ...   2.0   2.0   2.0   1.0   3.0    3.0   0.0   2.0   \n",
       "AAI_AAI            ...   3.0   1.0   2.0   0.0   1.0    0.0   1.0   0.0   \n",
       "AAK_AAK            ...   2.0   2.0   2.0   0.0   2.0    0.0   0.0   0.0   \n",
       "AAL_AAL            ...   3.0   0.0   2.0   0.0   2.0    2.0   1.0   2.0   \n",
       "AAM_AAM            ...   3.0   0.0   3.0   0.0   4.0    4.0   1.0   2.0   \n",
       "AAN_AAN            ...   1.0   1.0   2.0   0.0   2.0    5.0   0.0   2.0   \n",
       "AAP_AAP            ...   3.0   1.0   2.0   0.0   1.0    3.0   0.0   0.0   \n",
       "AAQ_AAQ            ...   0.0   1.0   2.0   0.0   2.0    2.0   1.0   2.0   \n",
       "AAR_AAR            ...   2.0   2.0   2.0   0.0   2.0    0.0   1.0   0.0   \n",
       "AAS_AAS            ...   0.0   2.0   4.0   0.0   4.0    1.0   1.0   0.0   \n",
       "AAT_AAT            ...   0.0   0.0   2.0   0.0   5.0    1.0   0.0   1.0   \n",
       "AAV_AAV            ...   0.0   2.0   1.0   2.0   3.0    2.0   1.0   3.0   \n",
       "ABA_ABA            ...   0.0   1.0   3.0   0.0   2.0    2.0   1.0   1.0   \n",
       "ABB_ABB            ...   0.0   2.0   2.0   2.0   5.0    5.0   1.0   3.0   \n",
       "ABD_ABD            ...   0.0   2.0   1.0   1.0   3.0    1.0   0.0   1.0   \n",
       "ABE_ABE            ...   3.0   2.0   0.0   0.0   1.0    0.0   0.0   1.0   \n",
       "ABG_ABG            ...   1.0   0.0   1.0   1.0   3.0    1.0   0.0   1.0   \n",
       "ABH_ABH            ...   0.0   1.0   2.0   2.0   5.0    4.0   1.0   2.0   \n",
       "ABI_ABI            ...   2.0   1.0   2.0   1.0   1.0    0.0   2.0   0.0   \n",
       "ABK_ABK            ...   0.0   2.0   1.0   1.0   2.0    2.0   1.0   1.0   \n",
       "ABL_ABL            ...   0.0   2.0   2.0   2.0   5.0    5.0   1.0   2.0   \n",
       "ABM_ABM            ...   1.0   0.0   2.0   0.0   1.0    0.0   1.0   0.0   \n",
       "ABP_ABP            ...   3.0   1.0   0.0   0.0   0.0    2.0   0.0   0.0   \n",
       "ABQ_ABQ            ...   1.0   1.0   2.0   0.0   1.0    0.0   1.0   0.0   \n",
       "...                ...   ...   ...   ...   ...   ...    ...   ...   ...   \n",
       "SACE_YBQ_SACE_YBQ  ...   2.0   1.0   0.0   0.0   3.0    0.0   1.0   1.0   \n",
       "SACE_YBR_SACE_YBR  ...   1.0   0.0   2.0   0.0   2.0    0.0   1.0   0.0   \n",
       "SACE_YBS_SACE_YBS  ...   0.0   2.0   2.0   2.0   5.0    5.0   0.0   2.0   \n",
       "SACE_YBT_SACE_YBT  ...   3.0   3.0   0.0   2.0   1.0    2.0   1.0   1.0   \n",
       "SACE_YBU_SACE_YBU  ...   0.0   2.0   2.0   2.0   5.0    5.0   0.0   2.0   \n",
       "SACE_YBV_SACE_YBV  ...   2.0   2.0   2.0   0.0   5.0    1.0   1.0   1.0   \n",
       "SACE_YBW_SACE_YBW  ...   0.0   2.0   2.0   2.0   2.0    3.0   0.0   3.0   \n",
       "SACE_YBX_SACE_YBX  ...   1.0   2.0   2.0   0.0   0.0    0.0   1.0   0.0   \n",
       "SACE_YBY_SACE_YBY  ...   1.0   1.0   2.0   0.0   1.0    0.0   1.0   1.0   \n",
       "SACE_YBZ_SACE_YBZ  ...   2.0   3.0   0.0   1.0   2.0    3.0   0.0   1.0   \n",
       "SACE_YCA_SACE_YCA  ...   3.0   1.0   0.0   0.0   2.0    0.0   1.0   0.0   \n",
       "SACE_YCB_SACE_YCB  ...   0.0   2.0   2.0   2.0   5.0    5.0   1.0   2.0   \n",
       "SACE_YCC_SACE_YCC  ...   1.0   0.0   3.0   0.0   1.0    1.0   1.0   2.0   \n",
       "SACE_YCD_SACE_YCD  ...   1.0   0.0   0.0   0.0   2.0    0.0   1.0   0.0   \n",
       "SACE_YCE_SACE_YCE  ...   0.0   0.0   1.0   0.0   3.0    3.0   0.0   1.0   \n",
       "SACE_YCF_SACE_YCF  ...   2.0   3.0   0.0   1.0   3.0    1.0   0.0   0.0   \n",
       "SACE_YCG_SACE_YCG  ...   1.0   2.0   2.0   1.0   1.0    2.0   0.0   0.0   \n",
       "SACE_YCH_SACE_YCH  ...   0.0   1.0   4.0   0.0   5.0    1.0   0.0   0.0   \n",
       "SACE_YCI_SACE_YCI  ...   2.0   2.0   3.0   0.0   4.0    1.0   0.0   1.0   \n",
       "SACE_YCK_SACE_YCK  ...   0.0   2.0   2.0   2.0   5.0    5.0   1.0   3.0   \n",
       "SACE_YCL_SACE_YCL  ...   0.0   0.0   1.0   2.0   1.0    4.0   0.0   2.0   \n",
       "SACE_YCM_SACE_YCM  ...   0.0   2.0   2.0   2.0   5.0    5.0   0.0   2.0   \n",
       "SACE_YCN_SACE_YCN  ...   0.0   2.0   1.0   2.0   4.0    5.0   0.0   3.0   \n",
       "SACE_YCO_SACE_YCO  ...   0.0   2.0   2.0   1.0   4.0    4.0   0.0   3.0   \n",
       "SACE_YCP_SACE_YCP  ...   3.0   1.0   0.0   0.0   2.0    0.0   1.0   0.0   \n",
       "SACE_YCQ_SACE_YCQ  ...   1.0   1.0   0.0   0.0   1.0    0.0   1.0   0.0   \n",
       "SACE_YCR_SACE_YCR  ...   0.0   2.0   2.0   2.0   3.0    5.0   1.0   2.0   \n",
       "SACE_YDM_SACE_YDM  ...   1.0   1.0   2.0   0.0   0.0    1.0   1.0   0.0   \n",
       "SACE_YDN_SACE_YDN  ...   0.0   1.0   4.0   1.0   5.0    5.0   0.0   3.0   \n",
       "SACE_YDO_SACE_YDO  ...   2.0   1.0   2.0   0.0   1.0    0.0   1.0   0.0   \n",
       "\n",
       "                   RPC82  QCR2  \n",
       "AAA_AAA              0.0   0.0  \n",
       "AAB_AAB              0.0   0.0  \n",
       "AAC_AAC              3.0   1.0  \n",
       "AAD_AAD              4.0   1.0  \n",
       "AAE_AAE              0.0   0.0  \n",
       "AAG_AAG              0.0   1.0  \n",
       "AAH_AAH              2.0   1.0  \n",
       "AAI_AAI              0.0   0.0  \n",
       "AAK_AAK              0.0   0.0  \n",
       "AAL_AAL              2.0   1.0  \n",
       "AAM_AAM              3.0   1.0  \n",
       "AAN_AAN              2.0   1.0  \n",
       "AAP_AAP              0.0   1.0  \n",
       "AAQ_AAQ              2.0   0.0  \n",
       "AAR_AAR              0.0   0.0  \n",
       "AAS_AAS              1.0   1.0  \n",
       "AAT_AAT              2.0   1.0  \n",
       "AAV_AAV              2.0   1.0  \n",
       "ABA_ABA              1.0   0.0  \n",
       "ABB_ABB              4.0   1.0  \n",
       "ABD_ABD              2.0   1.0  \n",
       "ABE_ABE              0.0   0.0  \n",
       "ABG_ABG              1.0   1.0  \n",
       "ABH_ABH              3.0   0.0  \n",
       "ABI_ABI              0.0   0.0  \n",
       "ABK_ABK              1.0   0.0  \n",
       "ABL_ABL              4.0   1.0  \n",
       "ABM_ABM              0.0   0.0  \n",
       "ABP_ABP              0.0   0.0  \n",
       "ABQ_ABQ              0.0   0.0  \n",
       "...                  ...   ...  \n",
       "SACE_YBQ_SACE_YBQ    0.0   0.0  \n",
       "SACE_YBR_SACE_YBR    0.0   0.0  \n",
       "SACE_YBS_SACE_YBS    3.0   1.0  \n",
       "SACE_YBT_SACE_YBT    2.0   1.0  \n",
       "SACE_YBU_SACE_YBU    1.0   1.0  \n",
       "SACE_YBV_SACE_YBV    2.0   0.0  \n",
       "SACE_YBW_SACE_YBW    4.0   0.0  \n",
       "SACE_YBX_SACE_YBX    0.0   0.0  \n",
       "SACE_YBY_SACE_YBY    0.0   0.0  \n",
       "SACE_YBZ_SACE_YBZ    0.0   1.0  \n",
       "SACE_YCA_SACE_YCA    0.0   0.0  \n",
       "SACE_YCB_SACE_YCB    4.0   1.0  \n",
       "SACE_YCC_SACE_YCC    2.0   0.0  \n",
       "SACE_YCD_SACE_YCD    0.0   0.0  \n",
       "SACE_YCE_SACE_YCE    1.0   1.0  \n",
       "SACE_YCF_SACE_YCF    0.0   1.0  \n",
       "SACE_YCG_SACE_YCG    1.0   1.0  \n",
       "SACE_YCH_SACE_YCH    2.0   1.0  \n",
       "SACE_YCI_SACE_YCI    1.0   1.0  \n",
       "SACE_YCK_SACE_YCK    4.0   1.0  \n",
       "SACE_YCL_SACE_YCL    1.0   1.0  \n",
       "SACE_YCM_SACE_YCM    3.0   1.0  \n",
       "SACE_YCN_SACE_YCN    3.0   1.0  \n",
       "SACE_YCO_SACE_YCO    4.0   1.0  \n",
       "SACE_YCP_SACE_YCP    0.0   0.0  \n",
       "SACE_YCQ_SACE_YCQ    0.0   0.0  \n",
       "SACE_YCR_SACE_YCR    3.0   1.0  \n",
       "SACE_YDM_SACE_YDM    0.0   0.0  \n",
       "SACE_YDN_SACE_YDN    4.0   1.0  \n",
       "SACE_YDO_SACE_YDO    0.0   0.0  \n",
       "\n",
       "[971 rows x 251 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yeast_gene_matrix_list[0]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLO9</th>\n",
       "      <th>BDH2</th>\n",
       "      <th>BDH1</th>\n",
       "      <th>ECM1</th>\n",
       "      <th>CNE1</th>\n",
       "      <th>GPB2</th>\n",
       "      <th>ACS1</th>\n",
       "      <th>FLC2</th>\n",
       "      <th>OAF1</th>\n",
       "      <th>AIM2</th>\n",
       "      <th>...</th>\n",
       "      <th>TIF3</th>\n",
       "      <th>MMS1</th>\n",
       "      <th>BSP1</th>\n",
       "      <th>VPS4</th>\n",
       "      <th>DPB2</th>\n",
       "      <th>BET2</th>\n",
       "      <th>AOS1</th>\n",
       "      <th>GDB1</th>\n",
       "      <th>ATG13</th>\n",
       "      <th>PZF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAA_AAA</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAB_AAB</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAC_AAC</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAD_AAD</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAE_AAE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 242 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         FLO9  BDH2  BDH1  ECM1  CNE1  GPB2  ACS1  FLC2  OAF1  AIM2  ...  \\\n",
       "AAA_AAA   4.0   1.0   1.0   2.0   1.0   0.0   3.0   1.0   2.0   2.0  ...   \n",
       "AAB_AAB   0.0   1.0   0.0   1.0   3.0   1.0   3.0   0.0   1.0   1.0  ...   \n",
       "AAC_AAC   6.0   0.0   0.0   2.0   2.0   0.0   2.0   2.0   1.0   0.0  ...   \n",
       "AAD_AAD  11.0   2.0   1.0   1.0   5.0   6.0   5.0   4.0   6.0   1.0  ...   \n",
       "AAE_AAE   0.0   0.0   0.0   1.0   2.0   0.0   2.0   0.0   2.0   1.0  ...   \n",
       "\n",
       "         TIF3  MMS1  BSP1  VPS4  DPB2  BET2  AOS1  GDB1  ATG13  PZF1  \n",
       "AAA_AAA   1.0   0.0   0.0   0.0   3.0   1.0   0.0   0.0    0.0   0.0  \n",
       "AAB_AAB   1.0   0.0   0.0   0.0   1.0   0.0   1.0   0.0    1.0   0.0  \n",
       "AAC_AAC   1.0   2.0   1.0   1.0   1.0   0.0   2.0   2.0    0.0   0.0  \n",
       "AAD_AAD   1.0   3.0   3.0   2.0   0.0   0.0   4.0   8.0    2.0   3.0  \n",
       "AAE_AAE   0.0   1.0   0.0   0.0   1.0   1.0   0.0   0.0    0.0   0.0  \n",
       "\n",
       "[5 rows x 242 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yeast_gene_matrix_list[1].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as dt\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, h1, h2, h3, output_size, lr):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, h1)\n",
    "        self.l2 = nn.Linear(h1, h2)\n",
    "        self.l3 = nn.Linear(h2, h3)\n",
    "        self.l4 = nn.Linear(h3, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l4(out)\n",
    "        return out\n",
    "     \n",
    "    def train_net(self, num_epochs, train_loader, test_loader):\n",
    "        max_rsq = 0\n",
    "        max_epoch = 0\n",
    "        for epoch in range (num_epochs):\n",
    "            total_loss = 0\n",
    "            counter = 0\n",
    "            for i, (data, target) in enumerate(train_loader):\n",
    "                self.zero_grad()\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = self.forward(data).to(device)\n",
    "                loss = self.loss_fn(output, target)\n",
    "                total_loss += loss\n",
    "                counter += 1\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            # empty arrays for empty tensor \n",
    "            empty1 = np.empty((0,1))\n",
    "            empty2 = np.empty((0,1))\n",
    "            output_tensor = torch.tensor(empty1, dtype = torch.float)\n",
    "            target_tensor = torch.tensor(empty2, dtype = torch.float)\n",
    "            # concatenate output and target to the empty tensors \n",
    "            for i, (data, target) in enumerate(test_loader): \n",
    "                data = data.to(device) \n",
    "                target = target.to(device) \n",
    "                output = self.forward(data).to(device)\n",
    "                target = torch.reshape(target, (len(target),1))\n",
    "                output_tensor = torch.cat((output_tensor, output.cpu()))\n",
    "                target_tensor = torch.cat((target_tensor, target.cpu()))\n",
    "            \n",
    "            #convert the tensor into numpy arrays and calculate correlation \n",
    "            output_array = output_tensor.detach().numpy()\n",
    "            target_array = target_tensor.detach().numpy()\n",
    "            if ((output_array.shape[0]!=target_array.shape[0]) or (output_array.shape[1]!=target_array.shape[1])):\n",
    "                raise Exception('Dimension mismatch')\n",
    "            result = stat.pearsonr(target_array, output_array)\n",
    "            print(\"Correlation: %f \\n P-Value: %f\" %(result[0], result[1])) \n",
    "            if(math.fabs(result[0])>math.fabs(max_rsq)):\n",
    "                max_rsq = result[0]\n",
    "                max_epoch = epoch\n",
    "            epoch += 1\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                .format(epoch, num_epochs, total_loss/len(train_loader)))\n",
    "    \n",
    "        print(\"Max correlation is: {} at {}th epoch\".format(max_rsq, max_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno_url = \"./phenoMatrix_35ConditionsNormalizedByYPD.csv\"\n",
    "pheno_yeast_matrix = pd.read_csv(pheno_url, delimiter='\\t', error_bad_lines=False)\n",
    "YPD6AU = pheno_yeast_matrix[\"YPD6AU\"]\n",
    "YPDBENOMYL200 = pheno_yeast_matrix[\"YPDBENOMYL200\"]\n",
    "YPDNACL1M = pheno_yeast_matrix[\"YPDNACL1M\"]\n",
    "YPDSDS = pheno_yeast_matrix[\"YPDSDS\"]\n",
    "YPGALACTOSE = pheno_yeast_matrix[\"YPGALACTOSE\"]\n",
    "YPGLYCEROL = pheno_yeast_matrix[\"YPGLYCEROL\"]\n",
    "YPRIBOSE = pheno_yeast_matrix[\"YPRIBOSE\"]\n",
    "YPSORBITOL = pheno_yeast_matrix[\"YPSORBITOL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "h1 = 100\n",
    "h2 = 50\n",
    "h3 = 20\n",
    "output_size = 1\n",
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 200\n",
    "split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_0 = torch.tensor(yeast_gene_matrix_list[0].values, dtype = torch.float)\n",
    "answer_0 = torch.tensor(YPD6AU, dtype = torch.float)\n",
    "train_set_0 = dt.TensorDataset(training_data_0, answer_0)\n",
    "train_size = int(split * len(train_set_0))\n",
    "test_size = len(train_set_0) - train_size \n",
    "train_dataset_0, test_dataset_0 = dt.random_split(train_set_0, [train_size, test_size])\n",
    "train_loader_0 = dt.DataLoader(dataset=train_dataset_0, batch_size=batch_size)\n",
    "test_loader_0 = dt.DataLoader(dataset=test_dataset_0, batch_size=batch_size)\n",
    "\n",
    "\n",
    "training_data_1 = torch.tensor(yeast_gene_matrix_list[1].values, dtype = torch.float)\n",
    "answer_1 = torch.tensor(YPDBENOMYL200, dtype = torch.float)\n",
    "train_set_1 = dt.TensorDataset(training_data_1, answer_1)\n",
    "train_dataset_1, test_dataset_1 = dt.random_split(train_set_1, [train_size, test_size])\n",
    "train_loader_1 = dt.DataLoader(dataset=train_dataset_1, batch_size=batch_size)\n",
    "test_loader_1 = dt.DataLoader(dataset=test_dataset_1, batch_size=batch_size)\n",
    "\n",
    "\n",
    "training_data_2 = torch.tensor(yeast_gene_matrix_list[2].values, dtype = torch.float)\n",
    "answer_2 = torch.tensor(YPDNACL1M, dtype = torch.float)\n",
    "train_set_2 = dt.TensorDataset(training_data_2, answer_2)\n",
    "train_dataset_2, test_dataset_2 = dt.random_split(train_set_2, [train_size, test_size])\n",
    "train_loader_2 = dt.DataLoader(dataset=train_dataset_2, batch_size=batch_size)\n",
    "test_loader_2 = dt.DataLoader(dataset=test_dataset_2, batch_size=batch_size)\n",
    "\n",
    "\n",
    "training_data_3 = torch.tensor(yeast_gene_matrix_list[3].values, dtype = torch.float)\n",
    "answer_3 = torch.tensor(YPDSDS, dtype = torch.float)\n",
    "train_set_3 = dt.TensorDataset(training_data_3, answer_3)\n",
    "train_dataset_3, test_dataset_3 = dt.random_split(train_set_3, [train_size, test_size])\n",
    "train_loader_3 = dt.DataLoader(dataset=train_dataset_3, batch_size=batch_size)\n",
    "test_loader_3 = dt.DataLoader(dataset=test_dataset_3, batch_size=batch_size)\n",
    "\n",
    "training_data_5 = torch.tensor(yeast_gene_matrix_list[4].values, dtype = torch.float)\n",
    "answer_5 = torch.tensor(YPGALACTOSE, dtype = torch.float)\n",
    "train_set_5 = dt.TensorDataset(training_data_5, answer_5)\n",
    "train_dataset_5, test_dataset_5 = dt.random_split(train_set_5, [train_size, test_size])\n",
    "train_loader_5 = dt.DataLoader(dataset=train_dataset_5, batch_size=batch_size)\n",
    "test_loader_5 = dt.DataLoader(dataset=test_dataset_5, batch_size=batch_size)\n",
    "\n",
    "training_data_6 = torch.tensor(yeast_gene_matrix_list[5].values, dtype = torch.float)\n",
    "answer_6 = torch.tensor(YPGLYCEROL, dtype = torch.float)\n",
    "train_set_6 = dt.TensorDataset(training_data_6, answer_6)\n",
    "train_dataset_6, test_dataset_6 = dt.random_split(train_set_6, [train_size, test_size])\n",
    "train_loader_6 = dt.DataLoader(dataset=train_dataset_6, batch_size=batch_size)\n",
    "test_loader_6 = dt.DataLoader(dataset=test_dataset_6, batch_size=batch_size)\n",
    "\n",
    "training_data_7 = torch.tensor(yeast_gene_matrix_list[6].values, dtype = torch.float)\n",
    "answer_7 = torch.tensor(YPRIBOSE, dtype = torch.float)\n",
    "train_set_7 = dt.TensorDataset(training_data_7, answer_7)\n",
    "train_dataset_7, test_dataset_7 = dt.random_split(train_set_7, [train_size, test_size])\n",
    "train_loader_7 = dt.DataLoader(dataset=train_dataset_7, batch_size=batch_size)\n",
    "test_loader_7 = dt.DataLoader(dataset=test_dataset_7, batch_size=batch_size)\n",
    "\n",
    "training_data_8 = torch.tensor(yeast_gene_matrix_list[7].values, dtype = torch.float)\n",
    "answer_8 = torch.tensor(YPSORBITOL, dtype = torch.float)\n",
    "train_set_8 = dt.TensorDataset(training_data_8, answer_8)\n",
    "train_dataset_8, test_dataset_8 = dt.random_split(train_set_8, [train_size, test_size])\n",
    "train_loader_8 = dt.DataLoader(dataset=train_dataset_8, batch_size=batch_size)\n",
    "test_loader_8 = dt.DataLoader(dataset=test_dataset_8, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_list = [train_loader_0, train_loader_1, train_loader_2, train_loader_3, train_loader_5, train_loader_6, train_loader_7, train_loader_8]\n",
    "test_loader_list = [test_loader_0, test_loader_1, test_loader_2, test_loader_3, test_loader_5, test_loader_6, test_loader_7, test_loader_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n"
     ]
    }
   ],
   "source": [
    "print(int(split * len(train_set_0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: -0.184503 \n",
      " P-Value: 0.009821\n",
      "Epoch [1/100], Loss: 0.0644\n",
      "Correlation: -0.128295 \n",
      " P-Value: 0.073869\n",
      "Epoch [2/100], Loss: 0.0340\n",
      "Correlation: 0.055252 \n",
      " P-Value: 0.442977\n",
      "Epoch [3/100], Loss: 0.0259\n",
      "Correlation: 0.187590 \n",
      " P-Value: 0.008638\n",
      "Epoch [4/100], Loss: 0.0233\n",
      "Correlation: 0.142453 \n",
      " P-Value: 0.046967\n",
      "Epoch [5/100], Loss: 0.0232\n",
      "Correlation: 0.018393 \n",
      " P-Value: 0.798536\n",
      "Epoch [6/100], Loss: 0.0219\n",
      "Correlation: -0.093430 \n",
      " P-Value: 0.193896\n",
      "Epoch [7/100], Loss: 0.0216\n",
      "Correlation: -0.034808 \n",
      " P-Value: 0.629035\n",
      "Epoch [8/100], Loss: 0.0210\n",
      "Correlation: -0.025308 \n",
      " P-Value: 0.725454\n",
      "Epoch [9/100], Loss: 0.0207\n",
      "Correlation: -0.053859 \n",
      " P-Value: 0.454577\n",
      "Epoch [10/100], Loss: 0.0207\n",
      "Correlation: -0.026486 \n",
      " P-Value: 0.713218\n",
      "Epoch [11/100], Loss: 0.0203\n",
      "Correlation: -0.066769 \n",
      " P-Value: 0.353707\n",
      "Epoch [12/100], Loss: 0.0203\n",
      "Correlation: -0.006655 \n",
      " P-Value: 0.926434\n",
      "Epoch [13/100], Loss: 0.0202\n",
      "Correlation: -0.034970 \n",
      " P-Value: 0.627435\n",
      "Epoch [14/100], Loss: 0.0201\n",
      "Correlation: -0.027758 \n",
      " P-Value: 0.700088\n",
      "Epoch [15/100], Loss: 0.0200\n",
      "Correlation: -0.028656 \n",
      " P-Value: 0.690874\n",
      "Epoch [16/100], Loss: 0.0199\n",
      "Correlation: -0.006048 \n",
      " P-Value: 0.933108\n",
      "Epoch [17/100], Loss: 0.0199\n",
      "Correlation: -0.009520 \n",
      " P-Value: 0.894933\n",
      "Epoch [18/100], Loss: 0.0199\n",
      "Correlation: -0.008611 \n",
      " P-Value: 0.904900\n",
      "Epoch [19/100], Loss: 0.0199\n",
      "Correlation: -0.008332 \n",
      " P-Value: 0.907955\n",
      "Epoch [20/100], Loss: 0.0198\n",
      "Correlation: 0.004363 \n",
      " P-Value: 0.951682\n",
      "Epoch [21/100], Loss: 0.0198\n",
      "Correlation: 0.012978 \n",
      " P-Value: 0.857091\n",
      "Epoch [22/100], Loss: 0.0198\n",
      "Correlation: 0.005894 \n",
      " P-Value: 0.934870\n",
      "Epoch [23/100], Loss: 0.0198\n",
      "Correlation: 0.003140 \n",
      " P-Value: 0.965189\n",
      "Epoch [24/100], Loss: 0.0197\n",
      "Correlation: 0.003140 \n",
      " P-Value: 0.965189\n",
      "Epoch [25/100], Loss: 0.0197\n",
      "Correlation: 0.009216 \n",
      " P-Value: 0.898286\n",
      "Epoch [26/100], Loss: 0.0197\n",
      "Correlation: 0.007740 \n",
      " P-Value: 0.914483\n",
      "Epoch [27/100], Loss: 0.0197\n",
      "Correlation: 0.009809 \n",
      " P-Value: 0.891752\n",
      "Epoch [28/100], Loss: 0.0197\n",
      "Correlation: 0.010531 \n",
      " P-Value: 0.883818\n",
      "Epoch [29/100], Loss: 0.0197\n",
      "Correlation: 0.010337 \n",
      " P-Value: 0.885946\n",
      "Epoch [30/100], Loss: 0.0197\n",
      "Correlation: 0.005206 \n",
      " P-Value: 0.942399\n",
      "Epoch [31/100], Loss: 0.0196\n",
      "Correlation: 0.014094 \n",
      " P-Value: 0.844949\n",
      "Epoch [32/100], Loss: 0.0196\n",
      "Correlation: 0.018695 \n",
      " P-Value: 0.795334\n",
      "Epoch [33/100], Loss: 0.0196\n",
      "Correlation: 0.009667 \n",
      " P-Value: 0.893296\n",
      "Epoch [34/100], Loss: 0.0196\n",
      "Correlation: 0.008078 \n",
      " P-Value: 0.910711\n",
      "Epoch [35/100], Loss: 0.0196\n",
      "Correlation: 0.005479 \n",
      " P-Value: 0.939383\n",
      "Epoch [36/100], Loss: 0.0196\n",
      "Correlation: 0.011128 \n",
      " P-Value: 0.877311\n",
      "Epoch [37/100], Loss: 0.0196\n",
      "Correlation: 0.011247 \n",
      " P-Value: 0.876000\n",
      "Epoch [38/100], Loss: 0.0196\n",
      "Correlation: 0.014699 \n",
      " P-Value: 0.838411\n",
      "Epoch [39/100], Loss: 0.0196\n",
      "Correlation: 0.010189 \n",
      " P-Value: 0.887601\n",
      "Epoch [40/100], Loss: 0.0196\n",
      "Correlation: 0.018622 \n",
      " P-Value: 0.796104\n",
      "Epoch [41/100], Loss: 0.0196\n",
      "Correlation: 0.012284 \n",
      " P-Value: 0.864654\n",
      "Epoch [42/100], Loss: 0.0196\n",
      "Correlation: 0.010609 \n",
      " P-Value: 0.882954\n",
      "Epoch [43/100], Loss: 0.0195\n",
      "Correlation: 0.009363 \n",
      " P-Value: 0.896631\n",
      "Epoch [44/100], Loss: 0.0195\n",
      "Correlation: 0.013695 \n",
      " P-Value: 0.849285\n",
      "Epoch [45/100], Loss: 0.0195\n",
      "Correlation: 0.014822 \n",
      " P-Value: 0.837075\n",
      "Epoch [46/100], Loss: 0.0195\n",
      "Correlation: 0.016945 \n",
      " P-Value: 0.814102\n",
      "Epoch [47/100], Loss: 0.0195\n",
      "Correlation: 0.012278 \n",
      " P-Value: 0.864733\n",
      "Epoch [48/100], Loss: 0.0195\n",
      "Correlation: 0.010326 \n",
      " P-Value: 0.886072\n",
      "Epoch [49/100], Loss: 0.0195\n",
      "Correlation: 0.009912 \n",
      " P-Value: 0.890592\n",
      "Epoch [50/100], Loss: 0.0195\n",
      "Correlation: 0.014460 \n",
      " P-Value: 0.840983\n",
      "Epoch [51/100], Loss: 0.0195\n",
      "Correlation: 0.016341 \n",
      " P-Value: 0.820647\n",
      "Epoch [52/100], Loss: 0.0195\n",
      "Correlation: 0.008116 \n",
      " P-Value: 0.910346\n",
      "Epoch [53/100], Loss: 0.0195\n",
      "Correlation: 0.014375 \n",
      " P-Value: 0.841903\n",
      "Epoch [54/100], Loss: 0.0195\n",
      "Correlation: 0.014019 \n",
      " P-Value: 0.845778\n",
      "Epoch [55/100], Loss: 0.0195\n",
      "Correlation: 0.006514 \n",
      " P-Value: 0.927981\n",
      "Epoch [56/100], Loss: 0.0195\n",
      "Correlation: 0.016808 \n",
      " P-Value: 0.815605\n",
      "Epoch [57/100], Loss: 0.0195\n",
      "Correlation: 0.013988 \n",
      " P-Value: 0.846125\n",
      "Epoch [58/100], Loss: 0.0195\n",
      "Correlation: 0.008317 \n",
      " P-Value: 0.908113\n",
      "Epoch [59/100], Loss: 0.0195\n",
      "Correlation: 0.016366 \n",
      " P-Value: 0.820352\n",
      "Epoch [60/100], Loss: 0.0195\n",
      "Correlation: 0.012665 \n",
      " P-Value: 0.860511\n",
      "Epoch [61/100], Loss: 0.0195\n",
      "Correlation: 0.009976 \n",
      " P-Value: 0.889902\n",
      "Epoch [62/100], Loss: 0.0195\n",
      "Correlation: 0.015789 \n",
      " P-Value: 0.826578\n",
      "Epoch [63/100], Loss: 0.0195\n",
      "Correlation: 0.012818 \n",
      " P-Value: 0.858854\n",
      "Epoch [64/100], Loss: 0.0195\n",
      "Correlation: 0.011836 \n",
      " P-Value: 0.869564\n",
      "Epoch [65/100], Loss: 0.0195\n",
      "Correlation: 0.012309 \n",
      " P-Value: 0.864390\n",
      "Epoch [66/100], Loss: 0.0195\n",
      "Correlation: 0.011179 \n",
      " P-Value: 0.876726\n",
      "Epoch [67/100], Loss: 0.0194\n",
      "Correlation: 0.011174 \n",
      " P-Value: 0.876814\n",
      "Epoch [68/100], Loss: 0.0194\n",
      "Correlation: 0.013158 \n",
      " P-Value: 0.855154\n",
      "Epoch [69/100], Loss: 0.0194\n",
      "Correlation: 0.009323 \n",
      " P-Value: 0.897051\n",
      "Epoch [70/100], Loss: 0.0194\n",
      "Correlation: 0.012209 \n",
      " P-Value: 0.865477\n",
      "Epoch [71/100], Loss: 0.0194\n",
      "Correlation: 0.013996 \n",
      " P-Value: 0.846033\n",
      "Epoch [72/100], Loss: 0.0194\n",
      "Correlation: 0.009420 \n",
      " P-Value: 0.896038\n",
      "Epoch [73/100], Loss: 0.0194\n",
      "Correlation: 0.015201 \n",
      " P-Value: 0.832967\n",
      "Epoch [74/100], Loss: 0.0194\n",
      "Correlation: 0.013244 \n",
      " P-Value: 0.854195\n",
      "Epoch [75/100], Loss: 0.0194\n",
      "Correlation: 0.008305 \n",
      " P-Value: 0.908270\n",
      "Epoch [76/100], Loss: 0.0194\n",
      "Correlation: 0.017807 \n",
      " P-Value: 0.804842\n",
      "Epoch [77/100], Loss: 0.0194\n",
      "Correlation: 0.014256 \n",
      " P-Value: 0.843191\n",
      "Epoch [78/100], Loss: 0.0194\n",
      "Correlation: 0.008407 \n",
      " P-Value: 0.907133\n",
      "Epoch [79/100], Loss: 0.0194\n",
      "Correlation: 0.017269 \n",
      " P-Value: 0.810633\n",
      "Epoch [80/100], Loss: 0.0194\n",
      "Correlation: 0.014387 \n",
      " P-Value: 0.841768\n",
      "Epoch [81/100], Loss: 0.0194\n",
      "Correlation: 0.008190 \n",
      " P-Value: 0.909502\n",
      "Epoch [82/100], Loss: 0.0194\n",
      "Correlation: 0.015484 \n",
      " P-Value: 0.829897\n",
      "Epoch [83/100], Loss: 0.0194\n",
      "Correlation: 0.018130 \n",
      " P-Value: 0.801371\n",
      "Epoch [84/100], Loss: 0.0194\n",
      "Correlation: 0.008927 \n",
      " P-Value: 0.901426\n",
      "Epoch [85/100], Loss: 0.0194\n",
      "Correlation: 0.014011 \n",
      " P-Value: 0.845848\n",
      "Epoch [86/100], Loss: 0.0194\n",
      "Correlation: 0.015138 \n",
      " P-Value: 0.833647\n",
      "Epoch [87/100], Loss: 0.0194\n",
      "Correlation: 0.010709 \n",
      " P-Value: 0.881882\n",
      "Epoch [88/100], Loss: 0.0194\n",
      "Correlation: 0.015490 \n",
      " P-Value: 0.829813\n",
      "Epoch [89/100], Loss: 0.0194\n",
      "Correlation: 0.013715 \n",
      " P-Value: 0.849072\n",
      "Epoch [90/100], Loss: 0.0194\n",
      "Correlation: 0.010927 \n",
      " P-Value: 0.879500\n",
      "Epoch [91/100], Loss: 0.0194\n",
      "Correlation: 0.015365 \n",
      " P-Value: 0.831172\n",
      "Epoch [92/100], Loss: 0.0194\n",
      "Correlation: 0.012946 \n",
      " P-Value: 0.857442\n",
      "Epoch [93/100], Loss: 0.0194\n",
      "Correlation: 0.013784 \n",
      " P-Value: 0.848318\n",
      "Epoch [94/100], Loss: 0.0194\n",
      "Correlation: 0.012947 \n",
      " P-Value: 0.857442\n",
      "Epoch [95/100], Loss: 0.0194\n",
      "Correlation: 0.014730 \n",
      " P-Value: 0.838059\n",
      "Epoch [96/100], Loss: 0.0194\n",
      "Correlation: 0.012734 \n",
      " P-Value: 0.859769\n",
      "Epoch [97/100], Loss: 0.0194\n",
      "Correlation: 0.014799 \n",
      " P-Value: 0.837294\n",
      "Epoch [98/100], Loss: 0.0194\n",
      "Correlation: 0.015072 \n",
      " P-Value: 0.834353\n",
      "Epoch [99/100], Loss: 0.0194\n",
      "Correlation: 0.014066 \n",
      " P-Value: 0.845271\n",
      "Epoch [100/100], Loss: 0.0194\n",
      "Max correlation is: [0.18759023] at 3th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 251\n",
    "net_0 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_0.train_net(num_epochs, train_loader_list[0], test_loader_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.087870 \n",
      " P-Value: 0.221896\n",
      "Epoch [1/100], Loss: 0.0129\n",
      "Correlation: 0.149757 \n",
      " P-Value: 0.036655\n",
      "Epoch [2/100], Loss: 0.0102\n",
      "Correlation: 0.108531 \n",
      " P-Value: 0.130973\n",
      "Epoch [3/100], Loss: 0.0099\n",
      "Correlation: 0.110968 \n",
      " P-Value: 0.122494\n",
      "Epoch [4/100], Loss: 0.0098\n",
      "Correlation: 0.096985 \n",
      " P-Value: 0.177404\n",
      "Epoch [5/100], Loss: 0.0098\n",
      "Correlation: 0.103985 \n",
      " P-Value: 0.147992\n",
      "Epoch [6/100], Loss: 0.0098\n",
      "Correlation: 0.091068 \n",
      " P-Value: 0.205460\n",
      "Epoch [7/100], Loss: 0.0097\n",
      "Correlation: 0.076196 \n",
      " P-Value: 0.289727\n",
      "Epoch [8/100], Loss: 0.0097\n",
      "Correlation: 0.098613 \n",
      " P-Value: 0.170206\n",
      "Epoch [9/100], Loss: 0.0097\n",
      "Correlation: 0.116476 \n",
      " P-Value: 0.104896\n",
      "Epoch [10/100], Loss: 0.0097\n",
      "Correlation: 0.129373 \n",
      " P-Value: 0.071457\n",
      "Epoch [11/100], Loss: 0.0097\n",
      "Correlation: 0.134594 \n",
      " P-Value: 0.060660\n",
      "Epoch [12/100], Loss: 0.0097\n",
      "Correlation: 0.141760 \n",
      " P-Value: 0.048059\n",
      "Epoch [13/100], Loss: 0.0097\n",
      "Correlation: 0.147578 \n",
      " P-Value: 0.039508\n",
      "Epoch [14/100], Loss: 0.0097\n",
      "Correlation: 0.153466 \n",
      " P-Value: 0.032196\n",
      "Epoch [15/100], Loss: 0.0097\n",
      "Correlation: 0.158103 \n",
      " P-Value: 0.027279\n",
      "Epoch [16/100], Loss: 0.0097\n",
      "Correlation: 0.155890 \n",
      " P-Value: 0.029539\n",
      "Epoch [17/100], Loss: 0.0097\n",
      "Correlation: 0.153360 \n",
      " P-Value: 0.032317\n",
      "Epoch [18/100], Loss: 0.0097\n",
      "Correlation: 0.150550 \n",
      " P-Value: 0.035660\n",
      "Epoch [19/100], Loss: 0.0097\n",
      "Correlation: 0.151267 \n",
      " P-Value: 0.034781\n",
      "Epoch [20/100], Loss: 0.0097\n",
      "Correlation: 0.152802 \n",
      " P-Value: 0.032959\n",
      "Epoch [21/100], Loss: 0.0097\n",
      "Correlation: 0.157543 \n",
      " P-Value: 0.027837\n",
      "Epoch [22/100], Loss: 0.0097\n",
      "Correlation: 0.161179 \n",
      " P-Value: 0.024385\n",
      "Epoch [23/100], Loss: 0.0097\n",
      "Correlation: 0.161798 \n",
      " P-Value: 0.023835\n",
      "Epoch [24/100], Loss: 0.0097\n",
      "Correlation: 0.161253 \n",
      " P-Value: 0.024319\n",
      "Epoch [25/100], Loss: 0.0097\n",
      "Correlation: 0.159853 \n",
      " P-Value: 0.025598\n",
      "Epoch [26/100], Loss: 0.0097\n",
      "Correlation: 0.159511 \n",
      " P-Value: 0.025920\n",
      "Epoch [27/100], Loss: 0.0097\n",
      "Correlation: 0.160128 \n",
      " P-Value: 0.025343\n",
      "Epoch [28/100], Loss: 0.0097\n",
      "Correlation: 0.161827 \n",
      " P-Value: 0.023810\n",
      "Epoch [29/100], Loss: 0.0097\n",
      "Correlation: 0.161694 \n",
      " P-Value: 0.023927\n",
      "Epoch [30/100], Loss: 0.0097\n",
      "Correlation: 0.161019 \n",
      " P-Value: 0.024529\n",
      "Epoch [31/100], Loss: 0.0097\n",
      "Correlation: 0.160450 \n",
      " P-Value: 0.025046\n",
      "Epoch [32/100], Loss: 0.0097\n",
      "Correlation: 0.161410 \n",
      " P-Value: 0.024178\n",
      "Epoch [33/100], Loss: 0.0097\n",
      "Correlation: 0.161290 \n",
      " P-Value: 0.024285\n",
      "Epoch [34/100], Loss: 0.0097\n",
      "Correlation: 0.161585 \n",
      " P-Value: 0.024023\n",
      "Epoch [35/100], Loss: 0.0097\n",
      "Correlation: 0.162099 \n",
      " P-Value: 0.023572\n",
      "Epoch [36/100], Loss: 0.0097\n",
      "Correlation: 0.163764 \n",
      " P-Value: 0.022161\n",
      "Epoch [37/100], Loss: 0.0097\n",
      "Correlation: 0.164799 \n",
      " P-Value: 0.021320\n",
      "Epoch [38/100], Loss: 0.0097\n",
      "Correlation: 0.166201 \n",
      " P-Value: 0.020226\n",
      "Epoch [39/100], Loss: 0.0097\n",
      "Correlation: 0.166837 \n",
      " P-Value: 0.019746\n",
      "Epoch [40/100], Loss: 0.0097\n",
      "Correlation: 0.165915 \n",
      " P-Value: 0.020445\n",
      "Epoch [41/100], Loss: 0.0097\n",
      "Correlation: 0.164973 \n",
      " P-Value: 0.021182\n",
      "Epoch [42/100], Loss: 0.0097\n",
      "Correlation: 0.166099 \n",
      " P-Value: 0.020304\n",
      "Epoch [43/100], Loss: 0.0097\n",
      "Correlation: 0.169228 \n",
      " P-Value: 0.018029\n",
      "Epoch [44/100], Loss: 0.0097\n",
      "Correlation: 0.168395 \n",
      " P-Value: 0.018611\n",
      "Epoch [45/100], Loss: 0.0097\n",
      "Correlation: 0.166893 \n",
      " P-Value: 0.019705\n",
      "Epoch [46/100], Loss: 0.0097\n",
      "Correlation: 0.165668 \n",
      " P-Value: 0.020636\n",
      "Epoch [47/100], Loss: 0.0097\n",
      "Correlation: 0.163303 \n",
      " P-Value: 0.022544\n",
      "Epoch [48/100], Loss: 0.0097\n",
      "Correlation: 0.163187 \n",
      " P-Value: 0.022641\n",
      "Epoch [49/100], Loss: 0.0097\n",
      "Correlation: 0.165689 \n",
      " P-Value: 0.020620\n",
      "Epoch [50/100], Loss: 0.0097\n",
      "Correlation: 0.165086 \n",
      " P-Value: 0.021092\n",
      "Epoch [51/100], Loss: 0.0097\n",
      "Correlation: 0.162254 \n",
      " P-Value: 0.023437\n",
      "Epoch [52/100], Loss: 0.0097\n",
      "Correlation: 0.159801 \n",
      " P-Value: 0.025647\n",
      "Epoch [53/100], Loss: 0.0097\n",
      "Correlation: 0.158565 \n",
      " P-Value: 0.026827\n",
      "Epoch [54/100], Loss: 0.0097\n",
      "Correlation: 0.158429 \n",
      " P-Value: 0.026959\n",
      "Epoch [55/100], Loss: 0.0097\n",
      "Correlation: 0.158790 \n",
      " P-Value: 0.026608\n",
      "Epoch [56/100], Loss: 0.0097\n",
      "Correlation: 0.159175 \n",
      " P-Value: 0.026239\n",
      "Epoch [57/100], Loss: 0.0097\n",
      "Correlation: 0.159653 \n",
      " P-Value: 0.025785\n",
      "Epoch [58/100], Loss: 0.0097\n",
      "Correlation: 0.160229 \n",
      " P-Value: 0.025250\n",
      "Epoch [59/100], Loss: 0.0097\n",
      "Correlation: 0.160901 \n",
      " P-Value: 0.024635\n",
      "Epoch [60/100], Loss: 0.0097\n",
      "Correlation: 0.162102 \n",
      " P-Value: 0.023570\n",
      "Epoch [61/100], Loss: 0.0097\n",
      "Correlation: 0.162692 \n",
      " P-Value: 0.023061\n",
      "Epoch [62/100], Loss: 0.0097\n",
      "Correlation: 0.161419 \n",
      " P-Value: 0.024170\n",
      "Epoch [63/100], Loss: 0.0097\n",
      "Correlation: 0.160246 \n",
      " P-Value: 0.025234\n",
      "Epoch [64/100], Loss: 0.0097\n",
      "Correlation: 0.161705 \n",
      " P-Value: 0.023917\n",
      "Epoch [65/100], Loss: 0.0097\n",
      "Correlation: 0.161754 \n",
      " P-Value: 0.023874\n",
      "Epoch [66/100], Loss: 0.0097\n",
      "Correlation: 0.158632 \n",
      " P-Value: 0.026761\n",
      "Epoch [67/100], Loss: 0.0097\n",
      "Correlation: 0.157078 \n",
      " P-Value: 0.028307\n",
      "Epoch [68/100], Loss: 0.0097\n",
      "Correlation: 0.158058 \n",
      " P-Value: 0.027323\n",
      "Epoch [69/100], Loss: 0.0097\n",
      "Correlation: 0.158202 \n",
      " P-Value: 0.027181\n",
      "Epoch [70/100], Loss: 0.0097\n",
      "Correlation: 0.157006 \n",
      " P-Value: 0.028380\n",
      "Epoch [71/100], Loss: 0.0097\n",
      "Correlation: 0.154438 \n",
      " P-Value: 0.031108\n",
      "Epoch [72/100], Loss: 0.0097\n",
      "Correlation: 0.154729 \n",
      " P-Value: 0.030788\n",
      "Epoch [73/100], Loss: 0.0097\n",
      "Correlation: 0.154978 \n",
      " P-Value: 0.030516\n",
      "Epoch [74/100], Loss: 0.0097\n",
      "Correlation: 0.153456 \n",
      " P-Value: 0.032207\n",
      "Epoch [75/100], Loss: 0.0097\n",
      "Correlation: 0.154052 \n",
      " P-Value: 0.031537\n",
      "Epoch [76/100], Loss: 0.0097\n",
      "Correlation: 0.151752 \n",
      " P-Value: 0.034196\n",
      "Epoch [77/100], Loss: 0.0097\n",
      "Correlation: 0.152160 \n",
      " P-Value: 0.033710\n",
      "Epoch [78/100], Loss: 0.0097\n",
      "Correlation: 0.152439 \n",
      " P-Value: 0.033382\n",
      "Epoch [79/100], Loss: 0.0097\n",
      "Correlation: 0.152163 \n",
      " P-Value: 0.033707\n",
      "Epoch [80/100], Loss: 0.0097\n",
      "Correlation: 0.152682 \n",
      " P-Value: 0.033098\n",
      "Epoch [81/100], Loss: 0.0097\n",
      "Correlation: 0.147488 \n",
      " P-Value: 0.039630\n",
      "Epoch [82/100], Loss: 0.0097\n",
      "Correlation: 0.146926 \n",
      " P-Value: 0.040398\n",
      "Epoch [83/100], Loss: 0.0097\n",
      "Correlation: 0.151531 \n",
      " P-Value: 0.034461\n",
      "Epoch [84/100], Loss: 0.0097\n",
      "Correlation: 0.151771 \n",
      " P-Value: 0.034173\n",
      "Epoch [85/100], Loss: 0.0097\n",
      "Correlation: 0.144865 \n",
      " P-Value: 0.043322\n",
      "Epoch [86/100], Loss: 0.0097\n",
      "Correlation: 0.145447 \n",
      " P-Value: 0.042479\n",
      "Epoch [87/100], Loss: 0.0097\n",
      "Correlation: 0.147639 \n",
      " P-Value: 0.039426\n",
      "Epoch [88/100], Loss: 0.0097\n",
      "Correlation: 0.148290 \n",
      " P-Value: 0.038556\n",
      "Epoch [89/100], Loss: 0.0097\n",
      "Correlation: 0.148079 \n",
      " P-Value: 0.038836\n",
      "Epoch [90/100], Loss: 0.0097\n",
      "Correlation: 0.149088 \n",
      " P-Value: 0.037512\n",
      "Epoch [91/100], Loss: 0.0097\n",
      "Correlation: 0.148314 \n",
      " P-Value: 0.038524\n",
      "Epoch [92/100], Loss: 0.0097\n",
      "Correlation: 0.143542 \n",
      " P-Value: 0.045291\n",
      "Epoch [93/100], Loss: 0.0097\n",
      "Correlation: 0.146276 \n",
      " P-Value: 0.041301\n",
      "Epoch [94/100], Loss: 0.0097\n",
      "Correlation: 0.146897 \n",
      " P-Value: 0.040438\n",
      "Epoch [95/100], Loss: 0.0097\n",
      "Correlation: 0.141552 \n",
      " P-Value: 0.048392\n",
      "Epoch [96/100], Loss: 0.0097\n",
      "Correlation: 0.141317 \n",
      " P-Value: 0.048770\n",
      "Epoch [97/100], Loss: 0.0097\n",
      "Correlation: 0.145809 \n",
      " P-Value: 0.041961\n",
      "Epoch [98/100], Loss: 0.0097\n",
      "Correlation: 0.143942 \n",
      " P-Value: 0.044688\n",
      "Epoch [99/100], Loss: 0.0097\n",
      "Correlation: 0.143045 \n",
      " P-Value: 0.046049\n",
      "Epoch [100/100], Loss: 0.0097\n",
      "Max correlation is: [0.16922806] at 43th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 242\n",
    "net_1 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_1.train_net(num_epochs, train_loader_list[1], test_loader_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.026885 \n",
      " P-Value: 0.709083\n",
      "Epoch [1/100], Loss: 0.0971\n",
      "Correlation: 0.073389 \n",
      " P-Value: 0.307909\n",
      "Epoch [2/100], Loss: 0.0294\n",
      "Correlation: 0.165474 \n",
      " P-Value: 0.020788\n",
      "Epoch [3/100], Loss: 0.0195\n",
      "Correlation: 0.176000 \n",
      " P-Value: 0.013850\n",
      "Epoch [4/100], Loss: 0.0171\n",
      "Correlation: 0.221100 \n",
      " P-Value: 0.001895\n",
      "Epoch [5/100], Loss: 0.0161\n",
      "Correlation: 0.229509 \n",
      " P-Value: 0.001249\n",
      "Epoch [6/100], Loss: 0.0153\n",
      "Correlation: 0.206251 \n",
      " P-Value: 0.003818\n",
      "Epoch [7/100], Loss: 0.0147\n",
      "Correlation: 0.198366 \n",
      " P-Value: 0.005438\n",
      "Epoch [8/100], Loss: 0.0140\n",
      "Correlation: 0.162026 \n",
      " P-Value: 0.023636\n",
      "Epoch [9/100], Loss: 0.0135\n",
      "Correlation: 0.168592 \n",
      " P-Value: 0.018472\n",
      "Epoch [10/100], Loss: 0.0128\n",
      "Correlation: 0.161874 \n",
      " P-Value: 0.023769\n",
      "Epoch [11/100], Loss: 0.0124\n",
      "Correlation: 0.156538 \n",
      " P-Value: 0.028862\n",
      "Epoch [12/100], Loss: 0.0119\n",
      "Correlation: 0.119229 \n",
      " P-Value: 0.096880\n",
      "Epoch [13/100], Loss: 0.0116\n",
      "Correlation: 0.074973 \n",
      " P-Value: 0.297561\n",
      "Epoch [14/100], Loss: 0.0113\n",
      "Correlation: 0.057292 \n",
      " P-Value: 0.426290\n",
      "Epoch [15/100], Loss: 0.0112\n",
      "Correlation: 0.057481 \n",
      " P-Value: 0.424767\n",
      "Epoch [16/100], Loss: 0.0111\n",
      "Correlation: 0.066012 \n",
      " P-Value: 0.359205\n",
      "Epoch [17/100], Loss: 0.0109\n",
      "Correlation: 0.073739 \n",
      " P-Value: 0.305607\n",
      "Epoch [18/100], Loss: 0.0109\n",
      "Correlation: 0.080450 \n",
      " P-Value: 0.263562\n",
      "Epoch [19/100], Loss: 0.0108\n",
      "Correlation: 0.086767 \n",
      " P-Value: 0.227772\n",
      "Epoch [20/100], Loss: 0.0107\n",
      "Correlation: 0.087785 \n",
      " P-Value: 0.222340\n",
      "Epoch [21/100], Loss: 0.0107\n",
      "Correlation: 0.084697 \n",
      " P-Value: 0.239097\n",
      "Epoch [22/100], Loss: 0.0106\n",
      "Correlation: 0.084929 \n",
      " P-Value: 0.237810\n",
      "Epoch [23/100], Loss: 0.0106\n",
      "Correlation: 0.086011 \n",
      " P-Value: 0.231863\n",
      "Epoch [24/100], Loss: 0.0105\n",
      "Correlation: 0.088284 \n",
      " P-Value: 0.219711\n",
      "Epoch [25/100], Loss: 0.0105\n",
      "Correlation: 0.089387 \n",
      " P-Value: 0.213986\n",
      "Epoch [26/100], Loss: 0.0105\n",
      "Correlation: 0.088498 \n",
      " P-Value: 0.218594\n",
      "Epoch [27/100], Loss: 0.0104\n",
      "Correlation: 0.087594 \n",
      " P-Value: 0.223356\n",
      "Epoch [28/100], Loss: 0.0104\n",
      "Correlation: 0.087043 \n",
      " P-Value: 0.226292\n",
      "Epoch [29/100], Loss: 0.0104\n",
      "Correlation: 0.085055 \n",
      " P-Value: 0.237115\n",
      "Epoch [30/100], Loss: 0.0104\n",
      "Correlation: 0.082369 \n",
      " P-Value: 0.252303\n",
      "Epoch [31/100], Loss: 0.0103\n",
      "Correlation: 0.080538 \n",
      " P-Value: 0.263040\n",
      "Epoch [32/100], Loss: 0.0103\n",
      "Correlation: 0.079471 \n",
      " P-Value: 0.269432\n",
      "Epoch [33/100], Loss: 0.0103\n",
      "Correlation: 0.078476 \n",
      " P-Value: 0.275498\n",
      "Epoch [34/100], Loss: 0.0103\n",
      "Correlation: 0.077588 \n",
      " P-Value: 0.280979\n",
      "Epoch [35/100], Loss: 0.0103\n",
      "Correlation: 0.077129 \n",
      " P-Value: 0.283847\n",
      "Epoch [36/100], Loss: 0.0103\n",
      "Correlation: 0.076954 \n",
      " P-Value: 0.284946\n",
      "Epoch [37/100], Loss: 0.0103\n",
      "Correlation: 0.076322 \n",
      " P-Value: 0.288927\n",
      "Epoch [38/100], Loss: 0.0102\n",
      "Correlation: 0.074669 \n",
      " P-Value: 0.299527\n",
      "Epoch [39/100], Loss: 0.0102\n",
      "Correlation: 0.072490 \n",
      " P-Value: 0.313891\n",
      "Epoch [40/100], Loss: 0.0102\n",
      "Correlation: 0.070063 \n",
      " P-Value: 0.330417\n",
      "Epoch [41/100], Loss: 0.0102\n",
      "Correlation: 0.068143 \n",
      " P-Value: 0.343867\n",
      "Epoch [42/100], Loss: 0.0102\n",
      "Correlation: 0.067165 \n",
      " P-Value: 0.350856\n",
      "Epoch [43/100], Loss: 0.0102\n",
      "Correlation: 0.067201 \n",
      " P-Value: 0.350591\n",
      "Epoch [44/100], Loss: 0.0102\n",
      "Correlation: 0.066559 \n",
      " P-Value: 0.355231\n",
      "Epoch [45/100], Loss: 0.0102\n",
      "Correlation: 0.065107 \n",
      " P-Value: 0.365846\n",
      "Epoch [46/100], Loss: 0.0102\n",
      "Correlation: 0.063088 \n",
      " P-Value: 0.380926\n",
      "Epoch [47/100], Loss: 0.0102\n",
      "Correlation: 0.061343 \n",
      " P-Value: 0.394272\n",
      "Epoch [48/100], Loss: 0.0102\n",
      "Correlation: 0.059587 \n",
      " P-Value: 0.407966\n",
      "Epoch [49/100], Loss: 0.0101\n",
      "Correlation: 0.057489 \n",
      " P-Value: 0.424704\n",
      "Epoch [50/100], Loss: 0.0101\n",
      "Correlation: 0.055541 \n",
      " P-Value: 0.440596\n",
      "Epoch [51/100], Loss: 0.0101\n",
      "Correlation: 0.054367 \n",
      " P-Value: 0.450324\n",
      "Epoch [52/100], Loss: 0.0101\n",
      "Correlation: 0.053688 \n",
      " P-Value: 0.456006\n",
      "Epoch [53/100], Loss: 0.0101\n",
      "Correlation: 0.052778 \n",
      " P-Value: 0.463695\n",
      "Epoch [54/100], Loss: 0.0101\n",
      "Correlation: 0.051605 \n",
      " P-Value: 0.473702\n",
      "Epoch [55/100], Loss: 0.0101\n",
      "Correlation: 0.050354 \n",
      " P-Value: 0.484503\n",
      "Epoch [56/100], Loss: 0.0101\n",
      "Correlation: 0.049465 \n",
      " P-Value: 0.492265\n",
      "Epoch [57/100], Loss: 0.0101\n",
      "Correlation: 0.048692 \n",
      " P-Value: 0.499063\n",
      "Epoch [58/100], Loss: 0.0101\n",
      "Correlation: 0.048201 \n",
      " P-Value: 0.503399\n",
      "Epoch [59/100], Loss: 0.0101\n",
      "Correlation: 0.048034 \n",
      " P-Value: 0.504877\n",
      "Epoch [60/100], Loss: 0.0101\n",
      "Correlation: 0.047273 \n",
      " P-Value: 0.511664\n",
      "Epoch [61/100], Loss: 0.0101\n",
      "Correlation: 0.046161 \n",
      " P-Value: 0.521655\n",
      "Epoch [62/100], Loss: 0.0101\n",
      "Correlation: 0.045623 \n",
      " P-Value: 0.526522\n",
      "Epoch [63/100], Loss: 0.0101\n",
      "Correlation: 0.045485 \n",
      " P-Value: 0.527769\n",
      "Epoch [64/100], Loss: 0.0101\n",
      "Correlation: 0.044866 \n",
      " P-Value: 0.533411\n",
      "Epoch [65/100], Loss: 0.0101\n",
      "Correlation: 0.043996 \n",
      " P-Value: 0.541392\n",
      "Epoch [66/100], Loss: 0.0101\n",
      "Correlation: 0.043512 \n",
      " P-Value: 0.545844\n",
      "Epoch [67/100], Loss: 0.0101\n",
      "Correlation: 0.042891 \n",
      " P-Value: 0.551607\n",
      "Epoch [68/100], Loss: 0.0101\n",
      "Correlation: 0.041860 \n",
      " P-Value: 0.561207\n",
      "Epoch [69/100], Loss: 0.0101\n",
      "Correlation: 0.041319 \n",
      " P-Value: 0.566290\n",
      "Epoch [70/100], Loss: 0.0101\n",
      "Correlation: 0.040965 \n",
      " P-Value: 0.569619\n",
      "Epoch [71/100], Loss: 0.0101\n",
      "Correlation: 0.040040 \n",
      " P-Value: 0.578372\n",
      "Epoch [72/100], Loss: 0.0101\n",
      "Correlation: 0.039075 \n",
      " P-Value: 0.587572\n",
      "Epoch [73/100], Loss: 0.0101\n",
      "Correlation: 0.038939 \n",
      " P-Value: 0.588874\n",
      "Epoch [74/100], Loss: 0.0101\n",
      "Correlation: 0.037820 \n",
      " P-Value: 0.599634\n",
      "Epoch [75/100], Loss: 0.0101\n",
      "Correlation: 0.037209 \n",
      " P-Value: 0.605553\n",
      "Epoch [76/100], Loss: 0.0101\n",
      "Correlation: 0.037468 \n",
      " P-Value: 0.603040\n",
      "Epoch [77/100], Loss: 0.0100\n",
      "Correlation: 0.036546 \n",
      " P-Value: 0.611995\n",
      "Epoch [78/100], Loss: 0.0100\n",
      "Correlation: 0.035551 \n",
      " P-Value: 0.621727\n",
      "Epoch [79/100], Loss: 0.0100\n",
      "Correlation: 0.035172 \n",
      " P-Value: 0.625453\n",
      "Epoch [80/100], Loss: 0.0100\n",
      "Correlation: 0.034321 \n",
      " P-Value: 0.633846\n",
      "Epoch [81/100], Loss: 0.0100\n",
      "Correlation: 0.033927 \n",
      " P-Value: 0.637756\n",
      "Epoch [82/100], Loss: 0.0100\n",
      "Correlation: 0.033754 \n",
      " P-Value: 0.639451\n",
      "Epoch [83/100], Loss: 0.0100\n",
      "Correlation: 0.033281 \n",
      " P-Value: 0.644163\n",
      "Epoch [84/100], Loss: 0.0100\n",
      "Correlation: 0.032895 \n",
      " P-Value: 0.648007\n",
      "Epoch [85/100], Loss: 0.0100\n",
      "Correlation: 0.032247 \n",
      " P-Value: 0.654487\n",
      "Epoch [86/100], Loss: 0.0100\n",
      "Correlation: 0.031798 \n",
      " P-Value: 0.658999\n",
      "Epoch [87/100], Loss: 0.0100\n",
      "Correlation: 0.031484 \n",
      " P-Value: 0.662163\n",
      "Epoch [88/100], Loss: 0.0100\n",
      "Correlation: 0.030600 \n",
      " P-Value: 0.671081\n",
      "Epoch [89/100], Loss: 0.0100\n",
      "Correlation: 0.029929 \n",
      " P-Value: 0.677893\n",
      "Epoch [90/100], Loss: 0.0100\n",
      "Correlation: 0.029820 \n",
      " P-Value: 0.678988\n",
      "Epoch [91/100], Loss: 0.0100\n",
      "Correlation: 0.030230 \n",
      " P-Value: 0.674845\n",
      "Epoch [92/100], Loss: 0.0100\n",
      "Correlation: 0.029551 \n",
      " P-Value: 0.681736\n",
      "Epoch [93/100], Loss: 0.0100\n",
      "Correlation: 0.029265 \n",
      " P-Value: 0.684649\n",
      "Epoch [94/100], Loss: 0.0100\n",
      "Correlation: 0.029597 \n",
      " P-Value: 0.681274\n",
      "Epoch [95/100], Loss: 0.0100\n",
      "Correlation: 0.029108 \n",
      " P-Value: 0.686254\n",
      "Epoch [96/100], Loss: 0.0100\n",
      "Correlation: 0.028338 \n",
      " P-Value: 0.694120\n",
      "Epoch [97/100], Loss: 0.0100\n",
      "Correlation: 0.028542 \n",
      " P-Value: 0.692047\n",
      "Epoch [98/100], Loss: 0.0100\n",
      "Correlation: 0.028004 \n",
      " P-Value: 0.697561\n",
      "Epoch [99/100], Loss: 0.0100\n",
      "Correlation: 0.027669 \n",
      " P-Value: 0.700995\n",
      "Epoch [100/100], Loss: 0.0100\n",
      "Max correlation is: [0.22950938] at 5th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 259\n",
    "net_2 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_2.train_net(num_epochs, train_loader_list[2], test_loader_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.059962 \n",
      " P-Value: 0.405014\n",
      "Epoch [1/100], Loss: 0.0315\n",
      "Correlation: -0.006777 \n",
      " P-Value: 0.925064\n",
      "Epoch [2/100], Loss: 0.0270\n",
      "Correlation: -0.042675 \n",
      " P-Value: 0.553607\n",
      "Epoch [3/100], Loss: 0.0269\n",
      "Correlation: -0.017286 \n",
      " P-Value: 0.810466\n",
      "Epoch [4/100], Loss: 0.0266\n",
      "Correlation: 0.062068 \n",
      " P-Value: 0.388693\n",
      "Epoch [5/100], Loss: 0.0265\n",
      "Correlation: 0.015178 \n",
      " P-Value: 0.833222\n",
      "Epoch [6/100], Loss: 0.0264\n",
      "Correlation: 0.048189 \n",
      " P-Value: 0.503509\n",
      "Epoch [7/100], Loss: 0.0264\n",
      "Correlation: -0.047696 \n",
      " P-Value: 0.507884\n",
      "Epoch [8/100], Loss: 0.0263\n",
      "Correlation: -0.009157 \n",
      " P-Value: 0.898892\n",
      "Epoch [9/100], Loss: 0.0263\n",
      "Correlation: -0.021372 \n",
      " P-Value: 0.766806\n",
      "Epoch [10/100], Loss: 0.0263\n",
      "Correlation: -0.006210 \n",
      " P-Value: 0.931338\n",
      "Epoch [11/100], Loss: 0.0263\n",
      "Correlation: -0.043421 \n",
      " P-Value: 0.546680\n",
      "Epoch [12/100], Loss: 0.0262\n",
      "Correlation: -0.049543 \n",
      " P-Value: 0.491569\n",
      "Epoch [13/100], Loss: 0.0262\n",
      "Correlation: -0.055952 \n",
      " P-Value: 0.437216\n",
      "Epoch [14/100], Loss: 0.0262\n",
      "Correlation: -0.044436 \n",
      " P-Value: 0.537350\n",
      "Epoch [15/100], Loss: 0.0262\n",
      "Correlation: -0.037830 \n",
      " P-Value: 0.599542\n",
      "Epoch [16/100], Loss: 0.0262\n",
      "Correlation: -0.036672 \n",
      " P-Value: 0.610766\n",
      "Epoch [17/100], Loss: 0.0262\n",
      "Correlation: -0.040254 \n",
      " P-Value: 0.576343\n",
      "Epoch [18/100], Loss: 0.0262\n",
      "Correlation: -0.043566 \n",
      " P-Value: 0.545345\n",
      "Epoch [19/100], Loss: 0.0262\n",
      "Correlation: -0.040700 \n",
      " P-Value: 0.572120\n",
      "Epoch [20/100], Loss: 0.0262\n",
      "Correlation: -0.040909 \n",
      " P-Value: 0.570155\n",
      "Epoch [21/100], Loss: 0.0262\n",
      "Correlation: -0.038451 \n",
      " P-Value: 0.593558\n",
      "Epoch [22/100], Loss: 0.0262\n",
      "Correlation: -0.043286 \n",
      " P-Value: 0.547933\n",
      "Epoch [23/100], Loss: 0.0262\n",
      "Correlation: -0.043847 \n",
      " P-Value: 0.542760\n",
      "Epoch [24/100], Loss: 0.0262\n",
      "Correlation: -0.047116 \n",
      " P-Value: 0.513065\n",
      "Epoch [25/100], Loss: 0.0262\n",
      "Correlation: -0.044282 \n",
      " P-Value: 0.538761\n",
      "Epoch [26/100], Loss: 0.0262\n",
      "Correlation: -0.046096 \n",
      " P-Value: 0.522239\n",
      "Epoch [27/100], Loss: 0.0262\n",
      "Correlation: -0.045958 \n",
      " P-Value: 0.523486\n",
      "Epoch [28/100], Loss: 0.0262\n",
      "Correlation: -0.044585 \n",
      " P-Value: 0.535983\n",
      "Epoch [29/100], Loss: 0.0262\n",
      "Correlation: -0.042436 \n",
      " P-Value: 0.555837\n",
      "Epoch [30/100], Loss: 0.0262\n",
      "Correlation: -0.044050 \n",
      " P-Value: 0.540888\n",
      "Epoch [31/100], Loss: 0.0262\n",
      "Correlation: -0.043762 \n",
      " P-Value: 0.543538\n",
      "Epoch [32/100], Loss: 0.0262\n",
      "Correlation: -0.042968 \n",
      " P-Value: 0.550879\n",
      "Epoch [33/100], Loss: 0.0262\n",
      "Correlation: -0.042602 \n",
      " P-Value: 0.554284\n",
      "Epoch [34/100], Loss: 0.0262\n",
      "Correlation: -0.042722 \n",
      " P-Value: 0.553179\n",
      "Epoch [35/100], Loss: 0.0262\n",
      "Correlation: -0.044595 \n",
      " P-Value: 0.535891\n",
      "Epoch [36/100], Loss: 0.0262\n",
      "Correlation: -0.044979 \n",
      " P-Value: 0.532382\n",
      "Epoch [37/100], Loss: 0.0262\n",
      "Correlation: -0.042730 \n",
      " P-Value: 0.553095\n",
      "Epoch [38/100], Loss: 0.0262\n",
      "Correlation: -0.041077 \n",
      " P-Value: 0.568572\n",
      "Epoch [39/100], Loss: 0.0262\n",
      "Correlation: -0.040880 \n",
      " P-Value: 0.570416\n",
      "Epoch [40/100], Loss: 0.0262\n",
      "Correlation: -0.040284 \n",
      " P-Value: 0.576063\n",
      "Epoch [41/100], Loss: 0.0262\n",
      "Correlation: -0.039208 \n",
      " P-Value: 0.586305\n",
      "Epoch [42/100], Loss: 0.0262\n",
      "Correlation: -0.038527 \n",
      " P-Value: 0.592837\n",
      "Epoch [43/100], Loss: 0.0262\n",
      "Correlation: -0.038308 \n",
      " P-Value: 0.594932\n",
      "Epoch [44/100], Loss: 0.0262\n",
      "Correlation: -0.039407 \n",
      " P-Value: 0.584401\n",
      "Epoch [45/100], Loss: 0.0262\n",
      "Correlation: -0.039170 \n",
      " P-Value: 0.586661\n",
      "Epoch [46/100], Loss: 0.0261\n",
      "Correlation: -0.038281 \n",
      " P-Value: 0.595194\n",
      "Epoch [47/100], Loss: 0.0261\n",
      "Correlation: -0.038605 \n",
      " P-Value: 0.592080\n",
      "Epoch [48/100], Loss: 0.0261\n",
      "Correlation: -0.038121 \n",
      " P-Value: 0.596742\n",
      "Epoch [49/100], Loss: 0.0261\n",
      "Correlation: -0.037646 \n",
      " P-Value: 0.601320\n",
      "Epoch [50/100], Loss: 0.0261\n",
      "Correlation: -0.037963 \n",
      " P-Value: 0.598260\n",
      "Epoch [51/100], Loss: 0.0261\n",
      "Correlation: -0.036775 \n",
      " P-Value: 0.609763\n",
      "Epoch [52/100], Loss: 0.0261\n",
      "Correlation: -0.035723 \n",
      " P-Value: 0.620046\n",
      "Epoch [53/100], Loss: 0.0261\n",
      "Correlation: -0.034633 \n",
      " P-Value: 0.630762\n",
      "Epoch [54/100], Loss: 0.0261\n",
      "Correlation: -0.034161 \n",
      " P-Value: 0.635422\n",
      "Epoch [55/100], Loss: 0.0261\n",
      "Correlation: -0.032178 \n",
      " P-Value: 0.655192\n",
      "Epoch [56/100], Loss: 0.0261\n",
      "Correlation: -0.031303 \n",
      " P-Value: 0.663981\n",
      "Epoch [57/100], Loss: 0.0261\n",
      "Correlation: -0.031698 \n",
      " P-Value: 0.659999\n",
      "Epoch [58/100], Loss: 0.0261\n",
      "Correlation: -0.030357 \n",
      " P-Value: 0.673548\n",
      "Epoch [59/100], Loss: 0.0261\n",
      "Correlation: -0.030802 \n",
      " P-Value: 0.669039\n",
      "Epoch [60/100], Loss: 0.0261\n",
      "Correlation: -0.029655 \n",
      " P-Value: 0.680680\n",
      "Epoch [61/100], Loss: 0.0261\n",
      "Correlation: -0.029893 \n",
      " P-Value: 0.678257\n",
      "Epoch [62/100], Loss: 0.0261\n",
      "Correlation: -0.027514 \n",
      " P-Value: 0.702607\n",
      "Epoch [63/100], Loss: 0.0261\n",
      "Correlation: -0.026498 \n",
      " P-Value: 0.713078\n",
      "Epoch [64/100], Loss: 0.0261\n",
      "Correlation: -0.026674 \n",
      " P-Value: 0.711269\n",
      "Epoch [65/100], Loss: 0.0261\n",
      "Correlation: -0.026003 \n",
      " P-Value: 0.718220\n",
      "Epoch [66/100], Loss: 0.0261\n",
      "Correlation: -0.025504 \n",
      " P-Value: 0.723415\n",
      "Epoch [67/100], Loss: 0.0261\n",
      "Correlation: -0.024162 \n",
      " P-Value: 0.737406\n",
      "Epoch [68/100], Loss: 0.0261\n",
      "Correlation: -0.023296 \n",
      " P-Value: 0.746497\n",
      "Epoch [69/100], Loss: 0.0261\n",
      "Correlation: -0.022372 \n",
      " P-Value: 0.756229\n",
      "Epoch [70/100], Loss: 0.0261\n",
      "Correlation: -0.020360 \n",
      " P-Value: 0.777529\n",
      "Epoch [71/100], Loss: 0.0261\n",
      "Correlation: -0.019511 \n",
      " P-Value: 0.786594\n",
      "Epoch [72/100], Loss: 0.0261\n",
      "Correlation: -0.016683 \n",
      " P-Value: 0.816948\n",
      "Epoch [73/100], Loss: 0.0261\n",
      "Correlation: -0.016279 \n",
      " P-Value: 0.821298\n",
      "Epoch [74/100], Loss: 0.0261\n",
      "Correlation: -0.015351 \n",
      " P-Value: 0.831340\n",
      "Epoch [75/100], Loss: 0.0261\n",
      "Correlation: -0.014211 \n",
      " P-Value: 0.843691\n",
      "Epoch [76/100], Loss: 0.0261\n",
      "Correlation: -0.013194 \n",
      " P-Value: 0.854760\n",
      "Epoch [77/100], Loss: 0.0261\n",
      "Correlation: -0.012548 \n",
      " P-Value: 0.861774\n",
      "Epoch [78/100], Loss: 0.0261\n",
      "Correlation: -0.010157 \n",
      " P-Value: 0.887922\n",
      "Epoch [79/100], Loss: 0.0261\n",
      "Correlation: -0.008761 \n",
      " P-Value: 0.903240\n",
      "Epoch [80/100], Loss: 0.0261\n",
      "Correlation: -0.007464 \n",
      " P-Value: 0.917504\n",
      "Epoch [81/100], Loss: 0.0261\n",
      "Correlation: -0.006609 \n",
      " P-Value: 0.926929\n",
      "Epoch [82/100], Loss: 0.0261\n",
      "Correlation: -0.006332 \n",
      " P-Value: 0.930027\n",
      "Epoch [83/100], Loss: 0.0261\n",
      "Correlation: -0.005022 \n",
      " P-Value: 0.944458\n",
      "Epoch [84/100], Loss: 0.0261\n",
      "Correlation: -0.003632 \n",
      " P-Value: 0.959748\n",
      "Epoch [85/100], Loss: 0.0261\n",
      "Correlation: -0.002130 \n",
      " P-Value: 0.976442\n",
      "Epoch [86/100], Loss: 0.0261\n",
      "Correlation: -0.000056 \n",
      " P-Value: 1.000000\n",
      "Epoch [87/100], Loss: 0.0261\n",
      "Correlation: 0.000147 \n",
      " P-Value: 1.000000\n",
      "Epoch [88/100], Loss: 0.0261\n",
      "Correlation: 0.001363 \n",
      " P-Value: 0.984712\n",
      "Epoch [89/100], Loss: 0.0261\n",
      "Correlation: 0.002582 \n",
      " P-Value: 0.971531\n",
      "Epoch [90/100], Loss: 0.0261\n",
      "Correlation: 0.001987 \n",
      " P-Value: 0.978046\n",
      "Epoch [91/100], Loss: 0.0261\n",
      "Correlation: 0.003002 \n",
      " P-Value: 0.966798\n",
      "Epoch [92/100], Loss: 0.0261\n",
      "Correlation: 0.005210 \n",
      " P-Value: 0.942399\n",
      "Epoch [93/100], Loss: 0.0261\n",
      "Correlation: 0.006808 \n",
      " P-Value: 0.924774\n",
      "Epoch [94/100], Loss: 0.0261\n",
      "Correlation: 0.005307 \n",
      " P-Value: 0.941335\n",
      "Epoch [95/100], Loss: 0.0261\n",
      "Correlation: 0.005727 \n",
      " P-Value: 0.936682\n",
      "Epoch [96/100], Loss: 0.0261\n",
      "Correlation: 0.008789 \n",
      " P-Value: 0.902942\n",
      "Epoch [97/100], Loss: 0.0261\n",
      "Correlation: 0.007506 \n",
      " P-Value: 0.917022\n",
      "Epoch [98/100], Loss: 0.0261\n",
      "Correlation: 0.006849 \n",
      " P-Value: 0.924293\n",
      "Epoch [99/100], Loss: 0.0261\n",
      "Correlation: 0.008294 \n",
      " P-Value: 0.908389\n",
      "Epoch [100/100], Loss: 0.0261\n",
      "Max correlation is: [0.06206805] at 4th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 252\n",
    "net_3 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_3.train_net(num_epochs, train_loader_list[3], test_loader_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.109461 \n",
      " P-Value: 0.127684\n",
      "Epoch [1/100], Loss: 0.5063\n",
      "Correlation: 0.108429 \n",
      " P-Value: 0.131338\n",
      "Epoch [2/100], Loss: 0.2160\n",
      "Correlation: 0.099240 \n",
      " P-Value: 0.167492\n",
      "Epoch [3/100], Loss: 0.2022\n",
      "Correlation: 0.052660 \n",
      " P-Value: 0.464698\n",
      "Epoch [4/100], Loss: 0.1514\n",
      "Correlation: -0.027080 \n",
      " P-Value: 0.707064\n",
      "Epoch [5/100], Loss: 0.1265\n",
      "Correlation: -0.052011 \n",
      " P-Value: 0.470224\n",
      "Epoch [6/100], Loss: 0.1216\n",
      "Correlation: -0.052621 \n",
      " P-Value: 0.465030\n",
      "Epoch [7/100], Loss: 0.1188\n",
      "Correlation: -0.015846 \n",
      " P-Value: 0.825967\n",
      "Epoch [8/100], Loss: 0.1125\n",
      "Correlation: -0.042767 \n",
      " P-Value: 0.552751\n",
      "Epoch [9/100], Loss: 0.1100\n",
      "Correlation: -0.052209 \n",
      " P-Value: 0.468529\n",
      "Epoch [10/100], Loss: 0.1057\n",
      "Correlation: -0.101195 \n",
      " P-Value: 0.159238\n",
      "Epoch [11/100], Loss: 0.1034\n",
      "Correlation: -0.096591 \n",
      " P-Value: 0.179178\n",
      "Epoch [12/100], Loss: 0.1004\n",
      "Correlation: -0.123617 \n",
      " P-Value: 0.085118\n",
      "Epoch [13/100], Loss: 0.0995\n",
      "Correlation: -0.126794 \n",
      " P-Value: 0.077341\n",
      "Epoch [14/100], Loss: 0.0977\n",
      "Correlation: -0.146983 \n",
      " P-Value: 0.040319\n",
      "Epoch [15/100], Loss: 0.0976\n",
      "Correlation: -0.149259 \n",
      " P-Value: 0.037290\n",
      "Epoch [16/100], Loss: 0.0965\n",
      "Correlation: -0.139866 \n",
      " P-Value: 0.051157\n",
      "Epoch [17/100], Loss: 0.0964\n",
      "Correlation: -0.117486 \n",
      " P-Value: 0.101899\n",
      "Epoch [18/100], Loss: 0.0956\n",
      "Correlation: -0.114516 \n",
      " P-Value: 0.110914\n",
      "Epoch [19/100], Loss: 0.0956\n",
      "Correlation: -0.110968 \n",
      " P-Value: 0.122491\n",
      "Epoch [20/100], Loss: 0.0950\n",
      "Correlation: -0.121891 \n",
      " P-Value: 0.089602\n",
      "Epoch [21/100], Loss: 0.0950\n",
      "Correlation: -0.115138 \n",
      " P-Value: 0.108977\n",
      "Epoch [22/100], Loss: 0.0946\n",
      "Correlation: -0.115680 \n",
      " P-Value: 0.107309\n",
      "Epoch [23/100], Loss: 0.0946\n",
      "Correlation: -0.108948 \n",
      " P-Value: 0.129493\n",
      "Epoch [24/100], Loss: 0.0942\n",
      "Correlation: -0.112232 \n",
      " P-Value: 0.118263\n",
      "Epoch [25/100], Loss: 0.0942\n",
      "Correlation: -0.106107 \n",
      " P-Value: 0.139851\n",
      "Epoch [26/100], Loss: 0.0939\n",
      "Correlation: -0.104123 \n",
      " P-Value: 0.147453\n",
      "Epoch [27/100], Loss: 0.0938\n",
      "Correlation: -0.098306 \n",
      " P-Value: 0.171545\n",
      "Epoch [28/100], Loss: 0.0936\n",
      "Correlation: -0.098114 \n",
      " P-Value: 0.172386\n",
      "Epoch [29/100], Loss: 0.0935\n",
      "Correlation: -0.094200 \n",
      " P-Value: 0.190230\n",
      "Epoch [30/100], Loss: 0.0933\n",
      "Correlation: -0.094752 \n",
      " P-Value: 0.187639\n",
      "Epoch [31/100], Loss: 0.0933\n",
      "Correlation: -0.090455 \n",
      " P-Value: 0.208539\n",
      "Epoch [32/100], Loss: 0.0931\n",
      "Correlation: -0.088545 \n",
      " P-Value: 0.218346\n",
      "Epoch [33/100], Loss: 0.0931\n",
      "Correlation: -0.086842 \n",
      " P-Value: 0.227369\n",
      "Epoch [34/100], Loss: 0.0929\n",
      "Correlation: -0.083984 \n",
      " P-Value: 0.243089\n",
      "Epoch [35/100], Loss: 0.0929\n",
      "Correlation: -0.084363 \n",
      " P-Value: 0.240962\n",
      "Epoch [36/100], Loss: 0.0928\n",
      "Correlation: -0.082886 \n",
      " P-Value: 0.249331\n",
      "Epoch [37/100], Loss: 0.0927\n",
      "Correlation: -0.080846 \n",
      " P-Value: 0.261211\n",
      "Epoch [38/100], Loss: 0.0926\n",
      "Correlation: -0.079178 \n",
      " P-Value: 0.271210\n",
      "Epoch [39/100], Loss: 0.0925\n",
      "Correlation: -0.078287 \n",
      " P-Value: 0.276659\n",
      "Epoch [40/100], Loss: 0.0925\n",
      "Correlation: -0.077156 \n",
      " P-Value: 0.283678\n",
      "Epoch [41/100], Loss: 0.0924\n",
      "Correlation: -0.075384 \n",
      " P-Value: 0.294915\n",
      "Epoch [42/100], Loss: 0.0923\n",
      "Correlation: -0.075222 \n",
      " P-Value: 0.295951\n",
      "Epoch [43/100], Loss: 0.0923\n",
      "Correlation: -0.072615 \n",
      " P-Value: 0.313061\n",
      "Epoch [44/100], Loss: 0.0922\n",
      "Correlation: -0.070199 \n",
      " P-Value: 0.329477\n",
      "Epoch [45/100], Loss: 0.0922\n",
      "Correlation: -0.068249 \n",
      " P-Value: 0.343114\n",
      "Epoch [46/100], Loss: 0.0921\n",
      "Correlation: -0.066348 \n",
      " P-Value: 0.356756\n",
      "Epoch [47/100], Loss: 0.0921\n",
      "Correlation: -0.065721 \n",
      " P-Value: 0.361335\n",
      "Epoch [48/100], Loss: 0.0920\n",
      "Correlation: -0.063858 \n",
      " P-Value: 0.375132\n",
      "Epoch [49/100], Loss: 0.0920\n",
      "Correlation: -0.064599 \n",
      " P-Value: 0.369608\n",
      "Epoch [50/100], Loss: 0.0919\n",
      "Correlation: -0.061929 \n",
      " P-Value: 0.389757\n",
      "Epoch [51/100], Loss: 0.0919\n",
      "Correlation: -0.061040 \n",
      " P-Value: 0.396608\n",
      "Epoch [52/100], Loss: 0.0919\n",
      "Correlation: -0.061956 \n",
      " P-Value: 0.389551\n",
      "Epoch [53/100], Loss: 0.0918\n",
      "Correlation: -0.059466 \n",
      " P-Value: 0.408918\n",
      "Epoch [54/100], Loss: 0.0918\n",
      "Correlation: -0.059232 \n",
      " P-Value: 0.410775\n",
      "Epoch [55/100], Loss: 0.0918\n",
      "Correlation: -0.057086 \n",
      " P-Value: 0.427961\n",
      "Epoch [56/100], Loss: 0.0917\n",
      "Correlation: -0.058051 \n",
      " P-Value: 0.420185\n",
      "Epoch [57/100], Loss: 0.0917\n",
      "Correlation: -0.056869 \n",
      " P-Value: 0.429727\n",
      "Epoch [58/100], Loss: 0.0917\n",
      "Correlation: -0.053459 \n",
      " P-Value: 0.457934\n",
      "Epoch [59/100], Loss: 0.0916\n",
      "Correlation: -0.056074 \n",
      " P-Value: 0.436208\n",
      "Epoch [60/100], Loss: 0.0916\n",
      "Correlation: -0.055001 \n",
      " P-Value: 0.445053\n",
      "Epoch [61/100], Loss: 0.0916\n",
      "Correlation: -0.052088 \n",
      " P-Value: 0.469563\n",
      "Epoch [62/100], Loss: 0.0916\n",
      "Correlation: -0.053076 \n",
      " P-Value: 0.461173\n",
      "Epoch [63/100], Loss: 0.0915\n",
      "Correlation: -0.053674 \n",
      " P-Value: 0.456122\n",
      "Epoch [64/100], Loss: 0.0915\n",
      "Correlation: -0.049942 \n",
      " P-Value: 0.488088\n",
      "Epoch [65/100], Loss: 0.0915\n",
      "Correlation: -0.049317 \n",
      " P-Value: 0.493556\n",
      "Epoch [66/100], Loss: 0.0915\n",
      "Correlation: -0.049741 \n",
      " P-Value: 0.489842\n",
      "Epoch [67/100], Loss: 0.0914\n",
      "Correlation: -0.048628 \n",
      " P-Value: 0.499625\n",
      "Epoch [68/100], Loss: 0.0914\n",
      "Correlation: -0.047443 \n",
      " P-Value: 0.510146\n",
      "Epoch [69/100], Loss: 0.0914\n",
      "Correlation: -0.046717 \n",
      " P-Value: 0.516649\n",
      "Epoch [70/100], Loss: 0.0914\n",
      "Correlation: -0.044384 \n",
      " P-Value: 0.537830\n",
      "Epoch [71/100], Loss: 0.0914\n",
      "Correlation: -0.045622 \n",
      " P-Value: 0.526528\n",
      "Epoch [72/100], Loss: 0.0914\n",
      "Correlation: -0.044515 \n",
      " P-Value: 0.536620\n",
      "Epoch [73/100], Loss: 0.0913\n",
      "Correlation: -0.042206 \n",
      " P-Value: 0.557972\n",
      "Epoch [74/100], Loss: 0.0913\n",
      "Correlation: -0.041903 \n",
      " P-Value: 0.560814\n",
      "Epoch [75/100], Loss: 0.0913\n",
      "Correlation: -0.040623 \n",
      " P-Value: 0.572854\n",
      "Epoch [76/100], Loss: 0.0913\n",
      "Correlation: -0.041280 \n",
      " P-Value: 0.566657\n",
      "Epoch [77/100], Loss: 0.0913\n",
      "Correlation: -0.041015 \n",
      " P-Value: 0.569146\n",
      "Epoch [78/100], Loss: 0.0913\n",
      "Correlation: -0.039300 \n",
      " P-Value: 0.585434\n",
      "Epoch [79/100], Loss: 0.0913\n",
      "Correlation: -0.038875 \n",
      " P-Value: 0.589490\n",
      "Epoch [80/100], Loss: 0.0913\n",
      "Correlation: -0.038896 \n",
      " P-Value: 0.589292\n",
      "Epoch [81/100], Loss: 0.0912\n",
      "Correlation: -0.036845 \n",
      " P-Value: 0.609086\n",
      "Epoch [82/100], Loss: 0.0912\n",
      "Correlation: -0.036678 \n",
      " P-Value: 0.610719\n",
      "Epoch [83/100], Loss: 0.0912\n",
      "Correlation: -0.032866 \n",
      " P-Value: 0.648306\n",
      "Epoch [84/100], Loss: 0.0912\n",
      "Correlation: -0.034453 \n",
      " P-Value: 0.632543\n",
      "Epoch [85/100], Loss: 0.0912\n",
      "Correlation: -0.034804 \n",
      " P-Value: 0.629068\n",
      "Epoch [86/100], Loss: 0.0912\n",
      "Correlation: -0.033356 \n",
      " P-Value: 0.643424\n",
      "Epoch [87/100], Loss: 0.0912\n",
      "Correlation: -0.030720 \n",
      " P-Value: 0.669871\n",
      "Epoch [88/100], Loss: 0.0912\n",
      "Correlation: -0.033600 \n",
      " P-Value: 0.640989\n",
      "Epoch [89/100], Loss: 0.0912\n",
      "Correlation: -0.030756 \n",
      " P-Value: 0.669509\n",
      "Epoch [90/100], Loss: 0.0912\n",
      "Correlation: -0.028371 \n",
      " P-Value: 0.693796\n",
      "Epoch [91/100], Loss: 0.0911\n",
      "Correlation: -0.030029 \n",
      " P-Value: 0.676873\n",
      "Epoch [92/100], Loss: 0.0912\n",
      "Correlation: -0.025017 \n",
      " P-Value: 0.728478\n",
      "Epoch [93/100], Loss: 0.0911\n",
      "Correlation: -0.028880 \n",
      " P-Value: 0.688585\n",
      "Epoch [94/100], Loss: 0.0911\n",
      "Correlation: -0.025583 \n",
      " P-Value: 0.722577\n",
      "Epoch [95/100], Loss: 0.0911\n",
      "Correlation: -0.025690 \n",
      " P-Value: 0.721478\n",
      "Epoch [96/100], Loss: 0.0911\n",
      "Correlation: -0.028633 \n",
      " P-Value: 0.691118\n",
      "Epoch [97/100], Loss: 0.0911\n",
      "Correlation: -0.022244 \n",
      " P-Value: 0.757583\n",
      "Epoch [98/100], Loss: 0.0911\n",
      "Correlation: -0.024808 \n",
      " P-Value: 0.730662\n",
      "Epoch [99/100], Loss: 0.0911\n",
      "Correlation: -0.025017 \n",
      " P-Value: 0.728478\n",
      "Epoch [100/100], Loss: 0.0911\n",
      "Max correlation is: [-0.1492595] at 15th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 251\n",
    "net_4 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_4.train_net(num_epochs, train_loader_list[4], test_loader_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.036026 \n",
      " P-Value: 0.617078\n",
      "Epoch [1/100], Loss: 0.1064\n",
      "Correlation: 0.030350 \n",
      " P-Value: 0.673607\n",
      "Epoch [2/100], Loss: 0.0635\n",
      "Correlation: 0.012176 \n",
      " P-Value: 0.865824\n",
      "Epoch [3/100], Loss: 0.0461\n",
      "Correlation: 0.002600 \n",
      " P-Value: 0.971149\n",
      "Epoch [4/100], Loss: 0.0423\n",
      "Correlation: 0.006653 \n",
      " P-Value: 0.926434\n",
      "Epoch [5/100], Loss: 0.0424\n",
      "Correlation: 0.024481 \n",
      " P-Value: 0.734073\n",
      "Epoch [6/100], Loss: 0.0403\n",
      "Correlation: 0.034580 \n",
      " P-Value: 0.631281\n",
      "Epoch [7/100], Loss: 0.0392\n",
      "Correlation: 0.034145 \n",
      " P-Value: 0.635578\n",
      "Epoch [8/100], Loss: 0.0387\n",
      "Correlation: 0.005409 \n",
      " P-Value: 0.940168\n",
      "Epoch [9/100], Loss: 0.0379\n",
      "Correlation: -0.017091 \n",
      " P-Value: 0.812537\n",
      "Epoch [10/100], Loss: 0.0372\n",
      "Correlation: -0.045864 \n",
      " P-Value: 0.524337\n",
      "Epoch [11/100], Loss: 0.0369\n",
      "Correlation: -0.055734 \n",
      " P-Value: 0.439000\n",
      "Epoch [12/100], Loss: 0.0365\n",
      "Correlation: -0.067279 \n",
      " P-Value: 0.350038\n",
      "Epoch [13/100], Loss: 0.0364\n",
      "Correlation: -0.074870 \n",
      " P-Value: 0.298230\n",
      "Epoch [14/100], Loss: 0.0362\n",
      "Correlation: -0.090396 \n",
      " P-Value: 0.208833\n",
      "Epoch [15/100], Loss: 0.0360\n",
      "Correlation: -0.092775 \n",
      " P-Value: 0.197055\n",
      "Epoch [16/100], Loss: 0.0358\n",
      "Correlation: -0.098770 \n",
      " P-Value: 0.169521\n",
      "Epoch [17/100], Loss: 0.0358\n",
      "Correlation: -0.108564 \n",
      " P-Value: 0.130855\n",
      "Epoch [18/100], Loss: 0.0356\n",
      "Correlation: -0.121195 \n",
      " P-Value: 0.091461\n",
      "Epoch [19/100], Loss: 0.0356\n",
      "Correlation: -0.139378 \n",
      " P-Value: 0.051982\n",
      "Epoch [20/100], Loss: 0.0354\n",
      "Correlation: -0.145442 \n",
      " P-Value: 0.042486\n",
      "Epoch [21/100], Loss: 0.0354\n",
      "Correlation: -0.138065 \n",
      " P-Value: 0.054254\n",
      "Epoch [22/100], Loss: 0.0353\n",
      "Correlation: -0.142203 \n",
      " P-Value: 0.047359\n",
      "Epoch [23/100], Loss: 0.0353\n",
      "Correlation: -0.146158 \n",
      " P-Value: 0.041468\n",
      "Epoch [24/100], Loss: 0.0352\n",
      "Correlation: -0.149518 \n",
      " P-Value: 0.036959\n",
      "Epoch [25/100], Loss: 0.0352\n",
      "Correlation: -0.142001 \n",
      " P-Value: 0.047677\n",
      "Epoch [26/100], Loss: 0.0351\n",
      "Correlation: -0.146039 \n",
      " P-Value: 0.041636\n",
      "Epoch [27/100], Loss: 0.0351\n",
      "Correlation: -0.147115 \n",
      " P-Value: 0.040139\n",
      "Epoch [28/100], Loss: 0.0350\n",
      "Correlation: -0.148673 \n",
      " P-Value: 0.038052\n",
      "Epoch [29/100], Loss: 0.0350\n",
      "Correlation: -0.137873 \n",
      " P-Value: 0.054593\n",
      "Epoch [30/100], Loss: 0.0349\n",
      "Correlation: -0.140891 \n",
      " P-Value: 0.049461\n",
      "Epoch [31/100], Loss: 0.0349\n",
      "Correlation: -0.143581 \n",
      " P-Value: 0.045232\n",
      "Epoch [32/100], Loss: 0.0349\n",
      "Correlation: -0.147703 \n",
      " P-Value: 0.039340\n",
      "Epoch [33/100], Loss: 0.0349\n",
      "Correlation: -0.131757 \n",
      " P-Value: 0.066348\n",
      "Epoch [34/100], Loss: 0.0348\n",
      "Correlation: -0.133198 \n",
      " P-Value: 0.063406\n",
      "Epoch [35/100], Loss: 0.0348\n",
      "Correlation: -0.134020 \n",
      " P-Value: 0.061776\n",
      "Epoch [36/100], Loss: 0.0348\n",
      "Correlation: -0.139212 \n",
      " P-Value: 0.052264\n",
      "Epoch [37/100], Loss: 0.0348\n",
      "Correlation: -0.124065 \n",
      " P-Value: 0.083986\n",
      "Epoch [38/100], Loss: 0.0347\n",
      "Correlation: -0.132926 \n",
      " P-Value: 0.063954\n",
      "Epoch [39/100], Loss: 0.0347\n",
      "Correlation: -0.129041 \n",
      " P-Value: 0.072193\n",
      "Epoch [40/100], Loss: 0.0347\n",
      "Correlation: -0.128510 \n",
      " P-Value: 0.073384\n",
      "Epoch [41/100], Loss: 0.0347\n",
      "Correlation: -0.119824 \n",
      " P-Value: 0.095216\n",
      "Epoch [42/100], Loss: 0.0346\n",
      "Correlation: -0.119750 \n",
      " P-Value: 0.095420\n",
      "Epoch [43/100], Loss: 0.0346\n",
      "Correlation: -0.114036 \n",
      " P-Value: 0.112430\n",
      "Epoch [44/100], Loss: 0.0346\n",
      "Correlation: -0.117441 \n",
      " P-Value: 0.102031\n",
      "Epoch [45/100], Loss: 0.0346\n",
      "Correlation: -0.110650 \n",
      " P-Value: 0.123576\n",
      "Epoch [46/100], Loss: 0.0346\n",
      "Correlation: -0.108258 \n",
      " P-Value: 0.131952\n",
      "Epoch [47/100], Loss: 0.0346\n",
      "Correlation: -0.104231 \n",
      " P-Value: 0.147031\n",
      "Epoch [48/100], Loss: 0.0346\n",
      "Correlation: -0.107334 \n",
      " P-Value: 0.135300\n",
      "Epoch [49/100], Loss: 0.0346\n",
      "Correlation: -0.101273 \n",
      " P-Value: 0.158914\n",
      "Epoch [50/100], Loss: 0.0346\n",
      "Correlation: -0.101100 \n",
      " P-Value: 0.159632\n",
      "Epoch [51/100], Loss: 0.0345\n",
      "Correlation: -0.098355 \n",
      " P-Value: 0.171331\n",
      "Epoch [52/100], Loss: 0.0345\n",
      "Correlation: -0.100227 \n",
      " P-Value: 0.163286\n",
      "Epoch [53/100], Loss: 0.0345\n",
      "Correlation: -0.096452 \n",
      " P-Value: 0.179805\n",
      "Epoch [54/100], Loss: 0.0345\n",
      "Correlation: -0.097770 \n",
      " P-Value: 0.173903\n",
      "Epoch [55/100], Loss: 0.0345\n",
      "Correlation: -0.095983 \n",
      " P-Value: 0.181943\n",
      "Epoch [56/100], Loss: 0.0345\n",
      "Correlation: -0.097618 \n",
      " P-Value: 0.174580\n",
      "Epoch [57/100], Loss: 0.0345\n",
      "Correlation: -0.095795 \n",
      " P-Value: 0.182805\n",
      "Epoch [58/100], Loss: 0.0345\n",
      "Correlation: -0.096135 \n",
      " P-Value: 0.181247\n",
      "Epoch [59/100], Loss: 0.0345\n",
      "Correlation: -0.096782 \n",
      " P-Value: 0.178314\n",
      "Epoch [60/100], Loss: 0.0345\n",
      "Correlation: -0.094808 \n",
      " P-Value: 0.187377\n",
      "Epoch [61/100], Loss: 0.0345\n",
      "Correlation: -0.093090 \n",
      " P-Value: 0.195532\n",
      "Epoch [62/100], Loss: 0.0344\n",
      "Correlation: -0.093801 \n",
      " P-Value: 0.192124\n",
      "Epoch [63/100], Loss: 0.0344\n",
      "Correlation: -0.093573 \n",
      " P-Value: 0.193213\n",
      "Epoch [64/100], Loss: 0.0344\n",
      "Correlation: -0.092083 \n",
      " P-Value: 0.200432\n",
      "Epoch [65/100], Loss: 0.0344\n",
      "Correlation: -0.092395 \n",
      " P-Value: 0.198908\n",
      "Epoch [66/100], Loss: 0.0344\n",
      "Correlation: -0.091667 \n",
      " P-Value: 0.202480\n",
      "Epoch [67/100], Loss: 0.0344\n",
      "Correlation: -0.092157 \n",
      " P-Value: 0.200067\n",
      "Epoch [68/100], Loss: 0.0344\n",
      "Correlation: -0.090876 \n",
      " P-Value: 0.206421\n",
      "Epoch [69/100], Loss: 0.0344\n",
      "Correlation: -0.090837 \n",
      " P-Value: 0.206614\n",
      "Epoch [70/100], Loss: 0.0344\n",
      "Correlation: -0.092211 \n",
      " P-Value: 0.199803\n",
      "Epoch [71/100], Loss: 0.0344\n",
      "Correlation: -0.091308 \n",
      " P-Value: 0.204260\n",
      "Epoch [72/100], Loss: 0.0344\n",
      "Correlation: -0.089921 \n",
      " P-Value: 0.211249\n",
      "Epoch [73/100], Loss: 0.0344\n",
      "Correlation: -0.092359 \n",
      " P-Value: 0.199079\n",
      "Epoch [74/100], Loss: 0.0344\n",
      "Correlation: -0.091419 \n",
      " P-Value: 0.203709\n",
      "Epoch [75/100], Loss: 0.0344\n",
      "Correlation: -0.087865 \n",
      " P-Value: 0.221919\n",
      "Epoch [76/100], Loss: 0.0344\n",
      "Correlation: -0.089178 \n",
      " P-Value: 0.215060\n",
      "Epoch [77/100], Loss: 0.0344\n",
      "Correlation: -0.090165 \n",
      " P-Value: 0.210008\n",
      "Epoch [78/100], Loss: 0.0344\n",
      "Correlation: -0.087730 \n",
      " P-Value: 0.222631\n",
      "Epoch [79/100], Loss: 0.0344\n",
      "Correlation: -0.085572 \n",
      " P-Value: 0.234264\n",
      "Epoch [80/100], Loss: 0.0344\n",
      "Correlation: -0.087333 \n",
      " P-Value: 0.224740\n",
      "Epoch [81/100], Loss: 0.0344\n",
      "Correlation: -0.087868 \n",
      " P-Value: 0.221901\n",
      "Epoch [82/100], Loss: 0.0344\n",
      "Correlation: -0.085923 \n",
      " P-Value: 0.232341\n",
      "Epoch [83/100], Loss: 0.0344\n",
      "Correlation: -0.085170 \n",
      " P-Value: 0.236474\n",
      "Epoch [84/100], Loss: 0.0344\n",
      "Correlation: -0.085773 \n",
      " P-Value: 0.233166\n",
      "Epoch [85/100], Loss: 0.0344\n",
      "Correlation: -0.084336 \n",
      " P-Value: 0.241112\n",
      "Epoch [86/100], Loss: 0.0344\n",
      "Correlation: -0.083738 \n",
      " P-Value: 0.244478\n",
      "Epoch [87/100], Loss: 0.0344\n",
      "Correlation: -0.083730 \n",
      " P-Value: 0.244522\n",
      "Epoch [88/100], Loss: 0.0343\n",
      "Correlation: -0.081805 \n",
      " P-Value: 0.255577\n",
      "Epoch [89/100], Loss: 0.0343\n",
      "Correlation: -0.081263 \n",
      " P-Value: 0.258751\n",
      "Epoch [90/100], Loss: 0.0343\n",
      "Correlation: -0.081341 \n",
      " P-Value: 0.258295\n",
      "Epoch [91/100], Loss: 0.0343\n",
      "Correlation: -0.080685 \n",
      " P-Value: 0.262162\n",
      "Epoch [92/100], Loss: 0.0343\n",
      "Correlation: -0.078979 \n",
      " P-Value: 0.272418\n",
      "Epoch [93/100], Loss: 0.0343\n",
      "Correlation: -0.078644 \n",
      " P-Value: 0.274468\n",
      "Epoch [94/100], Loss: 0.0343\n",
      "Correlation: -0.077594 \n",
      " P-Value: 0.280943\n",
      "Epoch [95/100], Loss: 0.0343\n",
      "Correlation: -0.077751 \n",
      " P-Value: 0.279964\n",
      "Epoch [96/100], Loss: 0.0343\n",
      "Correlation: -0.075275 \n",
      " P-Value: 0.295612\n",
      "Epoch [97/100], Loss: 0.0343\n",
      "Correlation: -0.074698 \n",
      " P-Value: 0.299346\n",
      "Epoch [98/100], Loss: 0.0343\n",
      "Correlation: -0.076191 \n",
      " P-Value: 0.289760\n",
      "Epoch [99/100], Loss: 0.0343\n",
      "Correlation: -0.075083 \n",
      " P-Value: 0.296853\n",
      "Epoch [100/100], Loss: 0.0343\n",
      "Max correlation is: [-0.14951755] at 24th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 242\n",
    "net_5 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_5.train_net(num_epochs, train_loader_list[5], test_loader_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.052458 \n",
      " P-Value: 0.466409\n",
      "Epoch [1/100], Loss: 0.0497\n",
      "Correlation: 0.014553 \n",
      " P-Value: 0.839980\n",
      "Epoch [2/100], Loss: 0.0259\n",
      "Correlation: -0.018627 \n",
      " P-Value: 0.796052\n",
      "Epoch [3/100], Loss: 0.0235\n",
      "Correlation: -0.096597 \n",
      " P-Value: 0.179153\n",
      "Epoch [4/100], Loss: 0.0225\n",
      "Correlation: -0.106896 \n",
      " P-Value: 0.136911\n",
      "Epoch [5/100], Loss: 0.0214\n",
      "Correlation: -0.123951 \n",
      " P-Value: 0.084272\n",
      "Epoch [6/100], Loss: 0.0210\n",
      "Correlation: -0.121519 \n",
      " P-Value: 0.090592\n",
      "Epoch [7/100], Loss: 0.0207\n",
      "Correlation: -0.090110 \n",
      " P-Value: 0.210288\n",
      "Epoch [8/100], Loss: 0.0204\n",
      "Correlation: -0.081228 \n",
      " P-Value: 0.258956\n",
      "Epoch [9/100], Loss: 0.0204\n",
      "Correlation: -0.067984 \n",
      " P-Value: 0.345000\n",
      "Epoch [10/100], Loss: 0.0202\n",
      "Correlation: -0.080260 \n",
      " P-Value: 0.264695\n",
      "Epoch [11/100], Loss: 0.0202\n",
      "Correlation: -0.067532 \n",
      " P-Value: 0.348225\n",
      "Epoch [12/100], Loss: 0.0200\n",
      "Correlation: -0.071615 \n",
      " P-Value: 0.319787\n",
      "Epoch [13/100], Loss: 0.0200\n",
      "Correlation: -0.071912 \n",
      " P-Value: 0.317777\n",
      "Epoch [14/100], Loss: 0.0200\n",
      "Correlation: -0.078560 \n",
      " P-Value: 0.274979\n",
      "Epoch [15/100], Loss: 0.0199\n",
      "Correlation: -0.083123 \n",
      " P-Value: 0.247971\n",
      "Epoch [16/100], Loss: 0.0199\n",
      "Correlation: -0.079808 \n",
      " P-Value: 0.267404\n",
      "Epoch [17/100], Loss: 0.0199\n",
      "Correlation: -0.081855 \n",
      " P-Value: 0.255289\n",
      "Epoch [18/100], Loss: 0.0199\n",
      "Correlation: -0.072584 \n",
      " P-Value: 0.313267\n",
      "Epoch [19/100], Loss: 0.0198\n",
      "Correlation: -0.067137 \n",
      " P-Value: 0.351057\n",
      "Epoch [20/100], Loss: 0.0198\n",
      "Correlation: -0.068695 \n",
      " P-Value: 0.339964\n",
      "Epoch [21/100], Loss: 0.0198\n",
      "Correlation: -0.070212 \n",
      " P-Value: 0.329380\n",
      "Epoch [22/100], Loss: 0.0198\n",
      "Correlation: -0.068285 \n",
      " P-Value: 0.342864\n",
      "Epoch [23/100], Loss: 0.0198\n",
      "Correlation: -0.063952 \n",
      " P-Value: 0.374426\n",
      "Epoch [24/100], Loss: 0.0198\n",
      "Correlation: -0.061894 \n",
      " P-Value: 0.390020\n",
      "Epoch [25/100], Loss: 0.0197\n",
      "Correlation: -0.061297 \n",
      " P-Value: 0.394621\n",
      "Epoch [26/100], Loss: 0.0197\n",
      "Correlation: -0.061015 \n",
      " P-Value: 0.396805\n",
      "Epoch [27/100], Loss: 0.0197\n",
      "Correlation: -0.061101 \n",
      " P-Value: 0.396143\n",
      "Epoch [28/100], Loss: 0.0197\n",
      "Correlation: -0.058468 \n",
      " P-Value: 0.416845\n",
      "Epoch [29/100], Loss: 0.0197\n",
      "Correlation: -0.058635 \n",
      " P-Value: 0.415510\n",
      "Epoch [30/100], Loss: 0.0197\n",
      "Correlation: -0.060733 \n",
      " P-Value: 0.398998\n",
      "Epoch [31/100], Loss: 0.0197\n",
      "Correlation: -0.060043 \n",
      " P-Value: 0.404383\n",
      "Epoch [32/100], Loss: 0.0197\n",
      "Correlation: -0.059630 \n",
      " P-Value: 0.407636\n",
      "Epoch [33/100], Loss: 0.0197\n",
      "Correlation: -0.058604 \n",
      " P-Value: 0.415761\n",
      "Epoch [34/100], Loss: 0.0197\n",
      "Correlation: -0.056522 \n",
      " P-Value: 0.432551\n",
      "Epoch [35/100], Loss: 0.0197\n",
      "Correlation: -0.057617 \n",
      " P-Value: 0.423673\n",
      "Epoch [36/100], Loss: 0.0197\n",
      "Correlation: -0.056720 \n",
      " P-Value: 0.430939\n",
      "Epoch [37/100], Loss: 0.0197\n",
      "Correlation: -0.058192 \n",
      " P-Value: 0.419051\n",
      "Epoch [38/100], Loss: 0.0197\n",
      "Correlation: -0.056123 \n",
      " P-Value: 0.435807\n",
      "Epoch [39/100], Loss: 0.0197\n",
      "Correlation: -0.057774 \n",
      " P-Value: 0.422413\n",
      "Epoch [40/100], Loss: 0.0196\n",
      "Correlation: -0.057324 \n",
      " P-Value: 0.426038\n",
      "Epoch [41/100], Loss: 0.0196\n",
      "Correlation: -0.055559 \n",
      " P-Value: 0.440442\n",
      "Epoch [42/100], Loss: 0.0196\n",
      "Correlation: -0.057585 \n",
      " P-Value: 0.423927\n",
      "Epoch [43/100], Loss: 0.0196\n",
      "Correlation: -0.059206 \n",
      " P-Value: 0.410979\n",
      "Epoch [44/100], Loss: 0.0196\n",
      "Correlation: -0.054898 \n",
      " P-Value: 0.445908\n",
      "Epoch [45/100], Loss: 0.0196\n",
      "Correlation: -0.055524 \n",
      " P-Value: 0.440729\n",
      "Epoch [46/100], Loss: 0.0196\n",
      "Correlation: -0.058348 \n",
      " P-Value: 0.417803\n",
      "Epoch [47/100], Loss: 0.0196\n",
      "Correlation: -0.059466 \n",
      " P-Value: 0.408918\n",
      "Epoch [48/100], Loss: 0.0196\n",
      "Correlation: -0.054921 \n",
      " P-Value: 0.445719\n",
      "Epoch [49/100], Loss: 0.0196\n",
      "Correlation: -0.057121 \n",
      " P-Value: 0.427678\n",
      "Epoch [50/100], Loss: 0.0196\n",
      "Correlation: -0.060342 \n",
      " P-Value: 0.402046\n",
      "Epoch [51/100], Loss: 0.0196\n",
      "Correlation: -0.057123 \n",
      " P-Value: 0.427661\n",
      "Epoch [52/100], Loss: 0.0196\n",
      "Correlation: -0.057759 \n",
      " P-Value: 0.422529\n",
      "Epoch [53/100], Loss: 0.0196\n",
      "Correlation: -0.060050 \n",
      " P-Value: 0.404332\n",
      "Epoch [54/100], Loss: 0.0196\n",
      "Correlation: -0.056963 \n",
      " P-Value: 0.428961\n",
      "Epoch [55/100], Loss: 0.0196\n",
      "Correlation: -0.056539 \n",
      " P-Value: 0.432413\n",
      "Epoch [56/100], Loss: 0.0196\n",
      "Correlation: -0.059386 \n",
      " P-Value: 0.409560\n",
      "Epoch [57/100], Loss: 0.0196\n",
      "Correlation: -0.056275 \n",
      " P-Value: 0.434562\n",
      "Epoch [58/100], Loss: 0.0196\n",
      "Correlation: -0.057348 \n",
      " P-Value: 0.425836\n",
      "Epoch [59/100], Loss: 0.0196\n",
      "Correlation: -0.058855 \n",
      " P-Value: 0.413766\n",
      "Epoch [60/100], Loss: 0.0196\n",
      "Correlation: -0.056914 \n",
      " P-Value: 0.429357\n",
      "Epoch [61/100], Loss: 0.0196\n",
      "Correlation: -0.059559 \n",
      " P-Value: 0.408187\n",
      "Epoch [62/100], Loss: 0.0196\n",
      "Correlation: -0.059264 \n",
      " P-Value: 0.410517\n",
      "Epoch [63/100], Loss: 0.0196\n",
      "Correlation: -0.056800 \n",
      " P-Value: 0.430285\n",
      "Epoch [64/100], Loss: 0.0196\n",
      "Correlation: -0.058796 \n",
      " P-Value: 0.414237\n",
      "Epoch [65/100], Loss: 0.0196\n",
      "Correlation: -0.059939 \n",
      " P-Value: 0.405197\n",
      "Epoch [66/100], Loss: 0.0196\n",
      "Correlation: -0.059039 \n",
      " P-Value: 0.412305\n",
      "Epoch [67/100], Loss: 0.0196\n",
      "Correlation: -0.059104 \n",
      " P-Value: 0.411785\n",
      "Epoch [68/100], Loss: 0.0196\n",
      "Correlation: -0.059266 \n",
      " P-Value: 0.410501\n",
      "Epoch [69/100], Loss: 0.0196\n",
      "Correlation: -0.060536 \n",
      " P-Value: 0.400529\n",
      "Epoch [70/100], Loss: 0.0196\n",
      "Correlation: -0.059054 \n",
      " P-Value: 0.412185\n",
      "Epoch [71/100], Loss: 0.0196\n",
      "Correlation: -0.061510 \n",
      " P-Value: 0.392980\n",
      "Epoch [72/100], Loss: 0.0196\n",
      "Correlation: -0.059010 \n",
      " P-Value: 0.412533\n",
      "Epoch [73/100], Loss: 0.0196\n",
      "Correlation: -0.060028 \n",
      " P-Value: 0.404507\n",
      "Epoch [74/100], Loss: 0.0196\n",
      "Correlation: -0.062229 \n",
      " P-Value: 0.387460\n",
      "Epoch [75/100], Loss: 0.0196\n",
      "Correlation: -0.059839 \n",
      " P-Value: 0.405987\n",
      "Epoch [76/100], Loss: 0.0196\n",
      "Correlation: -0.061019 \n",
      " P-Value: 0.396774\n",
      "Epoch [77/100], Loss: 0.0196\n",
      "Correlation: -0.061464 \n",
      " P-Value: 0.393331\n",
      "Epoch [78/100], Loss: 0.0196\n",
      "Correlation: -0.062268 \n",
      " P-Value: 0.387160\n",
      "Epoch [79/100], Loss: 0.0196\n",
      "Correlation: -0.059684 \n",
      " P-Value: 0.407207\n",
      "Epoch [80/100], Loss: 0.0196\n",
      "Correlation: -0.064607 \n",
      " P-Value: 0.369543\n",
      "Epoch [81/100], Loss: 0.0196\n",
      "Correlation: -0.059406 \n",
      " P-Value: 0.409393\n",
      "Epoch [82/100], Loss: 0.0196\n",
      "Correlation: -0.062623 \n",
      " P-Value: 0.384457\n",
      "Epoch [83/100], Loss: 0.0196\n",
      "Correlation: -0.062832 \n",
      " P-Value: 0.382876\n",
      "Epoch [84/100], Loss: 0.0196\n",
      "Correlation: -0.060593 \n",
      " P-Value: 0.400084\n",
      "Epoch [85/100], Loss: 0.0196\n",
      "Correlation: -0.060599 \n",
      " P-Value: 0.400035\n",
      "Epoch [86/100], Loss: 0.0196\n",
      "Correlation: -0.062702 \n",
      " P-Value: 0.383853\n",
      "Epoch [87/100], Loss: 0.0196\n",
      "Correlation: -0.060968 \n",
      " P-Value: 0.397168\n",
      "Epoch [88/100], Loss: 0.0196\n",
      "Correlation: -0.063829 \n",
      " P-Value: 0.375345\n",
      "Epoch [89/100], Loss: 0.0196\n",
      "Correlation: -0.062215 \n",
      " P-Value: 0.387566\n",
      "Epoch [90/100], Loss: 0.0196\n",
      "Correlation: -0.063217 \n",
      " P-Value: 0.379955\n",
      "Epoch [91/100], Loss: 0.0196\n",
      "Correlation: -0.060821 \n",
      " P-Value: 0.398312\n",
      "Epoch [92/100], Loss: 0.0196\n",
      "Correlation: -0.063973 \n",
      " P-Value: 0.374273\n",
      "Epoch [93/100], Loss: 0.0196\n",
      "Correlation: -0.060325 \n",
      " P-Value: 0.402173\n",
      "Epoch [94/100], Loss: 0.0196\n",
      "Correlation: -0.064755 \n",
      " P-Value: 0.368449\n",
      "Epoch [95/100], Loss: 0.0196\n",
      "Correlation: -0.063091 \n",
      " P-Value: 0.380908\n",
      "Epoch [96/100], Loss: 0.0196\n",
      "Correlation: -0.062928 \n",
      " P-Value: 0.382143\n",
      "Epoch [97/100], Loss: 0.0196\n",
      "Correlation: -0.062297 \n",
      " P-Value: 0.386941\n",
      "Epoch [98/100], Loss: 0.0196\n",
      "Correlation: -0.063949 \n",
      " P-Value: 0.374451\n",
      "Epoch [99/100], Loss: 0.0196\n",
      "Correlation: -0.061279 \n",
      " P-Value: 0.394760\n",
      "Epoch [100/100], Loss: 0.0196\n",
      "Max correlation is: [-0.12395083] at 5th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 235\n",
    "net_6 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_6.train_net(num_epochs, train_loader_list[6], test_loader_list[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation: 0.142367 \n",
      " P-Value: 0.047101\n",
      "Epoch [1/100], Loss: 0.2718\n",
      "Correlation: 0.135292 \n",
      " P-Value: 0.059324\n",
      "Epoch [2/100], Loss: 0.1491\n",
      "Correlation: 0.109294 \n",
      " P-Value: 0.128272\n",
      "Epoch [3/100], Loss: 0.0918\n",
      "Correlation: 0.054964 \n",
      " P-Value: 0.445363\n",
      "Epoch [4/100], Loss: 0.0590\n",
      "Correlation: -0.057399 \n",
      " P-Value: 0.425433\n",
      "Epoch [5/100], Loss: 0.0468\n",
      "Correlation: -0.053511 \n",
      " P-Value: 0.457502\n",
      "Epoch [6/100], Loss: 0.0467\n",
      "Correlation: -0.037105 \n",
      " P-Value: 0.606565\n",
      "Epoch [7/100], Loss: 0.0420\n",
      "Correlation: 0.006563 \n",
      " P-Value: 0.927428\n",
      "Epoch [8/100], Loss: 0.0367\n",
      "Correlation: 0.002441 \n",
      " P-Value: 0.973114\n",
      "Epoch [9/100], Loss: 0.0341\n",
      "Correlation: -0.055265 \n",
      " P-Value: 0.442870\n",
      "Epoch [10/100], Loss: 0.0321\n",
      "Correlation: -0.068440 \n",
      " P-Value: 0.341768\n",
      "Epoch [11/100], Loss: 0.0313\n",
      "Correlation: -0.032883 \n",
      " P-Value: 0.648143\n",
      "Epoch [12/100], Loss: 0.0306\n",
      "Correlation: -0.027822 \n",
      " P-Value: 0.699437\n",
      "Epoch [13/100], Loss: 0.0299\n",
      "Correlation: -0.023425 \n",
      " P-Value: 0.745129\n",
      "Epoch [14/100], Loss: 0.0295\n",
      "Correlation: -0.021113 \n",
      " P-Value: 0.769558\n",
      "Epoch [15/100], Loss: 0.0289\n",
      "Correlation: -0.032190 \n",
      " P-Value: 0.655062\n",
      "Epoch [16/100], Loss: 0.0286\n",
      "Correlation: -0.013432 \n",
      " P-Value: 0.852177\n",
      "Epoch [17/100], Loss: 0.0283\n",
      "Correlation: -0.011494 \n",
      " P-Value: 0.873305\n",
      "Epoch [18/100], Loss: 0.0279\n",
      "Correlation: -0.017482 \n",
      " P-Value: 0.808327\n",
      "Epoch [19/100], Loss: 0.0277\n",
      "Correlation: -0.018739 \n",
      " P-Value: 0.794857\n",
      "Epoch [20/100], Loss: 0.0275\n",
      "Correlation: -0.024352 \n",
      " P-Value: 0.735426\n",
      "Epoch [21/100], Loss: 0.0272\n",
      "Correlation: -0.020510 \n",
      " P-Value: 0.775947\n",
      "Epoch [22/100], Loss: 0.0271\n",
      "Correlation: -0.018496 \n",
      " P-Value: 0.797445\n",
      "Epoch [23/100], Loss: 0.0269\n",
      "Correlation: -0.020212 \n",
      " P-Value: 0.779140\n",
      "Epoch [24/100], Loss: 0.0267\n",
      "Correlation: -0.018864 \n",
      " P-Value: 0.793533\n",
      "Epoch [25/100], Loss: 0.0266\n",
      "Correlation: -0.020249 \n",
      " P-Value: 0.778732\n",
      "Epoch [26/100], Loss: 0.0264\n",
      "Correlation: -0.020757 \n",
      " P-Value: 0.773322\n",
      "Epoch [27/100], Loss: 0.0263\n",
      "Correlation: -0.018358 \n",
      " P-Value: 0.798936\n",
      "Epoch [28/100], Loss: 0.0262\n",
      "Correlation: -0.017644 \n",
      " P-Value: 0.806594\n",
      "Epoch [29/100], Loss: 0.0261\n",
      "Correlation: -0.017119 \n",
      " P-Value: 0.812237\n",
      "Epoch [30/100], Loss: 0.0260\n",
      "Correlation: -0.017201 \n",
      " P-Value: 0.811340\n",
      "Epoch [31/100], Loss: 0.0259\n",
      "Correlation: -0.016674 \n",
      " P-Value: 0.817025\n",
      "Epoch [32/100], Loss: 0.0258\n",
      "Correlation: -0.014403 \n",
      " P-Value: 0.841611\n",
      "Epoch [33/100], Loss: 0.0257\n",
      "Correlation: -0.013977 \n",
      " P-Value: 0.846241\n",
      "Epoch [34/100], Loss: 0.0257\n",
      "Correlation: -0.013417 \n",
      " P-Value: 0.852322\n",
      "Epoch [35/100], Loss: 0.0256\n",
      "Correlation: -0.012827 \n",
      " P-Value: 0.858752\n",
      "Epoch [36/100], Loss: 0.0255\n",
      "Correlation: -0.013089 \n",
      " P-Value: 0.855896\n",
      "Epoch [37/100], Loss: 0.0254\n",
      "Correlation: -0.013094 \n",
      " P-Value: 0.855822\n",
      "Epoch [38/100], Loss: 0.0254\n",
      "Correlation: -0.011941 \n",
      " P-Value: 0.868387\n",
      "Epoch [39/100], Loss: 0.0253\n",
      "Correlation: -0.011282 \n",
      " P-Value: 0.875623\n",
      "Epoch [40/100], Loss: 0.0253\n",
      "Correlation: -0.011243 \n",
      " P-Value: 0.876029\n",
      "Epoch [41/100], Loss: 0.0252\n",
      "Correlation: -0.010971 \n",
      " P-Value: 0.878993\n",
      "Epoch [42/100], Loss: 0.0252\n",
      "Correlation: -0.011321 \n",
      " P-Value: 0.875191\n",
      "Epoch [43/100], Loss: 0.0251\n",
      "Correlation: -0.010505 \n",
      " P-Value: 0.884129\n",
      "Epoch [44/100], Loss: 0.0251\n",
      "Correlation: -0.008911 \n",
      " P-Value: 0.901610\n",
      "Epoch [45/100], Loss: 0.0250\n",
      "Correlation: -0.009561 \n",
      " P-Value: 0.894487\n",
      "Epoch [46/100], Loss: 0.0250\n",
      "Correlation: -0.009663 \n",
      " P-Value: 0.893364\n",
      "Epoch [47/100], Loss: 0.0250\n",
      "Correlation: -0.008691 \n",
      " P-Value: 0.904029\n",
      "Epoch [48/100], Loss: 0.0249\n",
      "Correlation: -0.008586 \n",
      " P-Value: 0.905166\n",
      "Epoch [49/100], Loss: 0.0249\n",
      "Correlation: -0.007664 \n",
      " P-Value: 0.915335\n",
      "Epoch [50/100], Loss: 0.0249\n",
      "Correlation: -0.007190 \n",
      " P-Value: 0.920550\n",
      "Epoch [51/100], Loss: 0.0248\n",
      "Correlation: -0.007074 \n",
      " P-Value: 0.921793\n",
      "Epoch [52/100], Loss: 0.0248\n",
      "Correlation: -0.005956 \n",
      " P-Value: 0.934093\n",
      "Epoch [53/100], Loss: 0.0248\n",
      "Correlation: -0.005208 \n",
      " P-Value: 0.942399\n",
      "Epoch [54/100], Loss: 0.0248\n",
      "Correlation: -0.005418 \n",
      " P-Value: 0.940107\n",
      "Epoch [55/100], Loss: 0.0247\n",
      "Correlation: -0.005464 \n",
      " P-Value: 0.939563\n",
      "Epoch [56/100], Loss: 0.0247\n",
      "Correlation: -0.004455 \n",
      " P-Value: 0.950712\n",
      "Epoch [57/100], Loss: 0.0247\n",
      "Correlation: -0.004782 \n",
      " P-Value: 0.947146\n",
      "Epoch [58/100], Loss: 0.0247\n",
      "Correlation: -0.005855 \n",
      " P-Value: 0.935206\n",
      "Epoch [59/100], Loss: 0.0247\n",
      "Correlation: -0.005965 \n",
      " P-Value: 0.934038\n",
      "Epoch [60/100], Loss: 0.0246\n",
      "Correlation: -0.005060 \n",
      " P-Value: 0.944001\n",
      "Epoch [61/100], Loss: 0.0246\n",
      "Correlation: -0.005450 \n",
      " P-Value: 0.939684\n",
      "Epoch [62/100], Loss: 0.0246\n",
      "Correlation: -0.005661 \n",
      " P-Value: 0.937434\n",
      "Epoch [63/100], Loss: 0.0246\n",
      "Correlation: -0.004812 \n",
      " P-Value: 0.946734\n",
      "Epoch [64/100], Loss: 0.0246\n",
      "Correlation: -0.004171 \n",
      " P-Value: 0.953842\n",
      "Epoch [65/100], Loss: 0.0246\n",
      "Correlation: -0.004274 \n",
      " P-Value: 0.952750\n",
      "Epoch [66/100], Loss: 0.0245\n",
      "Correlation: -0.004384 \n",
      " P-Value: 0.951532\n",
      "Epoch [67/100], Loss: 0.0245\n",
      "Correlation: -0.004044 \n",
      " P-Value: 0.955204\n",
      "Epoch [68/100], Loss: 0.0245\n",
      "Correlation: -0.003555 \n",
      " P-Value: 0.960664\n",
      "Epoch [69/100], Loss: 0.0245\n",
      "Correlation: -0.003726 \n",
      " P-Value: 0.958763\n",
      "Epoch [70/100], Loss: 0.0245\n",
      "Correlation: -0.003488 \n",
      " P-Value: 0.961413\n",
      "Epoch [71/100], Loss: 0.0245\n",
      "Correlation: -0.002900 \n",
      " P-Value: 0.967916\n",
      "Epoch [72/100], Loss: 0.0245\n",
      "Correlation: -0.002746 \n",
      " P-Value: 0.969670\n",
      "Epoch [73/100], Loss: 0.0245\n",
      "Correlation: -0.002465 \n",
      " P-Value: 0.972709\n",
      "Epoch [74/100], Loss: 0.0245\n",
      "Correlation: -0.001897 \n",
      " P-Value: 0.978894\n",
      "Epoch [75/100], Loss: 0.0245\n",
      "Correlation: -0.002236 \n",
      " P-Value: 0.975233\n",
      "Epoch [76/100], Loss: 0.0245\n",
      "Correlation: -0.001372 \n",
      " P-Value: 0.984712\n",
      "Epoch [77/100], Loss: 0.0244\n",
      "Correlation: -0.001449 \n",
      " P-Value: 0.983785\n",
      "Epoch [78/100], Loss: 0.0244\n",
      "Correlation: -0.001391 \n",
      " P-Value: 0.984712\n",
      "Epoch [79/100], Loss: 0.0244\n",
      "Correlation: -0.001440 \n",
      " P-Value: 0.984242\n",
      "Epoch [80/100], Loss: 0.0244\n",
      "Correlation: -0.000181 \n",
      " P-Value: 1.000000\n",
      "Epoch [81/100], Loss: 0.0244\n",
      "Correlation: -0.000449 \n",
      " P-Value: 0.994595\n",
      "Epoch [82/100], Loss: 0.0244\n",
      "Correlation: 0.000327 \n",
      " P-Value: 0.997297\n",
      "Epoch [83/100], Loss: 0.0244\n",
      "Correlation: 0.000153 \n",
      " P-Value: 1.000000\n",
      "Epoch [84/100], Loss: 0.0244\n",
      "Correlation: 0.001192 \n",
      " P-Value: 0.986760\n",
      "Epoch [85/100], Loss: 0.0244\n",
      "Correlation: 0.000486 \n",
      " P-Value: 0.994595\n",
      "Epoch [86/100], Loss: 0.0244\n",
      "Correlation: 0.001128 \n",
      " P-Value: 0.987615\n",
      "Epoch [87/100], Loss: 0.0244\n",
      "Correlation: 0.002005 \n",
      " P-Value: 0.977716\n",
      "Epoch [88/100], Loss: 0.0244\n",
      "Correlation: 0.001746 \n",
      " P-Value: 0.980513\n",
      "Epoch [89/100], Loss: 0.0244\n",
      "Correlation: 0.002041 \n",
      " P-Value: 0.977391\n",
      "Epoch [90/100], Loss: 0.0244\n",
      "Correlation: 0.001617 \n",
      " P-Value: 0.982074\n",
      "Epoch [91/100], Loss: 0.0244\n",
      "Correlation: 0.002703 \n",
      " P-Value: 0.970155\n",
      "Epoch [92/100], Loss: 0.0244\n",
      "Correlation: 0.002338 \n",
      " P-Value: 0.974081\n",
      "Epoch [93/100], Loss: 0.0244\n",
      "Correlation: 0.003889 \n",
      " P-Value: 0.957032\n",
      "Epoch [94/100], Loss: 0.0244\n",
      "Correlation: 0.002791 \n",
      " P-Value: 0.969074\n",
      "Epoch [95/100], Loss: 0.0243\n",
      "Correlation: 0.003950 \n",
      " P-Value: 0.956358\n",
      "Epoch [96/100], Loss: 0.0243\n",
      "Correlation: 0.003590 \n",
      " P-Value: 0.960295\n",
      "Epoch [97/100], Loss: 0.0243\n",
      "Correlation: 0.004747 \n",
      " P-Value: 0.947491\n",
      "Epoch [98/100], Loss: 0.0243\n",
      "Correlation: 0.004928 \n",
      " P-Value: 0.945517\n",
      "Epoch [99/100], Loss: 0.0243\n",
      "Correlation: 0.004944 \n",
      " P-Value: 0.945317\n",
      "Epoch [100/100], Loss: 0.0243\n",
      "Max correlation is: [0.14236729] at 0th epoch\n"
     ]
    }
   ],
   "source": [
    "input_size = 232\n",
    "net_7 = Net(input_size, h1, h2, h3, output_size, lr).to(device)\n",
    "net_7.train_net(num_epochs, train_loader_list[7], test_loader_list[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
